{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"SDA Dropwizard Commons \u00b6 SDA Dropwizard Commons is a set of libraries to bootstrap services easily that follow the patterns and specifications promoted by the SDA SE. SDA Dropwizard Commons is separated in different modules that can be combined as needed. Most of the modules require the technologies that are recommended for services in the SDA Platform. These technologies include Dropwizard Jackson JAX-RS Jersey Swagger Hibernate Kafka MongoDB Open Policy Agent OpenTelemetry Modules in SDA Dropwizard Commons \u00b6 Server \u00b6 All modules prefixed with sda-commons-server- provide technology and configuration support used in backend services to provide REST Endpoints. Main Server Modules \u00b6 The main server modules help to bootstrap and test a Dropwizard application with convergent dependencies. Starter \u00b6 The module sda-commons-starter provides all basics required to build a service for the SDA Platform with Dropwizard. The module sda-commons-starter-example gives a small example on starting an application using defaults for the SDA Platform. Testing \u00b6 The module sda-commons-server-testing is the base module to add unit and integration tests for applications in the SDA SE infrastructure. Some modules have a more specialized testing module, e.g. the sda-commons-server-hibernate module has a sda-commons-server-hibernate-testing module, providing further support. Additional Server Modules \u00b6 The additional server modules add helpful technologies to the Dropwizard application. Auth \u00b6 The module sda-commons-server-auth provides support to add authentication using JSON Web Tokens with different sources for the public keys of the signing authorities. Circuit Breaker \u00b6 The module sda-commons-server-circuitbreaker provides support to inject circuit breakers into synchronous calls to other services. Consumer Token \u00b6 The module sda-commons-server-consumer adds support to track or require a consumer token identifying the calling application. Cross-Origin Resource Sharing \u00b6 The module sda-commons-server-cors adds support for CORS. This allows Cross-origin resource sharing for the service. Forms \u00b6 The module sda-commons-shared-forms adds all required dependencies to support multipart/* in Dropwizard applications. Dropwizard \u00b6 The module sda-commons-server-dropwizard provides io.dropwizard:dropwizard-core with convergent dependencies. All other SDA Dropwizard Commons Server modules use this dependency and are aligned to the versions provided by sda-commons-server-dropwizard . It also provides some common bundles that require no additional dependencies. Health Check \u00b6 The module sda-commons-server-healthcheck introduces the possibility to distinguish internal and external health checks. The module sda-commons-server-healthcheck-example presents a simple application that shows the usage of the bundle and implementation of new health checks. Hibernate \u00b6 The module sda-commons-server-hibernate provides access to relational databases with hibernate. The module sda-commons-server-hibernate-exmaple shows how to use the bundle within an application. Jackson \u00b6 The module sda-commons-server-jackson is used for several purposes * configure the ObjectMapper with the recommended default settings of SDA SE services. * provides support for linking resources with HAL * adds the ability to filter fields on client request * registers exception mapper to support the common error structure as defined within the rest guide Kafka \u00b6 The module sda-commons-server-kafka provides means to send and consume messages from a Kafka topic. The module sda-commons-server-kafka-example includes applications, one with consumer and one with producer examples. Key Management \u00b6 The module sda-commons-server-key-mgmt provides means to provide and map enumerations (keys) between the API data model and a possible implementation. MongoDB \u00b6 The module sda-commons-server-spring-data-mongo is used to work with MongoDB using Spring Data Mongo . The module sda-commons-server-mongo-testing provides a MongoDB instance for integration testing. The example package shows how to use the bundle within an application. Open Telemetry \u00b6 The module sda-commons-server-opentelemetry provides OpenTelemetry instrumentation for JAX-RS. Other bundles like sda-commons-client-jersey , sda-commons-server-spring-data-mongo or sda-commons-server-s3 come with built-in instrumentation. Besides instrumentation, it's also required to specify a collector, like Jaeger . The module sda-commons-server-opentelemetry-example shows how to use OpenTelemetry and Jaeger within an application and has examples for manual instrumentation. Prometheus \u00b6 The module sda-commons-server-prometheus provides an admin endpoint to serve metrics in a format that Prometheus can read. The module sda-commons-server-prometheus-example presents a simple application that shows the three main types of metrics to use in a service. S3 Object Storage \u00b6 The module sda-commons-server-s3 provides a client for an AWS S3-compatible object storage. The module sda-commons-server-s3-testing is used to provide an AWS S3-compatible Object Storage during integrations tests. Security \u00b6 The module sda-commons-server-security helps to configure a secure Dropwizard application. Shared Certificates \u00b6 The module sda-commons-shared-certificates adds support for trusting custom certificate authorities. Trace Token \u00b6 The module sda-commons-server-trace adds support to track create a trace token to correlate a set of service invocations that belongs to the same logically cohesive call of a higher level service offered by the SDA Platform, e.g. interaction service. . Weld \u00b6 The module sda-commons-server-weld is used to bootstrap Dropwizard applications inside a Weld-SE container and provides CDI support for servlets, listeners and resources. The module sda-commons-server-weld-example gives a small example on starting an application within a Weld container. YAML \u00b6 The module sda-commons-shared-yaml adds support for YAML-file handling. Client \u00b6 All modules prefixed with sda-commons-client- provide support for applications that use an HTTP client to access other services. Jersey \u00b6 The module sda-commons-client-jersey provides support for using Jersey clients withing the Dropwizard application. The module sda-commons-client-jersey-wiremock-testing bundles the WireMock dependencies to mock services in integration tests consistently to sda-commons library versions. The module sda-commons-client-jersey-example presents an example application that shows how to invoke services. Usage \u00b6 The compiled releases are publicly available via maven central . Include sda-commons-bom and sda-commons-dependencies as platform constraints. You will inherit all versions defined there and won't have to specify versions for them yourself. More details: - sda-commons-bom - sda-commons-dependencies Note: You need Gradle 5.x for platform dependencies. More information can be found here . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 project.ext { sdaCommonsVersion = 'x.x.x' } dependencies { // define platform dependencies for simplified dependency management compile enforcedPlatform(\"org.sdase.commons.sda-commons-dependencies:$sdaCommonsVersion\") compile enforcedPlatform(\"org.sdase.commons.sda-commons-bom:$sdaCommonsVersion\") ... // Add dependencies to sda-commons-modules (managed by sda-commons-bom) compile \"org.sdase.commons:sda-commons-client-jersey\" ... // Add other dependencies (managed by 'sda-commons-dependencies') compile 'org.glassfish.jersey.core:jersey-client' // Add other unmanaged dependencies compile 'org.apache.commons:commons-digester3:3.2' }","title":"Overview"},{"location":"#sda-dropwizard-commons","text":"SDA Dropwizard Commons is a set of libraries to bootstrap services easily that follow the patterns and specifications promoted by the SDA SE. SDA Dropwizard Commons is separated in different modules that can be combined as needed. Most of the modules require the technologies that are recommended for services in the SDA Platform. These technologies include Dropwizard Jackson JAX-RS Jersey Swagger Hibernate Kafka MongoDB Open Policy Agent OpenTelemetry","title":"SDA Dropwizard Commons"},{"location":"#modules-in-sda-dropwizard-commons","text":"","title":"Modules in SDA Dropwizard Commons"},{"location":"#server","text":"All modules prefixed with sda-commons-server- provide technology and configuration support used in backend services to provide REST Endpoints.","title":"Server"},{"location":"#main-server-modules","text":"The main server modules help to bootstrap and test a Dropwizard application with convergent dependencies.","title":"Main Server Modules"},{"location":"#starter","text":"The module sda-commons-starter provides all basics required to build a service for the SDA Platform with Dropwizard. The module sda-commons-starter-example gives a small example on starting an application using defaults for the SDA Platform.","title":"Starter"},{"location":"#testing","text":"The module sda-commons-server-testing is the base module to add unit and integration tests for applications in the SDA SE infrastructure. Some modules have a more specialized testing module, e.g. the sda-commons-server-hibernate module has a sda-commons-server-hibernate-testing module, providing further support.","title":"Testing"},{"location":"#additional-server-modules","text":"The additional server modules add helpful technologies to the Dropwizard application.","title":"Additional Server Modules"},{"location":"#auth","text":"The module sda-commons-server-auth provides support to add authentication using JSON Web Tokens with different sources for the public keys of the signing authorities.","title":"Auth"},{"location":"#circuit-breaker","text":"The module sda-commons-server-circuitbreaker provides support to inject circuit breakers into synchronous calls to other services.","title":"Circuit Breaker"},{"location":"#consumer-token","text":"The module sda-commons-server-consumer adds support to track or require a consumer token identifying the calling application.","title":"Consumer Token"},{"location":"#cross-origin-resource-sharing","text":"The module sda-commons-server-cors adds support for CORS. This allows Cross-origin resource sharing for the service.","title":"Cross-Origin Resource Sharing"},{"location":"#forms","text":"The module sda-commons-shared-forms adds all required dependencies to support multipart/* in Dropwizard applications.","title":"Forms"},{"location":"#dropwizard","text":"The module sda-commons-server-dropwizard provides io.dropwizard:dropwizard-core with convergent dependencies. All other SDA Dropwizard Commons Server modules use this dependency and are aligned to the versions provided by sda-commons-server-dropwizard . It also provides some common bundles that require no additional dependencies.","title":"Dropwizard"},{"location":"#health-check","text":"The module sda-commons-server-healthcheck introduces the possibility to distinguish internal and external health checks. The module sda-commons-server-healthcheck-example presents a simple application that shows the usage of the bundle and implementation of new health checks.","title":"Health Check"},{"location":"#hibernate","text":"The module sda-commons-server-hibernate provides access to relational databases with hibernate. The module sda-commons-server-hibernate-exmaple shows how to use the bundle within an application.","title":"Hibernate"},{"location":"#jackson","text":"The module sda-commons-server-jackson is used for several purposes * configure the ObjectMapper with the recommended default settings of SDA SE services. * provides support for linking resources with HAL * adds the ability to filter fields on client request * registers exception mapper to support the common error structure as defined within the rest guide","title":"Jackson"},{"location":"#kafka","text":"The module sda-commons-server-kafka provides means to send and consume messages from a Kafka topic. The module sda-commons-server-kafka-example includes applications, one with consumer and one with producer examples.","title":"Kafka"},{"location":"#key-management","text":"The module sda-commons-server-key-mgmt provides means to provide and map enumerations (keys) between the API data model and a possible implementation.","title":"Key Management"},{"location":"#mongodb","text":"The module sda-commons-server-spring-data-mongo is used to work with MongoDB using Spring Data Mongo . The module sda-commons-server-mongo-testing provides a MongoDB instance for integration testing. The example package shows how to use the bundle within an application.","title":"MongoDB"},{"location":"#open-telemetry","text":"The module sda-commons-server-opentelemetry provides OpenTelemetry instrumentation for JAX-RS. Other bundles like sda-commons-client-jersey , sda-commons-server-spring-data-mongo or sda-commons-server-s3 come with built-in instrumentation. Besides instrumentation, it's also required to specify a collector, like Jaeger . The module sda-commons-server-opentelemetry-example shows how to use OpenTelemetry and Jaeger within an application and has examples for manual instrumentation.","title":"Open Telemetry"},{"location":"#prometheus","text":"The module sda-commons-server-prometheus provides an admin endpoint to serve metrics in a format that Prometheus can read. The module sda-commons-server-prometheus-example presents a simple application that shows the three main types of metrics to use in a service.","title":"Prometheus"},{"location":"#s3-object-storage","text":"The module sda-commons-server-s3 provides a client for an AWS S3-compatible object storage. The module sda-commons-server-s3-testing is used to provide an AWS S3-compatible Object Storage during integrations tests.","title":"S3 Object Storage"},{"location":"#security","text":"The module sda-commons-server-security helps to configure a secure Dropwizard application.","title":"Security"},{"location":"#shared-certificates","text":"The module sda-commons-shared-certificates adds support for trusting custom certificate authorities.","title":"Shared Certificates"},{"location":"#trace-token","text":"The module sda-commons-server-trace adds support to track create a trace token to correlate a set of service invocations that belongs to the same logically cohesive call of a higher level service offered by the SDA Platform, e.g. interaction service. .","title":"Trace Token"},{"location":"#weld","text":"The module sda-commons-server-weld is used to bootstrap Dropwizard applications inside a Weld-SE container and provides CDI support for servlets, listeners and resources. The module sda-commons-server-weld-example gives a small example on starting an application within a Weld container.","title":"Weld"},{"location":"#yaml","text":"The module sda-commons-shared-yaml adds support for YAML-file handling.","title":"YAML"},{"location":"#client","text":"All modules prefixed with sda-commons-client- provide support for applications that use an HTTP client to access other services.","title":"Client"},{"location":"#jersey","text":"The module sda-commons-client-jersey provides support for using Jersey clients withing the Dropwizard application. The module sda-commons-client-jersey-wiremock-testing bundles the WireMock dependencies to mock services in integration tests consistently to sda-commons library versions. The module sda-commons-client-jersey-example presents an example application that shows how to invoke services.","title":"Jersey"},{"location":"#usage","text":"The compiled releases are publicly available via maven central . Include sda-commons-bom and sda-commons-dependencies as platform constraints. You will inherit all versions defined there and won't have to specify versions for them yourself. More details: - sda-commons-bom - sda-commons-dependencies Note: You need Gradle 5.x for platform dependencies. More information can be found here . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 project.ext { sdaCommonsVersion = 'x.x.x' } dependencies { // define platform dependencies for simplified dependency management compile enforcedPlatform(\"org.sdase.commons.sda-commons-dependencies:$sdaCommonsVersion\") compile enforcedPlatform(\"org.sdase.commons.sda-commons-bom:$sdaCommonsVersion\") ... // Add dependencies to sda-commons-modules (managed by sda-commons-bom) compile \"org.sdase.commons:sda-commons-client-jersey\" ... // Add other dependencies (managed by 'sda-commons-dependencies') compile 'org.glassfish.jersey.core:jersey-client' // Add other unmanaged dependencies compile 'org.apache.commons:commons-digester3:3.2' }","title":"Usage"},{"location":"bom/","text":"SDA Commons BOM \u00b6 Gradle define a mechanism that developers can leverage to align the versions of dependencies that belong to the same framework, or an umbrella of dependencies that need to be aligned to work well together. Using them will prevent version conflicts and aide you figure out which dependency versions work well with each other. This mechanism is called Bill of Materials (BOM). The module sda-commons-bom defines the versions of sda-commons modules.","title":"Bom"},{"location":"bom/#sda-commons-bom","text":"Gradle define a mechanism that developers can leverage to align the versions of dependencies that belong to the same framework, or an umbrella of dependencies that need to be aligned to work well together. Using them will prevent version conflicts and aide you figure out which dependency versions work well with each other. This mechanism is called Bill of Materials (BOM). The module sda-commons-bom defines the versions of sda-commons modules.","title":"SDA Commons BOM"},{"location":"client-jersey-example/","text":"SDA Commons Client Jersey Example \u00b6 This example module shows an application that uses the client-jersey bundle to invoke another service. Beside the initialization of the bundle, it includes the generation of three different clients: * Generic Platform client * Generic External client * External client using an interface that describes the API to generate a proxy * Generic External client with custom configurations The integration test shows how to set up a mock using the sda-commons-client-jersey-wiremock-testing module. Check the documentation for more details.","title":"Client Jersey Example"},{"location":"client-jersey-example/#sda-commons-client-jersey-example","text":"This example module shows an application that uses the client-jersey bundle to invoke another service. Beside the initialization of the bundle, it includes the generation of three different clients: * Generic Platform client * Generic External client * External client using an interface that describes the API to generate a proxy * Generic External client with custom configurations The integration test shows how to set up a mock using the sda-commons-client-jersey-wiremock-testing module. Check the documentation for more details.","title":"SDA Commons Client Jersey Example"},{"location":"client-jersey-wiremock-testing/","text":"SDA Commons Client Jersey WireMock Testing \u00b6 Testing dependencies for WireMock test framework. Extensions \u00b6 This module provides a Junit 5 extension to set up Wiremock for your tests. WireMockExtension \u00b6 Useful for setting up Wiremock for each one of your tests. Example: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 class WireMockExtensionTest { @RegisterExtension WireMockExtension wire = new WireMockExtension (); @BeforeEach void before () { wire . stubFor ( get ( \"/api/cars\" ) // NOSONAR . withHeader ( \"Accept\" , notMatching ( \"gzip\" )) . willReturn ( ok (). withHeader ( \"Content-type\" , \"application/json\" ). withBody ( \"[]\" ))); } // Tests } WireMockClassExtension \u00b6 Useful for setting up Wiremock once for your test class. Example: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 class WireMockClassExtensionTest { @RegisterExtension public static WireMockClassExtension wire = new WireMockClassExtension (); @BeforeAll public static void beforeAll () { wire . stubFor ( get ( \"/api/cars\" ) // NOSONAR . withHeader ( \"Accept\" , notMatching ( \"gzip\" )) . willReturn ( ok (). withHeader ( \"Content-type\" , \"application/json\" ). withBody ( \"[]\" ))); } // Tests }","title":"Client Jersey Wiremock Testing"},{"location":"client-jersey-wiremock-testing/#sda-commons-client-jersey-wiremock-testing","text":"Testing dependencies for WireMock test framework.","title":"SDA Commons Client Jersey WireMock Testing"},{"location":"client-jersey-wiremock-testing/#extensions","text":"This module provides a Junit 5 extension to set up Wiremock for your tests.","title":"Extensions"},{"location":"client-jersey-wiremock-testing/#wiremockextension","text":"Useful for setting up Wiremock for each one of your tests. Example: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 class WireMockExtensionTest { @RegisterExtension WireMockExtension wire = new WireMockExtension (); @BeforeEach void before () { wire . stubFor ( get ( \"/api/cars\" ) // NOSONAR . withHeader ( \"Accept\" , notMatching ( \"gzip\" )) . willReturn ( ok (). withHeader ( \"Content-type\" , \"application/json\" ). withBody ( \"[]\" ))); } // Tests }","title":"WireMockExtension"},{"location":"client-jersey-wiremock-testing/#wiremockclassextension","text":"Useful for setting up Wiremock once for your test class. Example: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 class WireMockClassExtensionTest { @RegisterExtension public static WireMockClassExtension wire = new WireMockClassExtension (); @BeforeAll public static void beforeAll () { wire . stubFor ( get ( \"/api/cars\" ) // NOSONAR . withHeader ( \"Accept\" , notMatching ( \"gzip\" )) . willReturn ( ok (). withHeader ( \"Content-type\" , \"application/json\" ). withBody ( \"[]\" ))); } // Tests }","title":"WireMockClassExtension"},{"location":"client-jersey/","text":"SDA Commons Client Jersey \u00b6 The module sda-commons-client-jersey provides support for using Jersey clients within the Dropwizard application. Usage \u00b6 The JerseyClientBundle must be added to the application. It provides a ClientFactory to create clients. The ClientFactory needs to be initialized and be available in the run(...) phase. Therefore, the bundle should be declared as field and not in the initialize method. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 public class MyApplication extends Application < MyConfiguration > { private JerseyClientBundle jerseyClientBundle = JerseyClientBundle . builder (). build (); public static void main ( final String [] args ) { new MyApplication (). run ( args ); } @Override public void initialize ( Bootstrap < MyConfiguration > bootstrap ) { // ... bootstrap . addBundle ( jerseyClientBundle ); // ... } @Override public void run ( MyConfiguration configuration , Environment environment ) { // ... ClientFactory clientFactory = jerseyClientBundle . getClientFactory (); } } The ClientFactory is able to create Jersey clients from interfaces defining the API with JAX-RS annotations. It may also create generic Jersey clients that can build the request definition with a fluent API: 1 2 3 4 Client googleClient = clientFactory . externalClient () . buildGenericClient ( \"google\" ) . target ( \"https://maps.google.com\" ); Response response = googleClient . path ( \"api\" ) /* ... */ . get (); Configuration \u00b6 All clients defined with the ClientFactory can be built either for the SDA Platform or for external services. Both SDA Platform and external clients are OpenTelemetry enabled. Clients for the SDA Platform have some magic added that clients for an external service don't have: Trace-Token SDA Platform clients always add the Trace-Token of the current incoming request context to the headers of the outgoing request. If no incoming request context is found, a new unique Trace-Token is generated for each request. It will be discarded when the outgoing request completes. Authorization SDA Platform clients can be configured to pass through the Authorization header of an incoming request context: 1 2 3 4 clientFactory . platformClient () . enableAuthenticationPassThrough () . api ( OtherSdaServiceClient . class ) . atTarget ( \"http://other-sda-service.sda.net/api\" ); - Consumer-Token SDA Platform clients are able to send a Consumer-Token header to identify the caller. The token that is used has to be configured and published to the bundle. Currently, the token is only the name of the consumer. 1 2 private JerseyClientBundle jerseyClientBundle = JerseyClientBundle . builder () . withConsumerTokenProvider ( MyConfiguration :: getConsumerToken ). build (); Then the clients can be configured to add a Consumer-Token header: 1 2 3 4 clientFactory . platformClient () . enableConsumerToken () . api ( OtherSdaServiceClient . class ) . atTarget ( \"http://other-sda-service.sda.net/api\" ); Writing API Clients as interfaces \u00b6 Client interfaces use the same annotations as the service definitions for REST endpoints. An example is the MockApiClient in the integration tests of this module. Error handling is different based on the return type: If a specific return type is defined (e.g. List<MyResource> getMyResources(); ), it is only returned for successful requests. In any error or redirect case, an exception is thrown. The thrown exception is a ClientRequestException wrapping jakarta.ws.rs.ProcessingException or subclasses of jakarta.ws.rs.WebApplicationException : - jakarta.ws.rs.RedirectionException to indicate redirects - jakarta.ws.rs.ClientErrorException for client errors - jakarta.ws.rs.ServerErrorException for server errors If the ClientRequestException exception is handled in the application code the application must close() the exception . If a jakarta.ws.rs.core.Response is defined as return type, HTTP errors and redirects can be read from the Response object. Remember to always close the Response object. It references open socket streams. In both variants a java.net.ConnectException may be thrown if the client can't connect to the server. Using Jersey Client \u00b6 Jersey Clients can be built using the client factory for cases where the API variant with an interface is not suitable. Jersey clients can not automatically convert jakarta.ws.rs.WebApplicationException into our ClientRequestException . To avoid passing through the error the application received to the caller of the application, the exceptions must be handled for all usages that expect a specific type as return value. The ClientErrorUtil can be used to convert the exceptions. In the following example, a 4xx or 5xx response will result in a ClientRequestException that causes a 500 response for the incoming request: 1 2 3 Client client = clientFactory . platformClient (). buildGenericClient ( \"test\" ) Invocation . Builder requestBuilder = client . target ( BASE_URL ). request ( MediaType . APPLICATION_JSON ); MyResource myResource = ClientErrorUtil . convertExceptions (() -> requestBuilder . get ( MyResource . class )); If the error should be handled in the application, the exception may be caught and the error can be read from the response. If the ClientRequestException is caught the implementation must close() it . If it is not handled or rethrown, the ClientRequestExceptionMapper will take care about closing the underlying open socket. Therefore, the ClientRequestException must not be wrapped as cause in another exception! 1 2 3 4 5 6 7 8 9 Client client = clientFactory . platformClient (). buildGenericClient ( \"test\" ) Invocation . Builder requestBuilder = client . target ( BASE_URL ). request ( MediaType . APPLICATION_JSON ); try { MyResource myResource = ClientErrorUtil . convertExceptions (() -> requestBuilder . get ( MyResource . class )); } catch ( ClientRequestException e ) { ApiError error = ClientErrorUtil . readErrorBody ( e ); e . close (); } Multipart Support \u00b6 To support sending multipart requests like file uploads, sda-commons-shared-forms has to be added to the project. The client is the configured automatically to support multipart. Concurrency \u00b6 If you plan to use the Jersey client from another thread, note that the authorization from the request context and the trace token are missing. This can cause issues. You can use ContainerRequestContextHolder.transferRequestContext to transfer the request context and MDC to another thread. 1 2 3 executorService . submit ( transferRequestContext (() -> { // MDC.get(\"Trace-Token\") returns the same value as the parent thread })). get (); HTTP Client Configuration and Proxy Support \u00b6 Each client can be configured with the standard Dropwizard configuration . Please note that this requires that there is a property in your Application's Configuration class. 1 2 3 4 5 6 7 8 9 10 11 12 13 import org.sdase.commons.client.jersey.HttpClientConfiguration ; public class MyConfiguration extends Configuration { private HttpClientConfiguration myClient = new HttpClientConfiguration (); public HttpClientConfiguration getMyClient () { return myClient ; } public void setMyClient ( HttpClientConfiguration myClient ) { this . myClient = myClient ; } } 1 Client client = clientFactory . platformClient ( configuration . getMyClient ()). buildGenericClient ( \"test\" ); 1 2 3 4 5 6 myClient : timeout : 500ms proxy : host : 192.168.52.11 port : 8080 scheme : http Tip: There is no need to make all configuration properties available as environment variables. Seldomly used properties can always be configured using System Properties . This configuration can be used to configure a proxy server if needed. Use this if all clients should use individual proxy configurations. In addition, each client can consume the standard proxy system properties . Please note that a specific proxy configuration in the HttpClientConfiguration disables the proxy system properties for the client using that configuration. This can be helpful when all clients in an Application should use the same proxy configuration (this includes the clients that are used by the sda-commons-server-auth bundle ). OIDC Client \u00b6 This module also provides support for requesting OIDC access tokens from respective providers (e.g. Keycloak) for subsequent requests to other services. Usage \u00b6 The OidcClient can be instantiated with a Jersey ClientFactory and the respectable OidcConfiguration . The client also implements caching that is configured according to the access tokens expiration time claim. 1 2 3 OidcClient oidcClient = new OidcClient ( jerseyClientBundle . getClientFactory (), configuration . getOidc ()); OidcResult oidcResult = oidcClient . createAccessToken (); String accessToken = oidcResult . getAccessToken (); An easy way to integrate the OidcClient into existing PlatformClients is by using the provided OidcRequestFilter and passing it to the PlatformClientBuilder: 1 2 3 4 5 6 7 8 9 10 OidcRequestFilter oidcRequestFilter = new OidcRequestFilter ( jerseyClientBundle . getClientFactory (), configuration . getOidc (), true ); ExternalServiceClient client = jerseyClientBundle . getClientFactory () . platformClient ( configuration . getAuditableArchiveHttpClient ()) . addFilter ( oidcRequestFilter ) . api ( ExternalServiceClient . class ) . atTarget ( externalServiceUrl ); Configuration \u00b6 The oidc client is configured in the config.yaml of the application. Example config for production to be used with environment variables of the cluster configuration: 1 2 3 4 5 6 7 8 9 10 oidc : disabled : ${OIDC_DISABLED:-false} grantType : ${OIDC_GRANT_TYPE:-client_credentials} clientId : ${OIDC_CLIENT_ID} clientSecret : ${OIDC_CLIENT_SECRET} username : ${OIDC_USERNAME} password : ${OIDC_PASSWORD} issuerUrl : ${OIDC_ISSUER_URL} cache : disabled : ${OIDC_CACHE_DISABLED:-false} OIDC_DISABLED Disable retrieving a new token completely. Not meant for production! Example: false OIDC_GRANT_TYPE Sets the OIDC grant type. Default: client_credentials Supported values: client_credentials Deprecated : password OIDC_CLIENT_ID The client id. Example: client_id OIDC_CLIENT_SECRET The client secret. Example: s3cr3t OIDC_USERNAME Deprecated : A username. Only used for grant type 'password'. Example: john OIDC_PASSWORD Deprecated : A password. Only used for grant type 'password'. Example: pa$$word OIDC_ISSUER_URL Contains the URL to the OpenID provider configuration document. Example: https://<keycloak.domain>/auth/realms/<realm> OIDC_CACHE_DISABLED Disable the caching of retrieved tokens. Default: false The grant type password is no longer allowed in OAuth2/2.1 and the support for it in this module will be deprecated at some point. It is highly recommended to use the grant type client_credentials . Tips and Tricks \u00b6 Basic Authentication \u00b6 In order to call http endpoints which require a Basic Authentication header set you can register the org.glassfish.jersey.client.authentication.HttpAuthenticationFeature using the JerseyClientBuilder. 1 2 3 4 5 6 jerseyClientBundle . getClientFactory () . externalClient () . addFeature ( HttpAuthenticationFeature . basic ( \"foo\" , \"bar\" )) . api ( ApiA . class ) . atTarget ( apiABaseUrl ); 3 rd Party jakarta.ws.rs-api Client Implementations in Classpath \u00b6 The clients used in sda-commons require the Jersey Client implementation. If you are facing problems with other jakarta.ws.rs-api implementations in the classpath (e.g. RestEasy which comes with the Keycloak SDK) the Jersey Client Builder must be propagated in your project as service. Therefore, the service definition src/main/resources/META-INF/services/jakarta.ws.rs.client.ClientBuilder must be added to your project containing: 1 org.glassfish.jersey.client.JerseyClientBuilder This works if the library that requires the other implementation does not rely on the Java ServiceLoader. Consume API clients without catching exceptions \u00b6 The Suppress* annotations can be used to suppress specific ClientRequestException thrown in any defined API method where the return type is not a Response . There are configurable suppressors for HTTP errors , connection timeouts , read timeouts and all other processing errors . Suppressed exceptions are logged at info level. The following client will convert a 404 Not Found error to null so that the consumer can handle an entity that is not found by checking if the return value is null . Any other error will lead to a 500 Internal Server Error because the ClientRequestException is converted in the ClientRequestExceptionMapper . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 @Path ( \"/examples\" ) public interface ExampleClient { @GET @Path ( \"/{exampleId}\" ) @Produces ( MediaType . APPLICATION_JSON ) @SuppressHttpErrorsToNull ( 404 ) Example getExample ( @PathParam ( \"exampleId\" ) String exampleId ); } public class ExampleService { private ExampleClient exampleClient ; private DoSomethingService doSomethingService ; public void doSomethingIfExampleExists ( String exampleId ) { Example example = exampleClient . getExample ( exampleId ); if ( example != null ) { doSomethingService ( example ); } } }","title":"Client Jersey"},{"location":"client-jersey/#sda-commons-client-jersey","text":"The module sda-commons-client-jersey provides support for using Jersey clients within the Dropwizard application.","title":"SDA Commons Client Jersey"},{"location":"client-jersey/#usage","text":"The JerseyClientBundle must be added to the application. It provides a ClientFactory to create clients. The ClientFactory needs to be initialized and be available in the run(...) phase. Therefore, the bundle should be declared as field and not in the initialize method. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 public class MyApplication extends Application < MyConfiguration > { private JerseyClientBundle jerseyClientBundle = JerseyClientBundle . builder (). build (); public static void main ( final String [] args ) { new MyApplication (). run ( args ); } @Override public void initialize ( Bootstrap < MyConfiguration > bootstrap ) { // ... bootstrap . addBundle ( jerseyClientBundle ); // ... } @Override public void run ( MyConfiguration configuration , Environment environment ) { // ... ClientFactory clientFactory = jerseyClientBundle . getClientFactory (); } } The ClientFactory is able to create Jersey clients from interfaces defining the API with JAX-RS annotations. It may also create generic Jersey clients that can build the request definition with a fluent API: 1 2 3 4 Client googleClient = clientFactory . externalClient () . buildGenericClient ( \"google\" ) . target ( \"https://maps.google.com\" ); Response response = googleClient . path ( \"api\" ) /* ... */ . get ();","title":"Usage"},{"location":"client-jersey/#configuration","text":"All clients defined with the ClientFactory can be built either for the SDA Platform or for external services. Both SDA Platform and external clients are OpenTelemetry enabled. Clients for the SDA Platform have some magic added that clients for an external service don't have: Trace-Token SDA Platform clients always add the Trace-Token of the current incoming request context to the headers of the outgoing request. If no incoming request context is found, a new unique Trace-Token is generated for each request. It will be discarded when the outgoing request completes. Authorization SDA Platform clients can be configured to pass through the Authorization header of an incoming request context: 1 2 3 4 clientFactory . platformClient () . enableAuthenticationPassThrough () . api ( OtherSdaServiceClient . class ) . atTarget ( \"http://other-sda-service.sda.net/api\" ); - Consumer-Token SDA Platform clients are able to send a Consumer-Token header to identify the caller. The token that is used has to be configured and published to the bundle. Currently, the token is only the name of the consumer. 1 2 private JerseyClientBundle jerseyClientBundle = JerseyClientBundle . builder () . withConsumerTokenProvider ( MyConfiguration :: getConsumerToken ). build (); Then the clients can be configured to add a Consumer-Token header: 1 2 3 4 clientFactory . platformClient () . enableConsumerToken () . api ( OtherSdaServiceClient . class ) . atTarget ( \"http://other-sda-service.sda.net/api\" );","title":"Configuration"},{"location":"client-jersey/#writing-api-clients-as-interfaces","text":"Client interfaces use the same annotations as the service definitions for REST endpoints. An example is the MockApiClient in the integration tests of this module. Error handling is different based on the return type: If a specific return type is defined (e.g. List<MyResource> getMyResources(); ), it is only returned for successful requests. In any error or redirect case, an exception is thrown. The thrown exception is a ClientRequestException wrapping jakarta.ws.rs.ProcessingException or subclasses of jakarta.ws.rs.WebApplicationException : - jakarta.ws.rs.RedirectionException to indicate redirects - jakarta.ws.rs.ClientErrorException for client errors - jakarta.ws.rs.ServerErrorException for server errors If the ClientRequestException exception is handled in the application code the application must close() the exception . If a jakarta.ws.rs.core.Response is defined as return type, HTTP errors and redirects can be read from the Response object. Remember to always close the Response object. It references open socket streams. In both variants a java.net.ConnectException may be thrown if the client can't connect to the server.","title":"Writing API Clients as interfaces"},{"location":"client-jersey/#using-jersey-client","text":"Jersey Clients can be built using the client factory for cases where the API variant with an interface is not suitable. Jersey clients can not automatically convert jakarta.ws.rs.WebApplicationException into our ClientRequestException . To avoid passing through the error the application received to the caller of the application, the exceptions must be handled for all usages that expect a specific type as return value. The ClientErrorUtil can be used to convert the exceptions. In the following example, a 4xx or 5xx response will result in a ClientRequestException that causes a 500 response for the incoming request: 1 2 3 Client client = clientFactory . platformClient (). buildGenericClient ( \"test\" ) Invocation . Builder requestBuilder = client . target ( BASE_URL ). request ( MediaType . APPLICATION_JSON ); MyResource myResource = ClientErrorUtil . convertExceptions (() -> requestBuilder . get ( MyResource . class )); If the error should be handled in the application, the exception may be caught and the error can be read from the response. If the ClientRequestException is caught the implementation must close() it . If it is not handled or rethrown, the ClientRequestExceptionMapper will take care about closing the underlying open socket. Therefore, the ClientRequestException must not be wrapped as cause in another exception! 1 2 3 4 5 6 7 8 9 Client client = clientFactory . platformClient (). buildGenericClient ( \"test\" ) Invocation . Builder requestBuilder = client . target ( BASE_URL ). request ( MediaType . APPLICATION_JSON ); try { MyResource myResource = ClientErrorUtil . convertExceptions (() -> requestBuilder . get ( MyResource . class )); } catch ( ClientRequestException e ) { ApiError error = ClientErrorUtil . readErrorBody ( e ); e . close (); }","title":"Using Jersey Client"},{"location":"client-jersey/#multipart-support","text":"To support sending multipart requests like file uploads, sda-commons-shared-forms has to be added to the project. The client is the configured automatically to support multipart.","title":"Multipart Support"},{"location":"client-jersey/#concurrency","text":"If you plan to use the Jersey client from another thread, note that the authorization from the request context and the trace token are missing. This can cause issues. You can use ContainerRequestContextHolder.transferRequestContext to transfer the request context and MDC to another thread. 1 2 3 executorService . submit ( transferRequestContext (() -> { // MDC.get(\"Trace-Token\") returns the same value as the parent thread })). get ();","title":"Concurrency"},{"location":"client-jersey/#http-client-configuration-and-proxy-support","text":"Each client can be configured with the standard Dropwizard configuration . Please note that this requires that there is a property in your Application's Configuration class. 1 2 3 4 5 6 7 8 9 10 11 12 13 import org.sdase.commons.client.jersey.HttpClientConfiguration ; public class MyConfiguration extends Configuration { private HttpClientConfiguration myClient = new HttpClientConfiguration (); public HttpClientConfiguration getMyClient () { return myClient ; } public void setMyClient ( HttpClientConfiguration myClient ) { this . myClient = myClient ; } } 1 Client client = clientFactory . platformClient ( configuration . getMyClient ()). buildGenericClient ( \"test\" ); 1 2 3 4 5 6 myClient : timeout : 500ms proxy : host : 192.168.52.11 port : 8080 scheme : http Tip: There is no need to make all configuration properties available as environment variables. Seldomly used properties can always be configured using System Properties . This configuration can be used to configure a proxy server if needed. Use this if all clients should use individual proxy configurations. In addition, each client can consume the standard proxy system properties . Please note that a specific proxy configuration in the HttpClientConfiguration disables the proxy system properties for the client using that configuration. This can be helpful when all clients in an Application should use the same proxy configuration (this includes the clients that are used by the sda-commons-server-auth bundle ).","title":"HTTP Client Configuration and Proxy Support"},{"location":"client-jersey/#oidc-client","text":"This module also provides support for requesting OIDC access tokens from respective providers (e.g. Keycloak) for subsequent requests to other services.","title":"OIDC Client"},{"location":"client-jersey/#usage_1","text":"The OidcClient can be instantiated with a Jersey ClientFactory and the respectable OidcConfiguration . The client also implements caching that is configured according to the access tokens expiration time claim. 1 2 3 OidcClient oidcClient = new OidcClient ( jerseyClientBundle . getClientFactory (), configuration . getOidc ()); OidcResult oidcResult = oidcClient . createAccessToken (); String accessToken = oidcResult . getAccessToken (); An easy way to integrate the OidcClient into existing PlatformClients is by using the provided OidcRequestFilter and passing it to the PlatformClientBuilder: 1 2 3 4 5 6 7 8 9 10 OidcRequestFilter oidcRequestFilter = new OidcRequestFilter ( jerseyClientBundle . getClientFactory (), configuration . getOidc (), true ); ExternalServiceClient client = jerseyClientBundle . getClientFactory () . platformClient ( configuration . getAuditableArchiveHttpClient ()) . addFilter ( oidcRequestFilter ) . api ( ExternalServiceClient . class ) . atTarget ( externalServiceUrl );","title":"Usage"},{"location":"client-jersey/#configuration_1","text":"The oidc client is configured in the config.yaml of the application. Example config for production to be used with environment variables of the cluster configuration: 1 2 3 4 5 6 7 8 9 10 oidc : disabled : ${OIDC_DISABLED:-false} grantType : ${OIDC_GRANT_TYPE:-client_credentials} clientId : ${OIDC_CLIENT_ID} clientSecret : ${OIDC_CLIENT_SECRET} username : ${OIDC_USERNAME} password : ${OIDC_PASSWORD} issuerUrl : ${OIDC_ISSUER_URL} cache : disabled : ${OIDC_CACHE_DISABLED:-false} OIDC_DISABLED Disable retrieving a new token completely. Not meant for production! Example: false OIDC_GRANT_TYPE Sets the OIDC grant type. Default: client_credentials Supported values: client_credentials Deprecated : password OIDC_CLIENT_ID The client id. Example: client_id OIDC_CLIENT_SECRET The client secret. Example: s3cr3t OIDC_USERNAME Deprecated : A username. Only used for grant type 'password'. Example: john OIDC_PASSWORD Deprecated : A password. Only used for grant type 'password'. Example: pa$$word OIDC_ISSUER_URL Contains the URL to the OpenID provider configuration document. Example: https://<keycloak.domain>/auth/realms/<realm> OIDC_CACHE_DISABLED Disable the caching of retrieved tokens. Default: false The grant type password is no longer allowed in OAuth2/2.1 and the support for it in this module will be deprecated at some point. It is highly recommended to use the grant type client_credentials .","title":"Configuration"},{"location":"client-jersey/#tips-and-tricks","text":"","title":"Tips and Tricks"},{"location":"client-jersey/#basic-authentication","text":"In order to call http endpoints which require a Basic Authentication header set you can register the org.glassfish.jersey.client.authentication.HttpAuthenticationFeature using the JerseyClientBuilder. 1 2 3 4 5 6 jerseyClientBundle . getClientFactory () . externalClient () . addFeature ( HttpAuthenticationFeature . basic ( \"foo\" , \"bar\" )) . api ( ApiA . class ) . atTarget ( apiABaseUrl );","title":"Basic Authentication"},{"location":"client-jersey/#3rd-party-jakartawsrs-api-client-implementations-in-classpath","text":"The clients used in sda-commons require the Jersey Client implementation. If you are facing problems with other jakarta.ws.rs-api implementations in the classpath (e.g. RestEasy which comes with the Keycloak SDK) the Jersey Client Builder must be propagated in your project as service. Therefore, the service definition src/main/resources/META-INF/services/jakarta.ws.rs.client.ClientBuilder must be added to your project containing: 1 org.glassfish.jersey.client.JerseyClientBuilder This works if the library that requires the other implementation does not rely on the Java ServiceLoader.","title":"3rd Party jakarta.ws.rs-api Client Implementations in Classpath"},{"location":"client-jersey/#consume-api-clients-without-catching-exceptions","text":"The Suppress* annotations can be used to suppress specific ClientRequestException thrown in any defined API method where the return type is not a Response . There are configurable suppressors for HTTP errors , connection timeouts , read timeouts and all other processing errors . Suppressed exceptions are logged at info level. The following client will convert a 404 Not Found error to null so that the consumer can handle an entity that is not found by checking if the return value is null . Any other error will lead to a 500 Internal Server Error because the ClientRequestException is converted in the ClientRequestExceptionMapper . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 @Path ( \"/examples\" ) public interface ExampleClient { @GET @Path ( \"/{exampleId}\" ) @Produces ( MediaType . APPLICATION_JSON ) @SuppressHttpErrorsToNull ( 404 ) Example getExample ( @PathParam ( \"exampleId\" ) String exampleId ); } public class ExampleService { private ExampleClient exampleClient ; private DoSomethingService doSomethingService ; public void doSomethingIfExampleExists ( String exampleId ) { Example example = exampleClient . getExample ( exampleId ); if ( example != null ) { doSomethingService ( example ); } } }","title":"Consume API clients without catching exceptions"},{"location":"dependencies/","text":"SDA Commons Dependencies \u00b6 The module sda-commons-dependencies defines the versions of sda-commons' dependencies. This concept is mostly known as either platform or \"bill of materials\". Usage \u00b6 Gradle 5 required! Services using sda-commons should import sda-commons-dependencies to make sure to use the same dependencies. Add the following code to your build.gradle : 1 2 3 dependencies { compile enforcedPlatform(\"org.sdase.commons.sda-commons-dependencies:$sdaCommonsVersion\") } After that you can use all dependencies that were already declared in sda-commons-dependencies without declaring the version.","title":"Dependencies"},{"location":"dependencies/#sda-commons-dependencies","text":"The module sda-commons-dependencies defines the versions of sda-commons' dependencies. This concept is mostly known as either platform or \"bill of materials\".","title":"SDA Commons Dependencies"},{"location":"dependencies/#usage","text":"Gradle 5 required! Services using sda-commons should import sda-commons-dependencies to make sure to use the same dependencies. Add the following code to your build.gradle : 1 2 3 dependencies { compile enforcedPlatform(\"org.sdase.commons.sda-commons-dependencies:$sdaCommonsVersion\") } After that you can use all dependencies that were already declared in sda-commons-dependencies without declaring the version.","title":"Usage"},{"location":"metadata-context/","text":"The Metadata Context \u00b6 Purpose \u00b6 The metadata context keeps contextual information of a business process across all involved microservices. The information that is stored in the metadata context is solely defined by the environment. Defining a metadata context for an environment is optional. If no context is defined, no metadata will be available for any service. The metadata should not affect the core business logic of a service but can be used to affect how a service communicates with the user (e.g. communication methods, templates or domains used for links) or how statistical data is evaluated. As the metadata is tracked for a whole business process across all involved services, it supports that services which are not in a synchronous call chain, can behave differently based on the entry point. All services in between and the service that uses information from the metadata context don't need to clutter their API with non-functional information, that may be different in each environment, to support features a service in the end of a process provides based on the initial user request. A service must be able to fulfill its purpose without any information from the metadata context. A service may use information from the metadata context to use or apply different business related or non-functional configuration for a specific business process but must have a default for these configurations as well. The metadata context is technically transferred via HTTP headers and Kafka message headers between services in the same environment. At runtime, it is held in a thread local variable to be available for the current process. The example below explains what kind of problems the metadata context can solve. Configuration and structure \u00b6 A metadata context consists of well-defined fields. They must be defined equally for all services in the same environment. Services based on this library use the environment variable METADATA_FIELDS as comma separated list of field names automatically. Field names are used as HTTP and Kafka message header keys. The fields must not conflict with officially defined header keys. The data type of the value is always text, represented as String in this library. Multiple values are supported for each field in a specific context. To simplify parsing, according to RFC 9110 5.2 , values must not contain commas. Creating metadata context information \u00b6 A metadata context for a specific business process should be initialized as early as possible. Technically the context is created by passing the headers of defined metadata fields to a service that supports the metadata context. No standard component should create a metadata context initially, because the metadata context always depends on the environment. The easiest way to initially create a metadata context is to add the respective headers in a proxy that exposes a service, e.g. an Ingress in Kubernetes. The proxy may derive the metadata information from the used domain, query params, user agent headers and other information available in the request. If getting the metadata information is more complex, an environment specific interaction service (according to the SDA service architecture) may be implemented and create the context programmatically. This service should act like a proxy and forward the request to a standard business service. Services that recreate a context for a process that was interrupted, use the context they stored along with the entity to activate it for current thread. Support metadata context in a service \u00b6 Any service that communicates synchronously with the HTTP platform clients provided by this library and asynchronously with Kafka consumers and producers initialized by this library and has no persistence layer most likely supports the metadata context. Exceptions may apply if business processes are interrupted or asynchronously processed without saving business data in a database. Services that persist data and therefore interrupt the business process until the data is loaded into memory again can support the metadata context if they store it along with the entity and recreate the runtime context when the entity is loaded and processed. Service that interrupts a process in memory or proceeds asynchronously must transfer the metadata context to the new thread to support the metadata context. Services that fulfill these requirements should mention the support of the metadata context in their deployment documentation. They should also mention the configuration environment variable METADATA_FIELDS that is automatically available for all services based on this library. Services that read information from the metadata context shall not use hard coded field names. They must not expect that specific fields are available in an environment. The environment, technically the operations team, is in the lead to define which metadata is available. A service only knows a specific purpose for which it supports metadata information. Whenever a service supports decisions based on metadata configuration, it must provide a configuration option which naming is based on the purpose to let the operations team define which metadata field should be used. The library supports to derive the actual field names from environment variables. The values are dependent on the environment as well. Therefore, the service must provide configuration options to map values of the metadata field to values it understands. Supporting information from the metadata context to distinguish the behavior of the service can be very complex due to the loose coupling and the high flexibility for the owners of the environment. Every configuration that can be modified by information of the metadata context must have a default as well, because the service may be used in an environment without configured metadata context or in a specific business process is no context available. How a service behaves and is configured when metadata context is available must be described in the deployment documentation to make this complex flexibility manageable by the operations team. Example: Template selection \u00b6 Imagine a business case that is advertised on different landing pages, where the user enters information and receives a result asynchronously via email. The email sent to the user should match the style of the landing page. Each landing page is available on a dedicated domain but all lead to the same backend services. The services may communicate asynchronously or even need additional information from the backoffice team to create the result. The operations team defines the METADATA_FIELDS as landing-page-source for all services in the environment to identify the landing page where the user entered their data. They configure the Ingress, to set either christmas-special or summer-edition as landing-page-source header - based on the domain of two landing pages. No service but the one that is generating the email is affected by this information. The core business rules that determine the result only need the information the user entered. The email content can be created independently as well. Only the service that renders the HTML part of the email must be aware of the source of the request. Therefore, it must have the ability to use multiple templates. Besides other optional input to determine the template to use, it must be configurable to use information from the metadata context. To keep the flexibility the metadata context offers, it defines the environment variable TEMPLATE_HINT_METADATA_FIELD . The operations team configures it as landing-page-source . The service developer can use TEMPLATE_HINT_METADATA_FIELD in the MetadataContext API to get the values of current context, but must be aware that the result is empty or contains multiple values. Additionally, the email templating service must provide configuration options to select a template based on the value as well as providing the templates. The templates and the configuration could be mounted as files in the container. The latter may be a yaml file and look like this: 1 2 3 4 5 6 7 # templates.yaml default : /mounts/templates/default.hbs byMetadata : - metadataValue : christmas-special templateFile : /mounts/templates/xmas.hbs - metadataValue : summer-edition templateFile : /mounts/templates/summer.hbs Variant of template selection \u00b6 The email example is very flexible and would support other environments as well, e.g. when A/B testing is used to find the perfect color for the use case. The email service is not involved in the evaluation of the A/B test, but has to send emails that match the styles of the A/B test. Such user tests will most likely be evaluated by dedicated UX tools and not by the backend services in the environment. No business service should be affected by this information. However, a service creating statistics for the use case could support the metadata context as well and provide additional evaluation for each variant. The frontend must provide any information in the request to distinguish between A and B. Then the proxy can convert the information in a header that is sent to the backend service, e.g. ab-variant=neon . The operations team of this environment will configure METADATA_FIELDS for all services and TEMPLATE_HINT_METADATA_FIELD as ab-variant and the email templating like this: 1 2 3 4 5 6 7 # templates.yaml default : /mounts/templates/default.hbs byMetadata : - metadataValue : pastel templateFile : /mounts/templates/pastel.hbs - metadataValue : neon templateFile : /mounts/templates/neon.hbs","title":"Metadata Context"},{"location":"metadata-context/#the-metadata-context","text":"","title":"The Metadata Context"},{"location":"metadata-context/#purpose","text":"The metadata context keeps contextual information of a business process across all involved microservices. The information that is stored in the metadata context is solely defined by the environment. Defining a metadata context for an environment is optional. If no context is defined, no metadata will be available for any service. The metadata should not affect the core business logic of a service but can be used to affect how a service communicates with the user (e.g. communication methods, templates or domains used for links) or how statistical data is evaluated. As the metadata is tracked for a whole business process across all involved services, it supports that services which are not in a synchronous call chain, can behave differently based on the entry point. All services in between and the service that uses information from the metadata context don't need to clutter their API with non-functional information, that may be different in each environment, to support features a service in the end of a process provides based on the initial user request. A service must be able to fulfill its purpose without any information from the metadata context. A service may use information from the metadata context to use or apply different business related or non-functional configuration for a specific business process but must have a default for these configurations as well. The metadata context is technically transferred via HTTP headers and Kafka message headers between services in the same environment. At runtime, it is held in a thread local variable to be available for the current process. The example below explains what kind of problems the metadata context can solve.","title":"Purpose"},{"location":"metadata-context/#configuration-and-structure","text":"A metadata context consists of well-defined fields. They must be defined equally for all services in the same environment. Services based on this library use the environment variable METADATA_FIELDS as comma separated list of field names automatically. Field names are used as HTTP and Kafka message header keys. The fields must not conflict with officially defined header keys. The data type of the value is always text, represented as String in this library. Multiple values are supported for each field in a specific context. To simplify parsing, according to RFC 9110 5.2 , values must not contain commas.","title":"Configuration and structure"},{"location":"metadata-context/#creating-metadata-context-information","text":"A metadata context for a specific business process should be initialized as early as possible. Technically the context is created by passing the headers of defined metadata fields to a service that supports the metadata context. No standard component should create a metadata context initially, because the metadata context always depends on the environment. The easiest way to initially create a metadata context is to add the respective headers in a proxy that exposes a service, e.g. an Ingress in Kubernetes. The proxy may derive the metadata information from the used domain, query params, user agent headers and other information available in the request. If getting the metadata information is more complex, an environment specific interaction service (according to the SDA service architecture) may be implemented and create the context programmatically. This service should act like a proxy and forward the request to a standard business service. Services that recreate a context for a process that was interrupted, use the context they stored along with the entity to activate it for current thread.","title":"Creating metadata context information"},{"location":"metadata-context/#support-metadata-context-in-a-service","text":"Any service that communicates synchronously with the HTTP platform clients provided by this library and asynchronously with Kafka consumers and producers initialized by this library and has no persistence layer most likely supports the metadata context. Exceptions may apply if business processes are interrupted or asynchronously processed without saving business data in a database. Services that persist data and therefore interrupt the business process until the data is loaded into memory again can support the metadata context if they store it along with the entity and recreate the runtime context when the entity is loaded and processed. Service that interrupts a process in memory or proceeds asynchronously must transfer the metadata context to the new thread to support the metadata context. Services that fulfill these requirements should mention the support of the metadata context in their deployment documentation. They should also mention the configuration environment variable METADATA_FIELDS that is automatically available for all services based on this library. Services that read information from the metadata context shall not use hard coded field names. They must not expect that specific fields are available in an environment. The environment, technically the operations team, is in the lead to define which metadata is available. A service only knows a specific purpose for which it supports metadata information. Whenever a service supports decisions based on metadata configuration, it must provide a configuration option which naming is based on the purpose to let the operations team define which metadata field should be used. The library supports to derive the actual field names from environment variables. The values are dependent on the environment as well. Therefore, the service must provide configuration options to map values of the metadata field to values it understands. Supporting information from the metadata context to distinguish the behavior of the service can be very complex due to the loose coupling and the high flexibility for the owners of the environment. Every configuration that can be modified by information of the metadata context must have a default as well, because the service may be used in an environment without configured metadata context or in a specific business process is no context available. How a service behaves and is configured when metadata context is available must be described in the deployment documentation to make this complex flexibility manageable by the operations team.","title":"Support metadata context in a service"},{"location":"metadata-context/#example-template-selection","text":"Imagine a business case that is advertised on different landing pages, where the user enters information and receives a result asynchronously via email. The email sent to the user should match the style of the landing page. Each landing page is available on a dedicated domain but all lead to the same backend services. The services may communicate asynchronously or even need additional information from the backoffice team to create the result. The operations team defines the METADATA_FIELDS as landing-page-source for all services in the environment to identify the landing page where the user entered their data. They configure the Ingress, to set either christmas-special or summer-edition as landing-page-source header - based on the domain of two landing pages. No service but the one that is generating the email is affected by this information. The core business rules that determine the result only need the information the user entered. The email content can be created independently as well. Only the service that renders the HTML part of the email must be aware of the source of the request. Therefore, it must have the ability to use multiple templates. Besides other optional input to determine the template to use, it must be configurable to use information from the metadata context. To keep the flexibility the metadata context offers, it defines the environment variable TEMPLATE_HINT_METADATA_FIELD . The operations team configures it as landing-page-source . The service developer can use TEMPLATE_HINT_METADATA_FIELD in the MetadataContext API to get the values of current context, but must be aware that the result is empty or contains multiple values. Additionally, the email templating service must provide configuration options to select a template based on the value as well as providing the templates. The templates and the configuration could be mounted as files in the container. The latter may be a yaml file and look like this: 1 2 3 4 5 6 7 # templates.yaml default : /mounts/templates/default.hbs byMetadata : - metadataValue : christmas-special templateFile : /mounts/templates/xmas.hbs - metadataValue : summer-edition templateFile : /mounts/templates/summer.hbs","title":"Example: Template selection"},{"location":"metadata-context/#variant-of-template-selection","text":"The email example is very flexible and would support other environments as well, e.g. when A/B testing is used to find the perfect color for the use case. The email service is not involved in the evaluation of the A/B test, but has to send emails that match the styles of the A/B test. Such user tests will most likely be evaluated by dedicated UX tools and not by the backend services in the environment. No business service should be affected by this information. However, a service creating statistics for the use case could support the metadata context as well and provide additional evaluation for each variant. The frontend must provide any information in the request to distinguish between A and B. Then the proxy can convert the information in a header that is sent to the backend service, e.g. ab-variant=neon . The operations team of this environment will configure METADATA_FIELDS for all services and TEMPLATE_HINT_METADATA_FIELD as ab-variant and the email templating like this: 1 2 3 4 5 6 7 # templates.yaml default : /mounts/templates/default.hbs byMetadata : - metadataValue : pastel templateFile : /mounts/templates/pastel.hbs - metadataValue : neon templateFile : /mounts/templates/neon.hbs","title":"Variant of template selection"},{"location":"server-auth-testing/","text":"SDA Commons Server Auth Testing \u00b6 This module provides support for testing applications that are secured with sda-commons-server-auth . To use the support, this module has to be added as dependency: 1 testCompile 'org.sdase.commons:sda-commons-server-auth-testing:<current-version>' Auth Extension \u00b6 The AuthClassExtension puts the AuthConfig in an environment variable named AUTH_KEYS (for backwards compatibility). The configuration in the test needs to use this property and the application is required to use the ConfigurationSubstitutionBundle from sda-commons-server-dropwizard : 1 2 3 4 5 6 7 8 9 10 11 12 13 public class MyApp extends Application < MyConfig > { @Override public void initialize ( Bootstrap < MyConfig > bootstrap ) { bootstrap . addBundle ( ConfigurationSubstitutionBundle . builder (). build ()); bootstrap . addBundle ( AuthBundle . builder (). withAuthConfigProvider ( MyConfig :: getAuth ). build ()); } @Override public void run ( MyConfig configuration , Environment environment ) { // ... } } 1 2 3 4 5 6 7 8 9 10 11 # test-config.yaml server : applicationConnectors : - type : http port : 0 adminConnectors : - type : http port : 0 # The configuration of the test auth bundle is injected here auth : ${AUTH_CONFIG_KEYS} To implement the test, the AuthClassExtension has to be initialized before the DropwizardAppExtension : 1 2 3 4 5 6 7 8 9 10 11 12 13 class AuthClassExtensionIT { @Order ( 0 ) @RegisterExtension static final AuthClassExtension AUTH = AuthClassExtension . builder (). build (); @Order ( 1 ) @RegisterExtension static final DropwizardAppExtension < AuthTestConfig > DW = new DropwizardAppExtension <> ( AuthTestApp . class , ResourceHelpers . resourceFilePath ( \"test-config.yaml\" )); // @Test } The AuthClassExtension provides functions to generate a valid token that matches to the auth configuration in tests. 1 2 3 4 5 6 7 8 Response response = createWebTarget () . path ( \"/secure\" ) . request ( APPLICATION_JSON ) . headers ( AUTH . auth () . addClaim ( \"test\" , \"testClaim\" ) . addClaims ( singletonMap ( \"mapKey\" , \"testClaimFromMap\" )) . buildAuthHeader ()) // creates a valid Authorization header with a valid JWT . get (); Examples can be found in the test source branch of the module sda-commons-server-auth-testing. There is An example app A test with authentication A test with disabled authentication An appropriate test config.yaml OPA Extension \u00b6 The Junit 5 OPA Extension is built around WireMock. The mock can be configured via the extension. To implement a test with an OPA Mock, the OpaClassExtension has to be initialized before DropwizardAppExtension implicitly by field declaration order or explicitly with a @Order(N) . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 public class OpaIT { @Order ( 0 ) @RegisterExtension static final OpaClassExtension OPA_EXTENSION = new OpaClassExtension (); @Order ( 1 ) @RegisterExtension static final DropwizardAppExtension < OpaBundeTestAppConfiguration > DW = new DropwizardAppExtension <> ( OpaBundleTestApp . class , ResourceHelpers . resourceFilePath ( \"test-opa-config.yaml\" ), ConfigOverride . config ( \"opa.baseUrl\" , OPA_EXTENSION :: getUrl )); // @Test } To control the OPA mock behavior, the following API is provided 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 // allow access to a given httpMethod/path combination OPA_EXTENSION . mock ( onRequest (). withHttpMethod ( httpMethod ). withPath ( path ). allow ()); // allow access to a given httpMethod/path/jwt combination OPA_EXTENSION . mock ( onRequest (). withHttpMethod ( httpMethod ). withPath ( path ). withJwt ( jwt ). allow ()); // deny access to a given httpMethod/path combination OPA_EXTENSION . mock ( onRequest (). withHttpMethod ( httpMethod ). withPath ( path ). deny ()); // allow access to a given httpMethod/path combination with constraint OPA_EXTENSION . mock ( onRequest (). withHttpMethod ( httpMethod ). withPath ( path ). allow (). withConstraint ( new ConstraintModel (...))); // the response is returned for all requests, if no more specific mock is configured OPA_EXTENSION . mock ( onAnyRequest (). answer ( new OpaResponse (...))); // the same options are available for any requests if no more specific mock is configured OPA_EXTENSION . mock ( onAnyRequest (). allow ()); OPA_EXTENSION . mock ( onAnyRequest (). answer ( new OpaResponse (...))); // It is possible to verify of the OPA has been invoked with parameters for the resource // defined by the path and the httpMethod verify ( int count , String httpMethod , String path ) // it is also possible to check against a builder instance OPA_EXTENSION . verify ( 1 , onRequest (). withHttpMethod ( httpMethod ). withPath ( path ). withJwt ( jwt )); Examples can be found in the test source branch of the module sda-commons-server-auth-testing. There is An example app A test with OPA A test with disabled OPA support . In this case, only empty constraints are within the principal An appropriate test config.yaml Example with activated AUTH and OPA bundle can be found here: - Example app - Test","title":"Server Auth Testing"},{"location":"server-auth-testing/#sda-commons-server-auth-testing","text":"This module provides support for testing applications that are secured with sda-commons-server-auth . To use the support, this module has to be added as dependency: 1 testCompile 'org.sdase.commons:sda-commons-server-auth-testing:<current-version>'","title":"SDA Commons Server Auth Testing"},{"location":"server-auth-testing/#auth-extension","text":"The AuthClassExtension puts the AuthConfig in an environment variable named AUTH_KEYS (for backwards compatibility). The configuration in the test needs to use this property and the application is required to use the ConfigurationSubstitutionBundle from sda-commons-server-dropwizard : 1 2 3 4 5 6 7 8 9 10 11 12 13 public class MyApp extends Application < MyConfig > { @Override public void initialize ( Bootstrap < MyConfig > bootstrap ) { bootstrap . addBundle ( ConfigurationSubstitutionBundle . builder (). build ()); bootstrap . addBundle ( AuthBundle . builder (). withAuthConfigProvider ( MyConfig :: getAuth ). build ()); } @Override public void run ( MyConfig configuration , Environment environment ) { // ... } } 1 2 3 4 5 6 7 8 9 10 11 # test-config.yaml server : applicationConnectors : - type : http port : 0 adminConnectors : - type : http port : 0 # The configuration of the test auth bundle is injected here auth : ${AUTH_CONFIG_KEYS} To implement the test, the AuthClassExtension has to be initialized before the DropwizardAppExtension : 1 2 3 4 5 6 7 8 9 10 11 12 13 class AuthClassExtensionIT { @Order ( 0 ) @RegisterExtension static final AuthClassExtension AUTH = AuthClassExtension . builder (). build (); @Order ( 1 ) @RegisterExtension static final DropwizardAppExtension < AuthTestConfig > DW = new DropwizardAppExtension <> ( AuthTestApp . class , ResourceHelpers . resourceFilePath ( \"test-config.yaml\" )); // @Test } The AuthClassExtension provides functions to generate a valid token that matches to the auth configuration in tests. 1 2 3 4 5 6 7 8 Response response = createWebTarget () . path ( \"/secure\" ) . request ( APPLICATION_JSON ) . headers ( AUTH . auth () . addClaim ( \"test\" , \"testClaim\" ) . addClaims ( singletonMap ( \"mapKey\" , \"testClaimFromMap\" )) . buildAuthHeader ()) // creates a valid Authorization header with a valid JWT . get (); Examples can be found in the test source branch of the module sda-commons-server-auth-testing. There is An example app A test with authentication A test with disabled authentication An appropriate test config.yaml","title":"Auth Extension"},{"location":"server-auth-testing/#opa-extension","text":"The Junit 5 OPA Extension is built around WireMock. The mock can be configured via the extension. To implement a test with an OPA Mock, the OpaClassExtension has to be initialized before DropwizardAppExtension implicitly by field declaration order or explicitly with a @Order(N) . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 public class OpaIT { @Order ( 0 ) @RegisterExtension static final OpaClassExtension OPA_EXTENSION = new OpaClassExtension (); @Order ( 1 ) @RegisterExtension static final DropwizardAppExtension < OpaBundeTestAppConfiguration > DW = new DropwizardAppExtension <> ( OpaBundleTestApp . class , ResourceHelpers . resourceFilePath ( \"test-opa-config.yaml\" ), ConfigOverride . config ( \"opa.baseUrl\" , OPA_EXTENSION :: getUrl )); // @Test } To control the OPA mock behavior, the following API is provided 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 // allow access to a given httpMethod/path combination OPA_EXTENSION . mock ( onRequest (). withHttpMethod ( httpMethod ). withPath ( path ). allow ()); // allow access to a given httpMethod/path/jwt combination OPA_EXTENSION . mock ( onRequest (). withHttpMethod ( httpMethod ). withPath ( path ). withJwt ( jwt ). allow ()); // deny access to a given httpMethod/path combination OPA_EXTENSION . mock ( onRequest (). withHttpMethod ( httpMethod ). withPath ( path ). deny ()); // allow access to a given httpMethod/path combination with constraint OPA_EXTENSION . mock ( onRequest (). withHttpMethod ( httpMethod ). withPath ( path ). allow (). withConstraint ( new ConstraintModel (...))); // the response is returned for all requests, if no more specific mock is configured OPA_EXTENSION . mock ( onAnyRequest (). answer ( new OpaResponse (...))); // the same options are available for any requests if no more specific mock is configured OPA_EXTENSION . mock ( onAnyRequest (). allow ()); OPA_EXTENSION . mock ( onAnyRequest (). answer ( new OpaResponse (...))); // It is possible to verify of the OPA has been invoked with parameters for the resource // defined by the path and the httpMethod verify ( int count , String httpMethod , String path ) // it is also possible to check against a builder instance OPA_EXTENSION . verify ( 1 , onRequest (). withHttpMethod ( httpMethod ). withPath ( path ). withJwt ( jwt )); Examples can be found in the test source branch of the module sda-commons-server-auth-testing. There is An example app A test with OPA A test with disabled OPA support . In this case, only empty constraints are within the principal An appropriate test config.yaml Example with activated AUTH and OPA bundle can be found here: - Example app - Test","title":"OPA Extension"},{"location":"server-auth/","text":"SDA Commons Server Auth \u00b6 This module provides an AuthBundle to authenticate users based on JSON Web Tokens and an OpaBundle to authorize the request with help of the Open Policy Agent . To use the bundle, a dependency to this module has to be added: 1 compile 'org.sdase.commons:sda-commons-server-auth:<current-version>' Auth Bundle \u00b6 The authentication creates a JwtPrincipal per request. This can be accessed from the SecurityContext : 1 2 3 4 5 6 7 8 9 10 11 12 13 @PermitAll @Path ( \"/secure\" ) public class SecureEndPoint { @Context private SecurityContext securityContext ; @GET public Response getSomethingSecure () { JwtPrincipal jwtPrincipal = ( JwtPrincipal ) securityContext . getUserPrincipal (); // ... } } To activate the authentication in an application, the bundle has to be added to the application: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 public class MyApplication extends Application < MyConfiguration > { public static void main ( final String [] args ) { new MyApplication (). run ( args ); } @Override public void initialize ( Bootstrap < MyConfiguration > bootstrap ) { // ... bootstrap . addBundle ( AuthBundle . builder (). withAuthConfigProvider ( MyConfiguration :: getAuth ). withAnnotatedAuthorization (). build ()); // ... } @Override public void run ( MyConfiguration configuration , Environment environment ) { // ... } } The Bundle can be configured to perform basic authorization based on whether a token is presented or not on endpoints that are annotated with @PermitAll with the setting .withAnnotatedAuthorization() . If the authorization should be handled by e.g. the OpaBundle , the option .withExternalAuthorization() still validates tokens sent in the Authorization header but also accepts requests without header to be processed and eventually rejected in a later stage. Configuration \u00b6 The configuration relies on the config.yaml of the application and the custom property where the AuthConfig is mapped. Usually this should be auth . 1 2 3 4 5 public class MyConfig extends Configuration { private AuthConfig auth ; // getters and setters } The config allows to set the leeway in seconds for validation of exp and nbf . Multiple sources for public keys for verification of the signature can be configured. Each source may refer to a certificate in PEM format an authentication provider root URL or a URL providing a JSON Web Key Set The authentication can be disabled for use in test and development environments. Be careful to NEVER disable authentication in production. For the authentication provider root URL and a jwks source a required issuer can be configured by the attribute requiredIssuer . If the attribute requiredIssuer is set, the issuer of the token must match to the provided required issuer. A warning will be logged if the host name of the source url does not match the host name of the required issuer. Example config: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 auth : # Disable all authentication, should be NEVER true in production disableAuth : false # The accepted leeway in seconds: leeway : 2 # Definition of key sources providing public keys to verify signed tokens. keys : # A public key derived from a local PEM certificate. # The pemKeyId must match the 'kid' the signing authority sets in the token. # It may be null if the 'kid' in the token is not set by the signing authority. # The pemKeyId is only considered for type: PEM - type : PEM location : file:///secure/example.pem pemKeyId : example # A public key derived from a PEM certificate that is provided from a http server # The pemKeyId must match the 'kid' the signing authority sets in the token. # It may be null if the 'kid' in the token is not set by the signing authority. # The pemSignAlg must match the signing algorithm used in the PEM. Defaults to RS256. # The pemKeyId and pemSignAlg is only considered for type: PEM - type : PEM location : http://example.com/keys/example.pem pemKeyId : example.com pemSignAlg : RS256 # Public keys will be loaded from the OpenID provider using discovery. - type : OPEN_ID_DISCOVERY location : https://keycloak.example.com/auth/realms/my-realm requiredIssuer : https://keycloak.example.com/auth/realms/my-realm - # Public keys will be loaded directly from the JWKS url of the OpenID provider. type : JWKS location : https://keycloak.example.com/auth/realms/my-realm/protocol/openid-connect/certs requiredIssuer : https://keycloak.example.com/auth/realms/my-realm # Comma separated string of OPEN_ID_DISCOVERY key sources with required issuer. Can be used to # shorten the configuration when the discovery base URL matches the iss claim, the IDP sets. # The value used for configuration here must exactly match the iss claim. # keys and issuers can be used at the same time. Both are added to the accepted key sources. issuers : \"https://keycloak.example.com/auth/realms/my-realm, https://keycloak.example.com/auth/realms/my-other-realm\" The config may be filled from environment variables if the ConfigurationSubstitutionBundle is used: 1 2 3 4 5 auth : disableAuth : ${DISABLE_AUTH:-false} leeway : ${AUTH_LEEWAY:-0} keys : ${AUTH_KEYS:-[]} issuers : \"${AUTH_ISSUERS:-}\" In this case, the AUTH_KEYS variable should contain a JSON array of KeyLocation objects: 1 2 3 4 5 6 7 8 9 10 [ { \"type\" : \"OPEN_ID_DISCOVERY\" , \"location\" : \"https://keycloak.example.com/auth/realms/my-realm\" }, { \"type\" : \"OPEN_ID_DISCOVERY\" , \"location\" : \"https://keycloak.example.com/auth/realms/my-other-realm\" } ] Periodical reloading of public keys provided via JWKS \u00b6 Public keys provided via a JSON Web Key Set (JWKS) are stored in a local key-cache. The cache updates every 5 minutes. It will also update when a JWT with unknown kid must be validated. No changes apply when the JWKS is unreachable. To avoid acceptance of tokens signed by revoked keys, all keys not available in the JWKS are removed on update. HTTP Client Configuration and Proxy Support \u00b6 The client that calls the OpenID Discovery endpoint or the JWKS url, is configurable with the standard Dropwizard configuration . Tip: There is no need to make all configuration properties available as environment variables. Seldomly used properties can always be configured using System Properties . 1 2 3 4 5 6 7 auth : keyLoaderClient : timeout : 500ms proxy : host : 192.168.52.11 port : 8080 scheme : http This configuration can be used to configure a proxy server if needed. Use this if all clients should use an individual proxy configuration. In addition, the client consumes the standard proxy system properties . Please note that a specific proxy configuration in the HttpClientConfiguration disables the proxy system properties for the client using that configuration. This can be helpful when all clients in an Application should use the same proxy configuration (this includes all clients that are created by the sda-commons-client-jersey bundle ). OPA Bundle \u00b6 Details about the authorization with Open Policy Agent are documented within the authorization concept (see Confluence). In short, Open Policy Agent acts as policy decision point and is started as sidecar to the actual service. The OPA Bundle acts as a client to the Open Policy Agent and is hooked in as request filter handling requests after they have been validated by the JwtAuthenticator and before they reach the service implementation. The OPA Bundle requires the AuthBundle in place, so that the JWT can be verified against public keys before it is handed over to OPA. In case the request does not contain a JWT token at all, the JWT verification in the AuthBundle will be skipped without error and further checks should be part of the OPA policy. If it is still required to reject requests that do not contain a JWT token the AuthBundle needs to be configured to perform basic authorization. This is achieved by setting .withAnnotatedAuthorization() on the AuhtBundle and annotate endpoints with @PermitAll . The OPA bundle requests the policy decision providing the following inputs * HTTP path as Array * HTTP method as String * validated JWT (if available) * all request headers (can be disabled in the OpaBundle builder) Remark to HTTP request headers: The bundle normalizes header names to lower case to simplify handling in OPA since HTTP specification defines header names as case-insensitive. Multivalued headers are not normalized with respect to the representation as list or single string with separator char. They are forwarded as parsed by the framework. Security note: Please be aware while a service might only consider one value of a specific header, the OPA is able to authorize on an array of those. Consider this in your policy when you want to make sure you authorize on the same value that a service might use to evaluate the output. These inputs can be accessed inside a policy .rego -file in this way: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 # each policy lies in a package that is referenced in the configuration of the OpaBundle package example # decode the JWT as new variable 'token' token = {\"payload\": payload} { not input.jwt == null io.jwt.decode(input.jwt, [_, payload, _]) } # deny by default default allow = false allow { # allow if path match '/contracts/:anyid' input.path = [\"contracts\", _] # allow if request method 'GET' is used input.httpMethod == \"GET\" # allow if 'claim' exists in the JWT payload token.payload.claim # allow if a request header 'HttpRequestHeaderName' has a certain value input.headers[\"httprequestheadername\"][_] == \"certain-value\" } # set some example constraints constraint1 := true # always true constraint2 := [ \"v2.1\", \"v2.2\" ] # always an array of \"v2.1\" and \"v2.2\" constraint3[token.payload.sub]. # always a set that contains the 'sub' claim from the token # or is empty if no token is present The response consists of two parts: The overall allow decision, and optional rules that represent constraints to limit data access within the service. These constraints are fully service dependent and MUST be applied when querying the database or filtering received data. The following listing presents a sample OPA result with a positive allow decision and two constraints, the first with boolean value and second with a list of string values. 1 2 3 4 5 6 7 8 { \"result\" : { \"allow\" : true , \"constraint1\" : true , \"constraint2\" : [ \"v2.1\" , \"v2.2\" ], \"constraint3\" : [ \"my-sub\" ] } } The following listing shows a corresponding model class to the example above: 1 2 3 4 5 6 7 8 9 10 public class ConstraintModel { private boolean constraint1 ; private List < String > constraint2 ; // could also be a Set<String> private List < String > constraint3 ; } The bundle creates a OpaJwtPrincipal for each request. You can retrieve the constraint model's data from the principal by invoking OpaJwtPrincipal#getConstraintsAsEntity . Data from an JwtPrincipal is copied to the new principal if existing. Beside the JWT, the constraints are included in this principal. The OpaJwtPrincipal includes a method to parse the constraints JSON string to a Java object. The OpaJwtPrincipal can be injected as field using @Context in request scoped beans like endpoint implementations or accessed from the SecurityContext . 1 2 3 4 5 6 7 8 9 10 11 @Path ( \"/secure\" ) public class SecureEndPoint { @Context private OpaJwtPrincipal opaJwtPrincipal ; @GET public Response getSomethingSecure () { // ... } } To activate the OPA integration in an application, the bundle has to be added to the application. The Kubernetes file of the service must also be adjusted to start OPA as sidecar. If you use the SdaPlatformBundle, there is a more convenient way to enable OPA support . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 public class MyApplication extends Application < MyConfiguration > { public static void main ( final String [] args ) { new MyApplication (). run ( args ); } @Override public void initialize ( Bootstrap < MyConfiguration > bootstrap ) { // ... bootstrap . addBundle ( OpaBundle . builder (). withOpaConfigProvider ( OpaBundeTestAppConfiguration :: getOpaConfig ). build ()); // ... } @Override public void run ( MyConfiguration configuration , Environment environment ) { // ... } } Configuration \u00b6 The configuration relies on the config.yaml of the application and the custom property where the OpaConfig is mapped. Usually this should be opa . 1 2 3 4 5 public class MyConfig extends Configuration { private OpaConfig opa ; // getters and setters } The config includes the connection data to the OPA sidecar and the path to the used policy endpoint. Example config: 1 2 3 4 5 6 7 8 9 10 11 12 opa : # Disable authorization. An empty prinicpal is created with an empty set of constraints disableOpa : false # Url to the OPA sidecar baseUrl : http://localhost:8181 # Package name of the policy file that should be evaluated for authorization decision # The package name is used to resolve the full path policyPackage : http.authz # Advanced configuration of the HTTP client that is used to call the Open Policy Agent opaClient : # timeout for OPA requests, default 500ms timeout : 500ms The config may be filled from environment variables if the ConfigurationSubstitutionBundle is used: 1 2 3 4 opa : disableOpa : ${DISABLE_OPA:-false} baseUrl : ${OPA_URL:-http://localhost:8181} policyPackage : ${OPA_POLICY_PACKAGE} HTTP Client Configuration and Proxy Support \u00b6 The client that calls the Open Policy Agent is configurable with the standard Dropwizard configuration . Tip: There is no need to make all configuration properties available as environment variables. Seldomly used properties can always be configured using System Properties . 1 2 3 opa : opaClient : timeout : 500ms This configuration can be used to configure a proxy server if needed. Please note that this client does not consume the standard proxy system properties but needs to be configured manually! The OPA should be deployed as near to the service as possible, so we don't expect the need for universal proxy settings. Testing \u00b6 sda-commons-server-auth-testing provides support for testing applications with authentication. Input Extensions \u00b6 The Bundle offers the option to register extensions that send custom data to the Open Policy Agent to be accessed during policy execution. Such extensions implement the OpaInputExtension interface and are registered in a dedicated namespace. The extension is called in the OpaAuthFilter and is able to access the current RequestContext to for example extract additional data from the request. Overriding existing input properties ( path , jwt , httpMethod ) is not possible. This extension option should only be used if the normal authorization via constraints is not powerful enough. In general, a custom extension should not be necessary for most use cases. Remark on accessing the request body in an input extension: The extension gets the ContainerRequestContext as input to access information about the request. Please be aware to not access the request entity since this might break your service. A test that shows the erroneous behavior can be found in OpaBundleBodyInputExtensionTest.java Security note: When creating new extensions, be aware that you might access properties of a request that are not yet validated in the method interface. This is especially important if your service expects single values that might be accessible as array value to the OPA. While the service (e.g. in case of query parameters) only considers the first value, make sure to not authorize on other values in the OPA. The following listing shows an example extension that adds a fixed boolean entry: 1 2 3 4 5 6 public class ExampleOpaInputExtension implements OpaInputExtension < Boolean > { @Override public Boolean createAdditionalInputContent ( ContainerRequestContext requestContext ) { return true ; } } Register the extension during the OpaBundle creation: 1 2 3 4 OpaBundle . builder () . withOpaConfigProvider ( YourConfiguration :: getOpa ) . withInputExtension ( \"myExtension\" , new ExampleOpaInputExtension ()) . build (); Access the additional input inside a policy .rego -file in this way: 1 2 3 4 5 6 7 exampleExtensionWorks { # check if path match '/contracts' input.path = [\"contracts\"] # check if the custom input has a certain value input.myExtension == true }","title":"Server Auth"},{"location":"server-auth/#sda-commons-server-auth","text":"This module provides an AuthBundle to authenticate users based on JSON Web Tokens and an OpaBundle to authorize the request with help of the Open Policy Agent . To use the bundle, a dependency to this module has to be added: 1 compile 'org.sdase.commons:sda-commons-server-auth:<current-version>'","title":"SDA Commons Server Auth"},{"location":"server-auth/#auth-bundle","text":"The authentication creates a JwtPrincipal per request. This can be accessed from the SecurityContext : 1 2 3 4 5 6 7 8 9 10 11 12 13 @PermitAll @Path ( \"/secure\" ) public class SecureEndPoint { @Context private SecurityContext securityContext ; @GET public Response getSomethingSecure () { JwtPrincipal jwtPrincipal = ( JwtPrincipal ) securityContext . getUserPrincipal (); // ... } } To activate the authentication in an application, the bundle has to be added to the application: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 public class MyApplication extends Application < MyConfiguration > { public static void main ( final String [] args ) { new MyApplication (). run ( args ); } @Override public void initialize ( Bootstrap < MyConfiguration > bootstrap ) { // ... bootstrap . addBundle ( AuthBundle . builder (). withAuthConfigProvider ( MyConfiguration :: getAuth ). withAnnotatedAuthorization (). build ()); // ... } @Override public void run ( MyConfiguration configuration , Environment environment ) { // ... } } The Bundle can be configured to perform basic authorization based on whether a token is presented or not on endpoints that are annotated with @PermitAll with the setting .withAnnotatedAuthorization() . If the authorization should be handled by e.g. the OpaBundle , the option .withExternalAuthorization() still validates tokens sent in the Authorization header but also accepts requests without header to be processed and eventually rejected in a later stage.","title":"Auth Bundle"},{"location":"server-auth/#configuration","text":"The configuration relies on the config.yaml of the application and the custom property where the AuthConfig is mapped. Usually this should be auth . 1 2 3 4 5 public class MyConfig extends Configuration { private AuthConfig auth ; // getters and setters } The config allows to set the leeway in seconds for validation of exp and nbf . Multiple sources for public keys for verification of the signature can be configured. Each source may refer to a certificate in PEM format an authentication provider root URL or a URL providing a JSON Web Key Set The authentication can be disabled for use in test and development environments. Be careful to NEVER disable authentication in production. For the authentication provider root URL and a jwks source a required issuer can be configured by the attribute requiredIssuer . If the attribute requiredIssuer is set, the issuer of the token must match to the provided required issuer. A warning will be logged if the host name of the source url does not match the host name of the required issuer. Example config: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 auth : # Disable all authentication, should be NEVER true in production disableAuth : false # The accepted leeway in seconds: leeway : 2 # Definition of key sources providing public keys to verify signed tokens. keys : # A public key derived from a local PEM certificate. # The pemKeyId must match the 'kid' the signing authority sets in the token. # It may be null if the 'kid' in the token is not set by the signing authority. # The pemKeyId is only considered for type: PEM - type : PEM location : file:///secure/example.pem pemKeyId : example # A public key derived from a PEM certificate that is provided from a http server # The pemKeyId must match the 'kid' the signing authority sets in the token. # It may be null if the 'kid' in the token is not set by the signing authority. # The pemSignAlg must match the signing algorithm used in the PEM. Defaults to RS256. # The pemKeyId and pemSignAlg is only considered for type: PEM - type : PEM location : http://example.com/keys/example.pem pemKeyId : example.com pemSignAlg : RS256 # Public keys will be loaded from the OpenID provider using discovery. - type : OPEN_ID_DISCOVERY location : https://keycloak.example.com/auth/realms/my-realm requiredIssuer : https://keycloak.example.com/auth/realms/my-realm - # Public keys will be loaded directly from the JWKS url of the OpenID provider. type : JWKS location : https://keycloak.example.com/auth/realms/my-realm/protocol/openid-connect/certs requiredIssuer : https://keycloak.example.com/auth/realms/my-realm # Comma separated string of OPEN_ID_DISCOVERY key sources with required issuer. Can be used to # shorten the configuration when the discovery base URL matches the iss claim, the IDP sets. # The value used for configuration here must exactly match the iss claim. # keys and issuers can be used at the same time. Both are added to the accepted key sources. issuers : \"https://keycloak.example.com/auth/realms/my-realm, https://keycloak.example.com/auth/realms/my-other-realm\" The config may be filled from environment variables if the ConfigurationSubstitutionBundle is used: 1 2 3 4 5 auth : disableAuth : ${DISABLE_AUTH:-false} leeway : ${AUTH_LEEWAY:-0} keys : ${AUTH_KEYS:-[]} issuers : \"${AUTH_ISSUERS:-}\" In this case, the AUTH_KEYS variable should contain a JSON array of KeyLocation objects: 1 2 3 4 5 6 7 8 9 10 [ { \"type\" : \"OPEN_ID_DISCOVERY\" , \"location\" : \"https://keycloak.example.com/auth/realms/my-realm\" }, { \"type\" : \"OPEN_ID_DISCOVERY\" , \"location\" : \"https://keycloak.example.com/auth/realms/my-other-realm\" } ]","title":"Configuration"},{"location":"server-auth/#periodical-reloading-of-public-keys-provided-via-jwks","text":"Public keys provided via a JSON Web Key Set (JWKS) are stored in a local key-cache. The cache updates every 5 minutes. It will also update when a JWT with unknown kid must be validated. No changes apply when the JWKS is unreachable. To avoid acceptance of tokens signed by revoked keys, all keys not available in the JWKS are removed on update.","title":"Periodical reloading of public keys provided via JWKS"},{"location":"server-auth/#http-client-configuration-and-proxy-support","text":"The client that calls the OpenID Discovery endpoint or the JWKS url, is configurable with the standard Dropwizard configuration . Tip: There is no need to make all configuration properties available as environment variables. Seldomly used properties can always be configured using System Properties . 1 2 3 4 5 6 7 auth : keyLoaderClient : timeout : 500ms proxy : host : 192.168.52.11 port : 8080 scheme : http This configuration can be used to configure a proxy server if needed. Use this if all clients should use an individual proxy configuration. In addition, the client consumes the standard proxy system properties . Please note that a specific proxy configuration in the HttpClientConfiguration disables the proxy system properties for the client using that configuration. This can be helpful when all clients in an Application should use the same proxy configuration (this includes all clients that are created by the sda-commons-client-jersey bundle ).","title":"HTTP Client Configuration and Proxy Support"},{"location":"server-auth/#opa-bundle","text":"Details about the authorization with Open Policy Agent are documented within the authorization concept (see Confluence). In short, Open Policy Agent acts as policy decision point and is started as sidecar to the actual service. The OPA Bundle acts as a client to the Open Policy Agent and is hooked in as request filter handling requests after they have been validated by the JwtAuthenticator and before they reach the service implementation. The OPA Bundle requires the AuthBundle in place, so that the JWT can be verified against public keys before it is handed over to OPA. In case the request does not contain a JWT token at all, the JWT verification in the AuthBundle will be skipped without error and further checks should be part of the OPA policy. If it is still required to reject requests that do not contain a JWT token the AuthBundle needs to be configured to perform basic authorization. This is achieved by setting .withAnnotatedAuthorization() on the AuhtBundle and annotate endpoints with @PermitAll . The OPA bundle requests the policy decision providing the following inputs * HTTP path as Array * HTTP method as String * validated JWT (if available) * all request headers (can be disabled in the OpaBundle builder) Remark to HTTP request headers: The bundle normalizes header names to lower case to simplify handling in OPA since HTTP specification defines header names as case-insensitive. Multivalued headers are not normalized with respect to the representation as list or single string with separator char. They are forwarded as parsed by the framework. Security note: Please be aware while a service might only consider one value of a specific header, the OPA is able to authorize on an array of those. Consider this in your policy when you want to make sure you authorize on the same value that a service might use to evaluate the output. These inputs can be accessed inside a policy .rego -file in this way: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 # each policy lies in a package that is referenced in the configuration of the OpaBundle package example # decode the JWT as new variable 'token' token = {\"payload\": payload} { not input.jwt == null io.jwt.decode(input.jwt, [_, payload, _]) } # deny by default default allow = false allow { # allow if path match '/contracts/:anyid' input.path = [\"contracts\", _] # allow if request method 'GET' is used input.httpMethod == \"GET\" # allow if 'claim' exists in the JWT payload token.payload.claim # allow if a request header 'HttpRequestHeaderName' has a certain value input.headers[\"httprequestheadername\"][_] == \"certain-value\" } # set some example constraints constraint1 := true # always true constraint2 := [ \"v2.1\", \"v2.2\" ] # always an array of \"v2.1\" and \"v2.2\" constraint3[token.payload.sub]. # always a set that contains the 'sub' claim from the token # or is empty if no token is present The response consists of two parts: The overall allow decision, and optional rules that represent constraints to limit data access within the service. These constraints are fully service dependent and MUST be applied when querying the database or filtering received data. The following listing presents a sample OPA result with a positive allow decision and two constraints, the first with boolean value and second with a list of string values. 1 2 3 4 5 6 7 8 { \"result\" : { \"allow\" : true , \"constraint1\" : true , \"constraint2\" : [ \"v2.1\" , \"v2.2\" ], \"constraint3\" : [ \"my-sub\" ] } } The following listing shows a corresponding model class to the example above: 1 2 3 4 5 6 7 8 9 10 public class ConstraintModel { private boolean constraint1 ; private List < String > constraint2 ; // could also be a Set<String> private List < String > constraint3 ; } The bundle creates a OpaJwtPrincipal for each request. You can retrieve the constraint model's data from the principal by invoking OpaJwtPrincipal#getConstraintsAsEntity . Data from an JwtPrincipal is copied to the new principal if existing. Beside the JWT, the constraints are included in this principal. The OpaJwtPrincipal includes a method to parse the constraints JSON string to a Java object. The OpaJwtPrincipal can be injected as field using @Context in request scoped beans like endpoint implementations or accessed from the SecurityContext . 1 2 3 4 5 6 7 8 9 10 11 @Path ( \"/secure\" ) public class SecureEndPoint { @Context private OpaJwtPrincipal opaJwtPrincipal ; @GET public Response getSomethingSecure () { // ... } } To activate the OPA integration in an application, the bundle has to be added to the application. The Kubernetes file of the service must also be adjusted to start OPA as sidecar. If you use the SdaPlatformBundle, there is a more convenient way to enable OPA support . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 public class MyApplication extends Application < MyConfiguration > { public static void main ( final String [] args ) { new MyApplication (). run ( args ); } @Override public void initialize ( Bootstrap < MyConfiguration > bootstrap ) { // ... bootstrap . addBundle ( OpaBundle . builder (). withOpaConfigProvider ( OpaBundeTestAppConfiguration :: getOpaConfig ). build ()); // ... } @Override public void run ( MyConfiguration configuration , Environment environment ) { // ... } }","title":"OPA Bundle"},{"location":"server-auth/#configuration_1","text":"The configuration relies on the config.yaml of the application and the custom property where the OpaConfig is mapped. Usually this should be opa . 1 2 3 4 5 public class MyConfig extends Configuration { private OpaConfig opa ; // getters and setters } The config includes the connection data to the OPA sidecar and the path to the used policy endpoint. Example config: 1 2 3 4 5 6 7 8 9 10 11 12 opa : # Disable authorization. An empty prinicpal is created with an empty set of constraints disableOpa : false # Url to the OPA sidecar baseUrl : http://localhost:8181 # Package name of the policy file that should be evaluated for authorization decision # The package name is used to resolve the full path policyPackage : http.authz # Advanced configuration of the HTTP client that is used to call the Open Policy Agent opaClient : # timeout for OPA requests, default 500ms timeout : 500ms The config may be filled from environment variables if the ConfigurationSubstitutionBundle is used: 1 2 3 4 opa : disableOpa : ${DISABLE_OPA:-false} baseUrl : ${OPA_URL:-http://localhost:8181} policyPackage : ${OPA_POLICY_PACKAGE}","title":"Configuration"},{"location":"server-auth/#http-client-configuration-and-proxy-support_1","text":"The client that calls the Open Policy Agent is configurable with the standard Dropwizard configuration . Tip: There is no need to make all configuration properties available as environment variables. Seldomly used properties can always be configured using System Properties . 1 2 3 opa : opaClient : timeout : 500ms This configuration can be used to configure a proxy server if needed. Please note that this client does not consume the standard proxy system properties but needs to be configured manually! The OPA should be deployed as near to the service as possible, so we don't expect the need for universal proxy settings.","title":"HTTP Client Configuration and Proxy Support"},{"location":"server-auth/#testing","text":"sda-commons-server-auth-testing provides support for testing applications with authentication.","title":"Testing"},{"location":"server-auth/#input-extensions","text":"The Bundle offers the option to register extensions that send custom data to the Open Policy Agent to be accessed during policy execution. Such extensions implement the OpaInputExtension interface and are registered in a dedicated namespace. The extension is called in the OpaAuthFilter and is able to access the current RequestContext to for example extract additional data from the request. Overriding existing input properties ( path , jwt , httpMethod ) is not possible. This extension option should only be used if the normal authorization via constraints is not powerful enough. In general, a custom extension should not be necessary for most use cases. Remark on accessing the request body in an input extension: The extension gets the ContainerRequestContext as input to access information about the request. Please be aware to not access the request entity since this might break your service. A test that shows the erroneous behavior can be found in OpaBundleBodyInputExtensionTest.java Security note: When creating new extensions, be aware that you might access properties of a request that are not yet validated in the method interface. This is especially important if your service expects single values that might be accessible as array value to the OPA. While the service (e.g. in case of query parameters) only considers the first value, make sure to not authorize on other values in the OPA. The following listing shows an example extension that adds a fixed boolean entry: 1 2 3 4 5 6 public class ExampleOpaInputExtension implements OpaInputExtension < Boolean > { @Override public Boolean createAdditionalInputContent ( ContainerRequestContext requestContext ) { return true ; } } Register the extension during the OpaBundle creation: 1 2 3 4 OpaBundle . builder () . withOpaConfigProvider ( YourConfiguration :: getOpa ) . withInputExtension ( \"myExtension\" , new ExampleOpaInputExtension ()) . build (); Access the additional input inside a policy .rego -file in this way: 1 2 3 4 5 6 7 exampleExtensionWorks { # check if path match '/contracts' input.path = [\"contracts\"] # check if the custom input has a certain value input.myExtension == true }","title":"Input Extensions"},{"location":"server-circuit-breaker/","text":"SDA Commons Server Circuit Breaker \u00b6 This module provides the CircuitBreakerBundle , a Dropwizard bundle that is used to inject circuit breakers into service calls. A circuit breaker is a pattern to make synchronous calls in a distributed system more resilient. This is especially relevant, if a called service hangs without a response, as such failures would cascade up into other services and influence the overall stability of the system. This module doesn't provide an own implementation, but uses the circuit breaker from resilience4j . The bundle also provides prometheus metrics that can be exported in combination with sda-commons-server-prometheus . Usage \u00b6 To create a circuit breaker, register the circuit breaker bundle in the application: 1 2 3 4 5 6 7 8 9 10 11 private final CircuitBreakerBundle < AppConfiguration > circuitBreakerBundle = CircuitBreakerBundle . builder () . withDefaultConfig () . build (); @Override public void initialize ( Bootstrap < AppConfiguration > bootstrap ) { bootstrap . addBundle ( circuitBreakerBundle ); ... } As the configuration depends on the environment and the load on the service, a different configuration might be required. A custom configuration can be specified globally at the bundle: 1 2 3 4 private final CircuitBreakerBundle < AppConfiguration > circuitBreakerBundle = CircuitBreakerBundle . builder () . < AppConfiguration > withCustomConfig ( new CircuitBreakerConfiguration (). setFailureRateThreshold ( 50.0f )) . build (); It's also possible to load the configuration from the Dropwizard configuration: 1 2 3 4 private final CircuitBreakerBundle < AppConfiguration > circuitBreakerBundle = CircuitBreakerBundle . builder () . withConfigProvider ( AppConfiguration :: getCircuitBreaker ) . build (); Creating a Circuit Breaker \u00b6 New circuit breakers can be created from the bundle using a builder pattern: 1 2 3 4 CircuitBreaker circuitBreaker = circuitBreakerBundle . createCircuitBreaker ( \"nameInMetrics\" ) . withDefaultConfig () . build (); In case the global bundle configuration isn't sufficient for the specific instance, a custom configuration can be provided (or loaded from the Dropwizard config using withConfigProvider ): 1 2 3 4 5 CircuitBreaker circuitBreaker = circuitBreakerBundle . createCircuitBreaker ( \"nameInMetrics\" ) . withCustomConfig ( new CircuitBreakerConfiguration () . setFailureRateThreshold ( 75.0f )) . build (); Method calls can be wrapped using the circuit breaker: 1 circuitBreaker . executeSupplier (() -> target . doAction ()); See the resilience4j documentation for a full usage guide. Wrapping with Proxies \u00b6 In case you would like to wrap all calls to an object with a circuit breaker, the module provides a simple way to achieve this. For example, a Jersey client can be wrapped with a proxy: 1 2 3 4 5 6 7 8 9 10 11 12 serviceClient = circuitBreakerBundle . createCircuitBreaker ( \"ServiceClient\" ) . withDefaultConfig ()) . wrap ( jerseyClientBundle . getClientFactory () . platformClient () . enableAuthenticationPassThrough () . enableConsumerToken () . api ( ServiceClient . class ) . atTarget ( configuration . getUrl ())); serviceClient . doAction ();","title":"Server Circuit Breaker"},{"location":"server-circuit-breaker/#sda-commons-server-circuit-breaker","text":"This module provides the CircuitBreakerBundle , a Dropwizard bundle that is used to inject circuit breakers into service calls. A circuit breaker is a pattern to make synchronous calls in a distributed system more resilient. This is especially relevant, if a called service hangs without a response, as such failures would cascade up into other services and influence the overall stability of the system. This module doesn't provide an own implementation, but uses the circuit breaker from resilience4j . The bundle also provides prometheus metrics that can be exported in combination with sda-commons-server-prometheus .","title":"SDA Commons Server Circuit Breaker"},{"location":"server-circuit-breaker/#usage","text":"To create a circuit breaker, register the circuit breaker bundle in the application: 1 2 3 4 5 6 7 8 9 10 11 private final CircuitBreakerBundle < AppConfiguration > circuitBreakerBundle = CircuitBreakerBundle . builder () . withDefaultConfig () . build (); @Override public void initialize ( Bootstrap < AppConfiguration > bootstrap ) { bootstrap . addBundle ( circuitBreakerBundle ); ... } As the configuration depends on the environment and the load on the service, a different configuration might be required. A custom configuration can be specified globally at the bundle: 1 2 3 4 private final CircuitBreakerBundle < AppConfiguration > circuitBreakerBundle = CircuitBreakerBundle . builder () . < AppConfiguration > withCustomConfig ( new CircuitBreakerConfiguration (). setFailureRateThreshold ( 50.0f )) . build (); It's also possible to load the configuration from the Dropwizard configuration: 1 2 3 4 private final CircuitBreakerBundle < AppConfiguration > circuitBreakerBundle = CircuitBreakerBundle . builder () . withConfigProvider ( AppConfiguration :: getCircuitBreaker ) . build ();","title":"Usage"},{"location":"server-circuit-breaker/#creating-a-circuit-breaker","text":"New circuit breakers can be created from the bundle using a builder pattern: 1 2 3 4 CircuitBreaker circuitBreaker = circuitBreakerBundle . createCircuitBreaker ( \"nameInMetrics\" ) . withDefaultConfig () . build (); In case the global bundle configuration isn't sufficient for the specific instance, a custom configuration can be provided (or loaded from the Dropwizard config using withConfigProvider ): 1 2 3 4 5 CircuitBreaker circuitBreaker = circuitBreakerBundle . createCircuitBreaker ( \"nameInMetrics\" ) . withCustomConfig ( new CircuitBreakerConfiguration () . setFailureRateThreshold ( 75.0f )) . build (); Method calls can be wrapped using the circuit breaker: 1 circuitBreaker . executeSupplier (() -> target . doAction ()); See the resilience4j documentation for a full usage guide.","title":"Creating a Circuit Breaker"},{"location":"server-circuit-breaker/#wrapping-with-proxies","text":"In case you would like to wrap all calls to an object with a circuit breaker, the module provides a simple way to achieve this. For example, a Jersey client can be wrapped with a proxy: 1 2 3 4 5 6 7 8 9 10 11 12 serviceClient = circuitBreakerBundle . createCircuitBreaker ( \"ServiceClient\" ) . withDefaultConfig ()) . wrap ( jerseyClientBundle . getClientFactory () . platformClient () . enableAuthenticationPassThrough () . enableConsumerToken () . api ( ServiceClient . class ) . atTarget ( configuration . getUrl ())); serviceClient . doAction ();","title":"Wrapping with Proxies"},{"location":"server-cloud-events/","text":"SDA Commons Server CloudEvents \u00b6 Provides some glue code to work with CloudEvents on top of Apache Kafka. \u26a0\ufe0f Experimental \u26a0 \u00b6 The bundle is still in an early state. APIs are open for change. Introduction \u00b6 CloudEvents is a general standard that can be used in combination with your favourite eventing tool like ActiveMQ or Kafka. The CloudEvents specification defines concrete bindings to define how the general specification should be applied to a specific tool. This module brings you the Kafka protocol binding . The following code examples will show you how you can use this bundle in combination with our Kafka bundle. Producing CloudEvents \u00b6 CloudEvents gives you two possibilities to encode your CloudEvents: BINARY content mode The binary content mode seperated event metadata and data; data will be put into the value of your Kafka record; metadata will be put into the headers STRUCTURED content mode The structured content mode keeps event metadata and data together in the payload, allowing simple forwarding of the same event across multiple routing hops, and across multiple protocols. We suggest using the structured content mode. It is very explicit and easy to use because you get the whole event as a simple POJO class. For simplicity, we recommend extending our base class: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 import org.sdase.commons.server.cloudevents.CloudEventV1 ; public class PartnerCreatedEvent extends CloudEventV1 < PartnerCreatedEvent . PartnerCreated > { public static class PartnerCreated { private String id ; public String getId () { return id ; } public PartnerCreated setId ( String id ) { this . id = id ; return this ; } } } You can simply use a standard KafkaJsonSerializer to publish the event. Take a look at the source code of the examples in directory src/test/java/.../app of the module sda-commons-server-cloud-events to get some ideas how you can work with CloudEvents. Consuming CloudEvents \u00b6 You can easily consume the CloudEvent using a standard KafkaJsonDeserializer .","title":"Server Cloud Events"},{"location":"server-cloud-events/#sda-commons-server-cloudevents","text":"Provides some glue code to work with CloudEvents on top of Apache Kafka.","title":"SDA Commons Server CloudEvents"},{"location":"server-cloud-events/#experimental","text":"The bundle is still in an early state. APIs are open for change.","title":"\u26a0\ufe0f Experimental \u26a0"},{"location":"server-cloud-events/#introduction","text":"CloudEvents is a general standard that can be used in combination with your favourite eventing tool like ActiveMQ or Kafka. The CloudEvents specification defines concrete bindings to define how the general specification should be applied to a specific tool. This module brings you the Kafka protocol binding . The following code examples will show you how you can use this bundle in combination with our Kafka bundle.","title":"Introduction"},{"location":"server-cloud-events/#producing-cloudevents","text":"CloudEvents gives you two possibilities to encode your CloudEvents: BINARY content mode The binary content mode seperated event metadata and data; data will be put into the value of your Kafka record; metadata will be put into the headers STRUCTURED content mode The structured content mode keeps event metadata and data together in the payload, allowing simple forwarding of the same event across multiple routing hops, and across multiple protocols. We suggest using the structured content mode. It is very explicit and easy to use because you get the whole event as a simple POJO class. For simplicity, we recommend extending our base class: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 import org.sdase.commons.server.cloudevents.CloudEventV1 ; public class PartnerCreatedEvent extends CloudEventV1 < PartnerCreatedEvent . PartnerCreated > { public static class PartnerCreated { private String id ; public String getId () { return id ; } public PartnerCreated setId ( String id ) { this . id = id ; return this ; } } } You can simply use a standard KafkaJsonSerializer to publish the event. Take a look at the source code of the examples in directory src/test/java/.../app of the module sda-commons-server-cloud-events to get some ideas how you can work with CloudEvents.","title":"Producing CloudEvents"},{"location":"server-cloud-events/#consuming-cloudevents","text":"You can easily consume the CloudEvent using a standard KafkaJsonDeserializer .","title":"Consuming CloudEvents"},{"location":"server-consumer/","text":"SDA Commons Server Consumer \u00b6 The module sda-commons-server-consumer adds support to track or require a consumer token identifying the calling application. Consumers can be differentiated in logs and metrics via a consumers token. The consumer token is provided via the Consumer-Token header. Right now a consumer token is only a name, no verification is done. The consumer name will be derived from the received token (currently they are the same) and is added to the MDC and as request property. If the consumer token is configured as required (= not optional ) which is the default when using the YAML configuration, the server will respond 401 Unauthorized when the client does not provide a consumer token. Usage \u00b6 The consumer token is loaded within a filter that is created and registered by the ConsumerTokenBundle which must be added to the Dropwizard application: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 public class MyApplication extends Application < MyConfiguration > { public static void main ( final String [] args ) { new MyApplication (). run ( args ); } @Override public void initialize ( Bootstrap < MyConfiguration > bootstrap ) { // ... bootstrap . addBundle ( ConsumerTokenBundle . builder () . withConfigProvider ( MyConfiguration :: getConsumerToken ) // required with exclude or optional is configurable in config.yml // alternative1: always require the token if path in not matched by exclude pattern // .withRequiredConsumerToken().withExcludePattern(\"publicResource/\\\\d+.*\") // alternative2: never require the token but track it if available // .withOptionalConsumerToken() . build ()); // ... } @Override public void run ( MyConfiguration configuration , Environment environment ) { // ... } } Exclude patterns \u00b6 When consumer token is set to required, exclude regex patterns can be defined to exclude some URLs to require a consumer token. The regex must match the resource path to exclude it. E.g. in http://localhost:8080/api/projects/1 , http://localhost:8080/api/ is the base path and projects/1 is the resource path. openapi.json and openapi.yaml \u00b6 When the OpenApiBundle from sda-commons-server-openapi is in the classpath, excludes for openapi.json and openapi.yaml are added automatically using the regular expression openapi\\.(json|yaml) to allow clients to load the OpenAPI definition without providing a consumer token. Configuration \u00b6 When the bundle is initialized withConfigProvider , the configuration class needs a field for the ConsumerTokenConfig : 1 2 3 4 5 6 7 8 9 10 11 12 public class MyConfig extends Configuration { private ConsumerTokenConfig consumerToken = new ConsumerTokenConfig (); public ConsumerTokenConfig getConsumerToken () { return consumerToken ; } public void setConsumerToken ( ConsumerTokenConfig consumerToken ) { this . consumerToken = consumerToken ; } } In the config.yml the consumer token may be configured with support for environment properties when the ConfigurationSubstitutionBundle is used: 1 2 3 consumerToken : optional : ${CONSUMER_TOKEN_OPTIONAL:-false} excludePatterns : ${CONSUMER_TOKEN_EXCLUDE:-[]}","title":"Server Consumer"},{"location":"server-consumer/#sda-commons-server-consumer","text":"The module sda-commons-server-consumer adds support to track or require a consumer token identifying the calling application. Consumers can be differentiated in logs and metrics via a consumers token. The consumer token is provided via the Consumer-Token header. Right now a consumer token is only a name, no verification is done. The consumer name will be derived from the received token (currently they are the same) and is added to the MDC and as request property. If the consumer token is configured as required (= not optional ) which is the default when using the YAML configuration, the server will respond 401 Unauthorized when the client does not provide a consumer token.","title":"SDA Commons Server Consumer"},{"location":"server-consumer/#usage","text":"The consumer token is loaded within a filter that is created and registered by the ConsumerTokenBundle which must be added to the Dropwizard application: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 public class MyApplication extends Application < MyConfiguration > { public static void main ( final String [] args ) { new MyApplication (). run ( args ); } @Override public void initialize ( Bootstrap < MyConfiguration > bootstrap ) { // ... bootstrap . addBundle ( ConsumerTokenBundle . builder () . withConfigProvider ( MyConfiguration :: getConsumerToken ) // required with exclude or optional is configurable in config.yml // alternative1: always require the token if path in not matched by exclude pattern // .withRequiredConsumerToken().withExcludePattern(\"publicResource/\\\\d+.*\") // alternative2: never require the token but track it if available // .withOptionalConsumerToken() . build ()); // ... } @Override public void run ( MyConfiguration configuration , Environment environment ) { // ... } }","title":"Usage"},{"location":"server-consumer/#exclude-patterns","text":"When consumer token is set to required, exclude regex patterns can be defined to exclude some URLs to require a consumer token. The regex must match the resource path to exclude it. E.g. in http://localhost:8080/api/projects/1 , http://localhost:8080/api/ is the base path and projects/1 is the resource path.","title":"Exclude patterns"},{"location":"server-consumer/#openapijson-and-openapiyaml","text":"When the OpenApiBundle from sda-commons-server-openapi is in the classpath, excludes for openapi.json and openapi.yaml are added automatically using the regular expression openapi\\.(json|yaml) to allow clients to load the OpenAPI definition without providing a consumer token.","title":"openapi.json and openapi.yaml"},{"location":"server-consumer/#configuration","text":"When the bundle is initialized withConfigProvider , the configuration class needs a field for the ConsumerTokenConfig : 1 2 3 4 5 6 7 8 9 10 11 12 public class MyConfig extends Configuration { private ConsumerTokenConfig consumerToken = new ConsumerTokenConfig (); public ConsumerTokenConfig getConsumerToken () { return consumerToken ; } public void setConsumerToken ( ConsumerTokenConfig consumerToken ) { this . consumerToken = consumerToken ; } } In the config.yml the consumer token may be configured with support for environment properties when the ConfigurationSubstitutionBundle is used: 1 2 3 consumerToken : optional : ${CONSUMER_TOKEN_OPTIONAL:-false} excludePatterns : ${CONSUMER_TOKEN_EXCLUDE:-[]}","title":"Configuration"},{"location":"server-cors/","text":"SDA Commons Server CORS \u00b6 The CORS bundle adds a CORS filter to the servlet to allow cross-origin resource sharing for this service. By doing so, UIs from other origins are allowed to access the service. Initialization \u00b6 To include the CORS filter in an application, the bundle has to be added to the application: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 public class MyApplication extends Application < MyConfiguration > { public static void main ( final String [] args ) { new MyApplication (). run ( args ); } @Override public void initialize ( Bootstrap < MyConfiguration > bootstrap ) { // ... bootstrap . addBundle ( CorsBundle . builder (). withCorsConfigProvider ( MyConfiguration :: getCors ). build ()); // ... } @Override public void run ( MyConfiguration configuration , Environment environment ) { // ... } } Configuration \u00b6 The CORS bundle requires an environment specific configuration in your YAML. Otherwise, no CORS headers will be added to the response and therefore cross-origin resource sharing will fail. 1 2 3 4 5 6 7 8 9 10 cors : # List of origins that are allowed to use the service. \"*\" allows all origins allowedOrigins : - \"*\" # Alternative: If the origins should be restricted, you should add the pattern # allowedOrigins: # - https://*.sdase.com # - https://*test.sdase.com # To use configurable patterns per environment the Json in Yaml syntax may be used with an environment placeholder: # allowedOrigins: ${CORS_ALLOWED_ORIGINS:-[\"*\"]} Application specific allowed headers, exposed headers and HTTP methods can be configured in the builder.","title":"Server Cors"},{"location":"server-cors/#sda-commons-server-cors","text":"The CORS bundle adds a CORS filter to the servlet to allow cross-origin resource sharing for this service. By doing so, UIs from other origins are allowed to access the service.","title":"SDA Commons Server CORS"},{"location":"server-cors/#initialization","text":"To include the CORS filter in an application, the bundle has to be added to the application: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 public class MyApplication extends Application < MyConfiguration > { public static void main ( final String [] args ) { new MyApplication (). run ( args ); } @Override public void initialize ( Bootstrap < MyConfiguration > bootstrap ) { // ... bootstrap . addBundle ( CorsBundle . builder (). withCorsConfigProvider ( MyConfiguration :: getCors ). build ()); // ... } @Override public void run ( MyConfiguration configuration , Environment environment ) { // ... } }","title":"Initialization"},{"location":"server-cors/#configuration","text":"The CORS bundle requires an environment specific configuration in your YAML. Otherwise, no CORS headers will be added to the response and therefore cross-origin resource sharing will fail. 1 2 3 4 5 6 7 8 9 10 cors : # List of origins that are allowed to use the service. \"*\" allows all origins allowedOrigins : - \"*\" # Alternative: If the origins should be restricted, you should add the pattern # allowedOrigins: # - https://*.sdase.com # - https://*test.sdase.com # To use configurable patterns per environment the Json in Yaml syntax may be used with an environment placeholder: # allowedOrigins: ${CORS_ALLOWED_ORIGINS:-[\"*\"]} Application specific allowed headers, exposed headers and HTTP methods can be configured in the builder.","title":"Configuration"},{"location":"server-dropwizard/","text":"SDA Commons Server Dropwizard \u00b6 sda-commons-server-dropwizard is an aggregator module that provides io.dropwizard:dropwizard-core with convergent dependencies and some common Dropwizard bundles for easier configuration that are not dependent on other technology than Dropwizard. This module fixes some version mixes of transitive dependencies in Dropwizard. Dependency convergence should be checked with gradlew dependencies on upgrades. Provided Bundles \u00b6 The Dropwizard module provides default Bundles that are useful for most Dropwizard applications. ConfigurationSubstitutionBundle \u00b6 The ConfigurationSubstitutionBundle allows to use placeholders for environment variables or system properties in the config.yaml of the application to dynamically configure the application at startup. Default values can be added after the variable name separated by :- The ConfigurationSubstitutionBundle supports the modifier toJsonString to convert String values in the config.yaml into valid Json String values by escaping the value and wrapping it in quotes. The default value of a value modified by toJsonString must be valid Json itself, e.g. null or \"my\\nmultiline\\nstring\" . 1 2 3 4 5 database : driverClass : org.postgresql.Driver user : ${POSTGRES_USER:-dev} password : ${POSTGRES_PASSWORD | toJsonString:-\"s3cr3t\"} url : ${POSTGRES_URL:-localhost:12345} ConfigurationValueSupplierBundle \u00b6 The ConfigurationValueSupplierBundle provides a Supplier for a configuration value. It may be used if the type of the configuration itself should not be known by the class that is configured. This may be the case if another bundle or a service should be configured either by a configuration property or another service. A configuration value may be supplied like this: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 public class MyApplication extends Application < MyConfiguration > { public static void main ( final String [] args ) { new MyApplication (). run ( args ); } @Override public void initialize ( Bootstrap < MyConfiguration > bootstrap ) { // ... ConfigurationValueSupplierBundle < MyConfiguration , String > configStringBundle = ConfigurationValueSupplierBundle . builder (). withAccessor ( MyConfiguration :: getConfigString ). build (); bootstrap . addBundle ( configStringBundle ); Supplier < Optional < String >> configStringSupplier = configStringBundle . supplier (); // configStringSupplier may be added to other bundles and services, it's get() method can be access after run() // ... } @Override public void run ( MyConfiguration configuration , Environment environment ) { // ... } } DefaultLoggingConfigurationBundle \u00b6 The DefaultLoggingConfigurationBundle , is used to configure the console logger with the settings desired by the SDA. The bundle sets the log threshold for the console appender to INFO and uses the following log format: 1 [%d] [%-5level] [%X{Trace-Token}] %logger{36} - %msg%n Make sure to add the bundle after the ConfigurationSubstitutionBundle if it's present. Logging related configuration is not required by this bundle. 1 2 3 public void initialize(Bootstrap<Configuration> bootstrap) { bootstrap.addBundle(DefaultLoggingConfigurationBundle.builder().build()); } MetadataContextBundle \u00b6 The MetadataContextBundle enables the metadata context handling for an application. If you want to make use of the data in the metadata context, you should read the dedicated documentation . If your service is required to support the metadata context but is not interested in the data, continue here: Services that use the bundle - can access the current MetadataContext in their implementation - will automatically load the context from incoming HTTP requests into the thread handling the request - will automatically load the context from consumed Kafka messages into the thread handling the message and the error when handling the message fails when the consumer is configured with one of the provided MessageListenerStrategy implementations - will automatically propagate the context to other services via HTTP when using a platform client from the JerseyClientBundle - will automatically propagate the context in produced Kafka messages when the producer is created or registered by the KafkaBundle - are configurable by the property or environment variable METADATA_FIELDS to be aware of the metadata used in a specific environment Services that interrupt a business process should persist the context from MetadataContext.detachedCurrent() and restore it with MetadataContext.createContext(\u2026) when the process continues. Interrupting a business process means that processing is stopped and continued later in a new thread or even another instance of the service. Most likely, this will happen when a business entity is stored based on a request and loaded later for further processing by a scheduler or due to a new user interaction. In this case, the DetachedMetadataContext must be persisted along with the entity and recreated when the entity is loaded. The DetachedMetadataContext can be defined as field in any MongoDB entity. Services that handle requests or messages in parallel must transfer the metadata context to their Runnable or Callable with MetadataContext.transferMetadataContext(\u2026) . In most cases, developers should prefer ContainerRequestContextHolder.transferRequestContext(\u2026) , which also transfers the metadata context. Services that use the MetadataContextBundle and take care of interrupted processes and parallel execution, may add a link like this in their deployment documentation: 1 This service keeps track of the [ metadata context ]( https://github.com/SDA-SE/sda-dropwizard-commons/blob/main/sda-commons-server-dropwizard/README.md#metadatacontextbundle ). JSON Logging \u00b6 To enable JSON logging , set the environment variable ENABLE_JSON_LOGGING to \"true\" . We recommend JSON logging in production as they are better parsable by tools. However, they are hard to read for human beings, so better deactivate them when working with a service locally. Healthcheck Request Logging \u00b6 By default, Dropwizard logs all incoming requests to the console, this includes health checks like /ping and /healthcheck/internal . As these are very frequent and can quickly pollute the logs, they can be disabled by setting the environment variable DISABLE_HEALTHCHECK_LOGS to \"true\" . This will be overwritten by any manual configuration to the FilterFactories. With DISABLE_HEALTHCHECK_LOGS active request logs for all paths related to monitoring are disabled: - /ping - /healthcheck - /healthcheck/internal - /metrics - /metrics/prometheus","title":"Server Dropwizard"},{"location":"server-dropwizard/#sda-commons-server-dropwizard","text":"sda-commons-server-dropwizard is an aggregator module that provides io.dropwizard:dropwizard-core with convergent dependencies and some common Dropwizard bundles for easier configuration that are not dependent on other technology than Dropwizard. This module fixes some version mixes of transitive dependencies in Dropwizard. Dependency convergence should be checked with gradlew dependencies on upgrades.","title":"SDA Commons Server Dropwizard"},{"location":"server-dropwizard/#provided-bundles","text":"The Dropwizard module provides default Bundles that are useful for most Dropwizard applications.","title":"Provided Bundles"},{"location":"server-dropwizard/#configurationsubstitutionbundle","text":"The ConfigurationSubstitutionBundle allows to use placeholders for environment variables or system properties in the config.yaml of the application to dynamically configure the application at startup. Default values can be added after the variable name separated by :- The ConfigurationSubstitutionBundle supports the modifier toJsonString to convert String values in the config.yaml into valid Json String values by escaping the value and wrapping it in quotes. The default value of a value modified by toJsonString must be valid Json itself, e.g. null or \"my\\nmultiline\\nstring\" . 1 2 3 4 5 database : driverClass : org.postgresql.Driver user : ${POSTGRES_USER:-dev} password : ${POSTGRES_PASSWORD | toJsonString:-\"s3cr3t\"} url : ${POSTGRES_URL:-localhost:12345}","title":"ConfigurationSubstitutionBundle"},{"location":"server-dropwizard/#configurationvaluesupplierbundle","text":"The ConfigurationValueSupplierBundle provides a Supplier for a configuration value. It may be used if the type of the configuration itself should not be known by the class that is configured. This may be the case if another bundle or a service should be configured either by a configuration property or another service. A configuration value may be supplied like this: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 public class MyApplication extends Application < MyConfiguration > { public static void main ( final String [] args ) { new MyApplication (). run ( args ); } @Override public void initialize ( Bootstrap < MyConfiguration > bootstrap ) { // ... ConfigurationValueSupplierBundle < MyConfiguration , String > configStringBundle = ConfigurationValueSupplierBundle . builder (). withAccessor ( MyConfiguration :: getConfigString ). build (); bootstrap . addBundle ( configStringBundle ); Supplier < Optional < String >> configStringSupplier = configStringBundle . supplier (); // configStringSupplier may be added to other bundles and services, it's get() method can be access after run() // ... } @Override public void run ( MyConfiguration configuration , Environment environment ) { // ... } }","title":"ConfigurationValueSupplierBundle"},{"location":"server-dropwizard/#defaultloggingconfigurationbundle","text":"The DefaultLoggingConfigurationBundle , is used to configure the console logger with the settings desired by the SDA. The bundle sets the log threshold for the console appender to INFO and uses the following log format: 1 [%d] [%-5level] [%X{Trace-Token}] %logger{36} - %msg%n Make sure to add the bundle after the ConfigurationSubstitutionBundle if it's present. Logging related configuration is not required by this bundle. 1 2 3 public void initialize(Bootstrap<Configuration> bootstrap) { bootstrap.addBundle(DefaultLoggingConfigurationBundle.builder().build()); }","title":"DefaultLoggingConfigurationBundle"},{"location":"server-dropwizard/#metadatacontextbundle","text":"The MetadataContextBundle enables the metadata context handling for an application. If you want to make use of the data in the metadata context, you should read the dedicated documentation . If your service is required to support the metadata context but is not interested in the data, continue here: Services that use the bundle - can access the current MetadataContext in their implementation - will automatically load the context from incoming HTTP requests into the thread handling the request - will automatically load the context from consumed Kafka messages into the thread handling the message and the error when handling the message fails when the consumer is configured with one of the provided MessageListenerStrategy implementations - will automatically propagate the context to other services via HTTP when using a platform client from the JerseyClientBundle - will automatically propagate the context in produced Kafka messages when the producer is created or registered by the KafkaBundle - are configurable by the property or environment variable METADATA_FIELDS to be aware of the metadata used in a specific environment Services that interrupt a business process should persist the context from MetadataContext.detachedCurrent() and restore it with MetadataContext.createContext(\u2026) when the process continues. Interrupting a business process means that processing is stopped and continued later in a new thread or even another instance of the service. Most likely, this will happen when a business entity is stored based on a request and loaded later for further processing by a scheduler or due to a new user interaction. In this case, the DetachedMetadataContext must be persisted along with the entity and recreated when the entity is loaded. The DetachedMetadataContext can be defined as field in any MongoDB entity. Services that handle requests or messages in parallel must transfer the metadata context to their Runnable or Callable with MetadataContext.transferMetadataContext(\u2026) . In most cases, developers should prefer ContainerRequestContextHolder.transferRequestContext(\u2026) , which also transfers the metadata context. Services that use the MetadataContextBundle and take care of interrupted processes and parallel execution, may add a link like this in their deployment documentation: 1 This service keeps track of the [ metadata context ]( https://github.com/SDA-SE/sda-dropwizard-commons/blob/main/sda-commons-server-dropwizard/README.md#metadatacontextbundle ).","title":"MetadataContextBundle"},{"location":"server-dropwizard/#json-logging","text":"To enable JSON logging , set the environment variable ENABLE_JSON_LOGGING to \"true\" . We recommend JSON logging in production as they are better parsable by tools. However, they are hard to read for human beings, so better deactivate them when working with a service locally.","title":"JSON Logging"},{"location":"server-dropwizard/#healthcheck-request-logging","text":"By default, Dropwizard logs all incoming requests to the console, this includes health checks like /ping and /healthcheck/internal . As these are very frequent and can quickly pollute the logs, they can be disabled by setting the environment variable DISABLE_HEALTHCHECK_LOGS to \"true\" . This will be overwritten by any manual configuration to the FilterFactories. With DISABLE_HEALTHCHECK_LOGS active request logs for all paths related to monitoring are disabled: - /ping - /healthcheck - /healthcheck/internal - /metrics - /metrics/prometheus","title":"Healthcheck Request Logging"},{"location":"server-error-handling/","text":"SDA Commons Error Handling example \u00b6 The SDA commons libraries provide support to generate the common error structures as described in the REST guide. This implementation is part of the sda-commons-server-jackson bundle. The default way to inform clients about errors, exceptions should be thrown. These exceptions are mapped to the common error structure using JAX-RS exception mapper ( jakarta.ws.rs.ext.ExceptionMapper ) automatically, if the Jackson bundle is added to the application. Using a response object ( jakarta.ws.rs.core.Response ) to inform clients about exceptions will not necessarily result in the agreed error structure. This is only the case if as entity, an ApiError is used. This example project shows how the error structure will be generated correctly for the following situations: * throwing NotFoundException as one example for using standard JAX-RS web application exceptions. All of these exceptions are mapped automatically * throwing an ApiException . This might be necessary, if a special response code is required for that no default WebApplicationException is defined, e.g. error status 422. * returning an error within a response object (not that exception is not logged automatically by mapper) * returning an error due to validation fault * returning an error created from a new custom validator","title":"Server Error Handling"},{"location":"server-error-handling/#sda-commons-error-handling-example","text":"The SDA commons libraries provide support to generate the common error structures as described in the REST guide. This implementation is part of the sda-commons-server-jackson bundle. The default way to inform clients about errors, exceptions should be thrown. These exceptions are mapped to the common error structure using JAX-RS exception mapper ( jakarta.ws.rs.ext.ExceptionMapper ) automatically, if the Jackson bundle is added to the application. Using a response object ( jakarta.ws.rs.core.Response ) to inform clients about exceptions will not necessarily result in the agreed error structure. This is only the case if as entity, an ApiError is used. This example project shows how the error structure will be generated correctly for the following situations: * throwing NotFoundException as one example for using standard JAX-RS web application exceptions. All of these exceptions are mapped automatically * throwing an ApiException . This might be necessary, if a special response code is required for that no default WebApplicationException is defined, e.g. error status 422. * returning an error within a response object (not that exception is not logged automatically by mapper) * returning an error due to validation fault * returning an error created from a new custom validator","title":"SDA Commons Error Handling example"},{"location":"server-healthcheck-example/","text":"SDA Commons Server Health Check Example \u00b6 This module presents an example application on how to create and register health checks. The application introduces the InternalHealthCheckEndpointBundle to create the new endpoint with internal health check information only. It registers one external health check, that checks another service, and an internal health check, that checks the state of a simple thread. The thread can be quit with a method call for testing reasons.","title":"Server Healthcheck Example"},{"location":"server-healthcheck-example/#sda-commons-server-health-check-example","text":"This module presents an example application on how to create and register health checks. The application introduces the InternalHealthCheckEndpointBundle to create the new endpoint with internal health check information only. It registers one external health check, that checks another service, and an internal health check, that checks the state of a simple thread. The thread can be quit with a method call for testing reasons.","title":"SDA Commons Server Health Check Example"},{"location":"server-healthcheck/","text":"SDA Commons Server Health Check \u00b6 In a Microservice architecture pattern sometimes a service instance can be incapable of handling requests but is still running. For example, it might have run out of database connections. When this occurs, the monitoring system should generate an alert. Health States \u00b6 In the SDA Platform, following three health states are distinguished: Liveness: is a service instance running Readiness: is a service instance running and ready to process requests Fitness: is a service instance running and ready and all dependent components outside the platform are functional. If a service is not ready, it is also not fit. Liveness \u00b6 A liveness check monitors if the service instance is (still) responding. To verify this, Dropwizard provides a ping endpoint at the admin port: 1 http://{serviceURL}:{adminPort}/ping In case the check fails the service instance should be restarted. This can be achieved using a Kubernetes liveness probe that can be added to a pod: 1 2 3 4 5 6 livenessProbe: httpGet: path: /ping port: 8081 initialDelaySeconds: 10 periodSeconds: 30 Readiness \u00b6 A readiness check monitors if the service instance is running and is currently ready to process requests. Health checks should therefor monitor all components that are required to fulfil its purpose and are under governance of the service. This includes databases that are owned by the service, but doesn't include other services that are consumed by the service. The SDA Platform classifies these health checks as internal . Consumed services inside the SDA Platform are responsible for their monitoring themselves. Including another service of the platform in the own health checks would hide the cause of errors in the monitoring and increases the time for fixing. To verify this, Dropwizard provides health checks, however has no distinction between internal and external health checks. Therefor the InternalHealthCheckEndpointBundle publishes a new endpoint at the admin port: 1 http://{serviceURL}:{adminPort}/healthcheck/internal This endpoint only considers internal health checks. The response status is 500 if at least one internal health check reports unhealthy. All health checks that are not explicitly marked as external are internal health checks. If the check fails the service instance should not receive new incoming requests. This can be achieved using a Kubernetes readiness probe that can be added to a pod: 1 2 3 4 5 6 readinessProbe: httpGet: path: /healthcheck/internal port: 8081 initialDelaySeconds: 10 periodSeconds: 30 Fitness \u00b6 A fitness check monitors if the service instance is running and ready and are all dependent components outside the platform are functional. If a service is not ready, it is also not fit. For example a service might require an external service that is not part of our SLA. In that case the fitness check allows to monitor such downtimes. The SDA platform classifies such health checks as external . Dropwizard does not support external health checks by default, but the marker annotation ExternalHealthCheck makes it possible to mark a health check as external . This causes the health check to be excluded from the internal health check endpoint at: 1 http://{serviceURL}:{adminPort}/healthcheck/internal However, the result of the health check is still available at the default health check endpoint: 1 http://{serviceURL}:{adminPort}/healthcheck The ExternalServiceHealthCheck is a configurable implementation to check if a dependent external REST API is healthy by sending a GET request and validating the response status. If a fitness check fails, an automatic action can not be taken. Instead, an admin could be notified to communicate with the external partner. In combination with the sda-commons-server-prometheus package the results of the health checks are exported to prometheus for long term monitoring.","title":"Server Healthcheck"},{"location":"server-healthcheck/#sda-commons-server-health-check","text":"In a Microservice architecture pattern sometimes a service instance can be incapable of handling requests but is still running. For example, it might have run out of database connections. When this occurs, the monitoring system should generate an alert.","title":"SDA Commons Server Health Check"},{"location":"server-healthcheck/#health-states","text":"In the SDA Platform, following three health states are distinguished: Liveness: is a service instance running Readiness: is a service instance running and ready to process requests Fitness: is a service instance running and ready and all dependent components outside the platform are functional. If a service is not ready, it is also not fit.","title":"Health States"},{"location":"server-healthcheck/#liveness","text":"A liveness check monitors if the service instance is (still) responding. To verify this, Dropwizard provides a ping endpoint at the admin port: 1 http://{serviceURL}:{adminPort}/ping In case the check fails the service instance should be restarted. This can be achieved using a Kubernetes liveness probe that can be added to a pod: 1 2 3 4 5 6 livenessProbe: httpGet: path: /ping port: 8081 initialDelaySeconds: 10 periodSeconds: 30","title":"Liveness"},{"location":"server-healthcheck/#readiness","text":"A readiness check monitors if the service instance is running and is currently ready to process requests. Health checks should therefor monitor all components that are required to fulfil its purpose and are under governance of the service. This includes databases that are owned by the service, but doesn't include other services that are consumed by the service. The SDA Platform classifies these health checks as internal . Consumed services inside the SDA Platform are responsible for their monitoring themselves. Including another service of the platform in the own health checks would hide the cause of errors in the monitoring and increases the time for fixing. To verify this, Dropwizard provides health checks, however has no distinction between internal and external health checks. Therefor the InternalHealthCheckEndpointBundle publishes a new endpoint at the admin port: 1 http://{serviceURL}:{adminPort}/healthcheck/internal This endpoint only considers internal health checks. The response status is 500 if at least one internal health check reports unhealthy. All health checks that are not explicitly marked as external are internal health checks. If the check fails the service instance should not receive new incoming requests. This can be achieved using a Kubernetes readiness probe that can be added to a pod: 1 2 3 4 5 6 readinessProbe: httpGet: path: /healthcheck/internal port: 8081 initialDelaySeconds: 10 periodSeconds: 30","title":"Readiness"},{"location":"server-healthcheck/#fitness","text":"A fitness check monitors if the service instance is running and ready and are all dependent components outside the platform are functional. If a service is not ready, it is also not fit. For example a service might require an external service that is not part of our SLA. In that case the fitness check allows to monitor such downtimes. The SDA platform classifies such health checks as external . Dropwizard does not support external health checks by default, but the marker annotation ExternalHealthCheck makes it possible to mark a health check as external . This causes the health check to be excluded from the internal health check endpoint at: 1 http://{serviceURL}:{adminPort}/healthcheck/internal However, the result of the health check is still available at the default health check endpoint: 1 http://{serviceURL}:{adminPort}/healthcheck The ExternalServiceHealthCheck is a configurable implementation to check if a dependent external REST API is healthy by sending a GET request and validating the response status. If a fitness check fails, an automatic action can not be taken. Instead, an admin could be notified to communicate with the external partner. In combination with the sda-commons-server-prometheus package the results of the health checks are exported to prometheus for long term monitoring.","title":"Fitness"},{"location":"server-hibernate-example/","text":"SDA Commons Server Hibernate Example \u00b6 The module provides an example application on how to use the HibernateBundle . The application provides a simple REST endpoint to demonstrate the creation of a transactional context using the @UnitOfWork annotation. More details can be found on the Dropwizard Hibernate documentation page It also comprises two example models, one for REST resources and one for the hibernate entity model. Manager objects that encapsulates the db access can be tested separately in Unit tests as shown in this example.","title":"Server Hibernate Example"},{"location":"server-hibernate-example/#sda-commons-server-hibernate-example","text":"The module provides an example application on how to use the HibernateBundle . The application provides a simple REST endpoint to demonstrate the creation of a transactional context using the @UnitOfWork annotation. More details can be found on the Dropwizard Hibernate documentation page It also comprises two example models, one for REST resources and one for the hibernate entity model. Manager objects that encapsulates the db access can be tested separately in Unit tests as shown in this example.","title":"SDA Commons Server Hibernate Example"},{"location":"server-hibernate-testing/","text":"SDA Commons Server Hibernate Testing \u00b6 The module sda-commons-server-hibernate-testing is the base module to add unit and integration tests for applications using Hibernate in the SDA SE infrastructure. This module provides * Database Rider * DbUnit * H2 Further Information \u00b6 Database Rider Getting Started H2 Features","title":"Server Hibernate Testing"},{"location":"server-hibernate-testing/#sda-commons-server-hibernate-testing","text":"The module sda-commons-server-hibernate-testing is the base module to add unit and integration tests for applications using Hibernate in the SDA SE infrastructure. This module provides * Database Rider * DbUnit * H2","title":"SDA Commons Server Hibernate Testing"},{"location":"server-hibernate-testing/#further-information","text":"Database Rider Getting Started H2 Features","title":"Further Information"},{"location":"server-hibernate/","text":"SDA Commons Server Hibernate \u00b6 The module sda-commons-server-hibernate provides access to relational databases with hibernate. It is primarily designed to use PostgreSQL in production and H2 in memory in tests. It provides - configuration of the database to use, - db migration using Flyway , - a health check for the database connection sda-commons-server-hibernate ships with org.postgresql:postgresql , org.flywaydb:flyway-core and org.flywaydb:flyway-database-postgresql at compile scope. Usage \u00b6 Initialization \u00b6 The HibernateBundle should be added as field in the application class instead of being anonymously added in the initialize method like other bundles of this library. Implementations need to refer to the instance to get access to the SessionFactory . The Dropwizard applications config class needs to provide a DataSourceFactory . The bundle builder requires to define the getter of the DataSourceFactory as method reference to access the configuration. One or more packages that should be scanned for entity classes must be defined. Entity classes should be described using JPA annotations . The packages to scan may be either added as String or can be derived from a class or marker interface in the root of the model packages. 1 2 3 4 5 6 7 8 9 10 public class MyApplication extends Application < MyConfiguration > { private final HibernateBundle < HibernateITestConfiguration > hibernateBundle = HibernateBundle . builder () . withConfigurationProvider ( MyConfiguration :: getDatabase ) . withEntityScanPackageClass ( MyEntity . class ) . build (); // ... } In the context of a CDI application, the SessionFactory instance that is created in the HibernateBundle should be provided as CDI bean, so it can be injected into managers, repositories or however the data access objects are named in the application: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 @ApplicationScoped public class MyCdiApplication extends Application < MyConfiguration > { private final HibernateBundle < HibernateITestConfiguration > hibernateBundle = HibernateBundle . builder () . withConfigurationProvider ( MyConfiguration :: getDatabase ) . withEntityScanPackageClass ( MyEntity . class ) . build (); public static void main ( final String [] args ) throws Exception { // from sda-commons-server-weld DropwizardWeldHelper . run ( SolutionServiceApplication . class , args ); } // ... @jakarta.enterprise.inject.Produces public SessionFactory sessionFactory () { return hibernateBundle . sessionFactory (); } } 1 2 3 4 5 6 7 8 9 public class MyEntityManager extends io . dropwizard . hibernate . AbstractDAO < MyEntity > { @Inject public MyEntityManager ( SessionFactory sessionFactory ) { super ( sessionFactory ); } // .... } Configuration \u00b6 The database connection is configured in the config.yaml of the application. It uses the format of Dropwizard Hibernate . Defaults are defined for PostgreSQL. The default PostgreSQL schema is public . The key ( database in the examples) depends on the applications configuration class. Example config for production : 1 2 3 4 database : user : username password : s3cr3t url : jdbc:postgresql://postgres-host:5432/my_service Example config for developer machines using local-infra : 1 2 3 4 5 6 database : user : dbuser password : sda123 url : jdbc:postgresql://localhost:5435/postgres properties : currentSchema : my_service Example config for testing : 1 2 3 4 5 6 7 database : driverClass : org.h2.Driver user : sa password : sa url : jdbc:h2:mem:test;DB_CLOSE_DELAY=-1 properties : hibernate.dialect : org.hibernate.dialect.H2Dialect Database access \u00b6 DAOs or repositories are built by extending the AbstractDAO . The Dropwizard Hibernate Documentation has an example. The required SessionFactory is provided by the HibernateBundle instance. Schema migration \u00b6 For database schema migration, Flyway is used by the DbMigrationService and works with the same DataSourceFactory as the HibernateBundle . It may be used in a custom ConfiguredCommand in each application. Therefore, defaults for the command name and the documentation is provided: 1 2 3 4 5 6 7 8 9 10 11 public class DbMigrationCommand extends ConfiguredCommand < MyAppConfig > { public DbMigrationCommand () { super ( DbMigrationService . DEFAULT_COMMAND_NAME , DbMigrationService . DEFAULT_COMMAND_DOC ); } @Override protected void run ( Bootstrap < MyAppConfig > bootstrap , Namespace namespace , MyAppConfig configuration ) { new DbMigrationService ( configuration . getDatabase ()). migrateDatabase (); } } The command needs to be added to the application: 1 2 3 4 5 6 7 8 9 public class HibernateApp extends Application < MyAppConfig > { @Override public void initialize ( Bootstrap < MyAppConfig > bootstrap ) { // ... bootstrap . addCommand ( new DbMigrationCommand ()); // ... } // ... } The command is then executed from command line to migrate the database: 1 java -jar my-application.jar migrateDB ./config.yaml DB migration scripts are expected in src/main/resources/db.migration/*.sql . Health check \u00b6 A health check with the name hibernate is automatically registered to test the connection to the database. By default SELECT 1 is used as test query. It can be customized in the config.yaml : 1 2 database : validationQuery : SELECT 2 Testing \u00b6 For testing database access with Hibernate we suggest to use sda-commons-hibernate-testing module. Dependencies to be added: 1 testCompile 'org.sdase.commons:sda-commons-server-hibernate-testing:<VERSION>' For creating tests without a Dropwizard application please refer to the DbMigrationServiceTest as example. For creating full integration tests of a Dropwizard application please refer to HibernateIT as example.","title":"Server Hibernate"},{"location":"server-hibernate/#sda-commons-server-hibernate","text":"The module sda-commons-server-hibernate provides access to relational databases with hibernate. It is primarily designed to use PostgreSQL in production and H2 in memory in tests. It provides - configuration of the database to use, - db migration using Flyway , - a health check for the database connection sda-commons-server-hibernate ships with org.postgresql:postgresql , org.flywaydb:flyway-core and org.flywaydb:flyway-database-postgresql at compile scope.","title":"SDA Commons Server Hibernate"},{"location":"server-hibernate/#usage","text":"","title":"Usage"},{"location":"server-hibernate/#initialization","text":"The HibernateBundle should be added as field in the application class instead of being anonymously added in the initialize method like other bundles of this library. Implementations need to refer to the instance to get access to the SessionFactory . The Dropwizard applications config class needs to provide a DataSourceFactory . The bundle builder requires to define the getter of the DataSourceFactory as method reference to access the configuration. One or more packages that should be scanned for entity classes must be defined. Entity classes should be described using JPA annotations . The packages to scan may be either added as String or can be derived from a class or marker interface in the root of the model packages. 1 2 3 4 5 6 7 8 9 10 public class MyApplication extends Application < MyConfiguration > { private final HibernateBundle < HibernateITestConfiguration > hibernateBundle = HibernateBundle . builder () . withConfigurationProvider ( MyConfiguration :: getDatabase ) . withEntityScanPackageClass ( MyEntity . class ) . build (); // ... } In the context of a CDI application, the SessionFactory instance that is created in the HibernateBundle should be provided as CDI bean, so it can be injected into managers, repositories or however the data access objects are named in the application: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 @ApplicationScoped public class MyCdiApplication extends Application < MyConfiguration > { private final HibernateBundle < HibernateITestConfiguration > hibernateBundle = HibernateBundle . builder () . withConfigurationProvider ( MyConfiguration :: getDatabase ) . withEntityScanPackageClass ( MyEntity . class ) . build (); public static void main ( final String [] args ) throws Exception { // from sda-commons-server-weld DropwizardWeldHelper . run ( SolutionServiceApplication . class , args ); } // ... @jakarta.enterprise.inject.Produces public SessionFactory sessionFactory () { return hibernateBundle . sessionFactory (); } } 1 2 3 4 5 6 7 8 9 public class MyEntityManager extends io . dropwizard . hibernate . AbstractDAO < MyEntity > { @Inject public MyEntityManager ( SessionFactory sessionFactory ) { super ( sessionFactory ); } // .... }","title":"Initialization"},{"location":"server-hibernate/#configuration","text":"The database connection is configured in the config.yaml of the application. It uses the format of Dropwizard Hibernate . Defaults are defined for PostgreSQL. The default PostgreSQL schema is public . The key ( database in the examples) depends on the applications configuration class. Example config for production : 1 2 3 4 database : user : username password : s3cr3t url : jdbc:postgresql://postgres-host:5432/my_service Example config for developer machines using local-infra : 1 2 3 4 5 6 database : user : dbuser password : sda123 url : jdbc:postgresql://localhost:5435/postgres properties : currentSchema : my_service Example config for testing : 1 2 3 4 5 6 7 database : driverClass : org.h2.Driver user : sa password : sa url : jdbc:h2:mem:test;DB_CLOSE_DELAY=-1 properties : hibernate.dialect : org.hibernate.dialect.H2Dialect","title":"Configuration"},{"location":"server-hibernate/#database-access","text":"DAOs or repositories are built by extending the AbstractDAO . The Dropwizard Hibernate Documentation has an example. The required SessionFactory is provided by the HibernateBundle instance.","title":"Database access"},{"location":"server-hibernate/#schema-migration","text":"For database schema migration, Flyway is used by the DbMigrationService and works with the same DataSourceFactory as the HibernateBundle . It may be used in a custom ConfiguredCommand in each application. Therefore, defaults for the command name and the documentation is provided: 1 2 3 4 5 6 7 8 9 10 11 public class DbMigrationCommand extends ConfiguredCommand < MyAppConfig > { public DbMigrationCommand () { super ( DbMigrationService . DEFAULT_COMMAND_NAME , DbMigrationService . DEFAULT_COMMAND_DOC ); } @Override protected void run ( Bootstrap < MyAppConfig > bootstrap , Namespace namespace , MyAppConfig configuration ) { new DbMigrationService ( configuration . getDatabase ()). migrateDatabase (); } } The command needs to be added to the application: 1 2 3 4 5 6 7 8 9 public class HibernateApp extends Application < MyAppConfig > { @Override public void initialize ( Bootstrap < MyAppConfig > bootstrap ) { // ... bootstrap . addCommand ( new DbMigrationCommand ()); // ... } // ... } The command is then executed from command line to migrate the database: 1 java -jar my-application.jar migrateDB ./config.yaml DB migration scripts are expected in src/main/resources/db.migration/*.sql .","title":"Schema migration"},{"location":"server-hibernate/#health-check","text":"A health check with the name hibernate is automatically registered to test the connection to the database. By default SELECT 1 is used as test query. It can be customized in the config.yaml : 1 2 database : validationQuery : SELECT 2","title":"Health check"},{"location":"server-hibernate/#testing","text":"For testing database access with Hibernate we suggest to use sda-commons-hibernate-testing module. Dependencies to be added: 1 testCompile 'org.sdase.commons:sda-commons-server-hibernate-testing:<VERSION>' For creating tests without a Dropwizard application please refer to the DbMigrationServiceTest as example. For creating full integration tests of a Dropwizard application please refer to HibernateIT as example.","title":"Testing"},{"location":"server-jackson/","text":"SDA Commons Server Jackson \u00b6 The module sda-commons-server-jackson is used to configure the ObjectMapper with the recommended default settings of SDA SE services. It also provides support for linking resources with HAL and adds the ability to filter fields on client request. The JacksonConfigurationBundle is used to configure the JSON serializer. It adds various error mappers to support the SDA error message standard. These replace the default Dropwizard error mappers but also additional new mappers are added, e.g. mapping JaxRs Exceptions, such as NotFound and NotAuthorized. All mappers do log the errors when mapping. The framework already provides a predefined exception mapper for InvalidTypeException if the framework default for feature FAIL_ON_INVALID_SUBTYPE is customized. See section Customize the ObjectMapper below for details about customizing the default object mapper settings. If the feature is enabled the InvalidTypeIdExceptionMapper produces a response in the common error format. The ObjectMapperConfigurationUtil can be used to receive an ObjectMapper with the recommended settings for usage outside a Dropwizard application. The default ObjectMapper is configured to be fault-tolerant to avoid failures in deserialization. JSR-303 validations should be used to validate input data. For serialization the bundle disables FAIL_ON_EMPTY_BEANS , WRITE_DATES_AS_TIMESTAMPS , WRITE_DURATIONS_AS_TIMESTAMPS . For deserialization the bundle disables FAIL_ON_UNKNOWN_PROPERTIES , FAIL_ON_IGNORED_PROPERTIES , FAIL_ON_INVALID_SUBTYPE and enables ACCEPT_SINGLE_VALUE_AS_ARRAY , READ_UNKNOWN_ENUM_VALUES_AS_NULL , READ_UNKNOWN_ENUM_VALUES_USING_DEFAULT_VALUE . The FuzzyEnumModule from Dropwizard is removed as it lacks support of newer Jackson features for enumerations. Usage \u00b6 In the application class, the bundle is added in the initialize method: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 import JacksonConfigurationBundle ; import io.dropwizard.core.Application ; public class MyApplication extends Application < MyConfiguration > { public static void main ( final String [] args ) { new MyApplication (). run ( args ); } @Override public void initialize ( Bootstrap < MyConfiguration > bootstrap ) { // ... bootstrap . addBundle ( JacksonConfigurationBundle . builder (). build ()); // ... } @Override public void run ( MyConfiguration configuration , Environment environment ) { // ... } } Date and Time formatting \u00b6 It is strongly recommended to use LocalDate for dates without time ZonedDateTime for date and times Duration for durations with time resolution Period for durations with day resolution All these types can be read and written in JSON as ISO 8601 formats. ZonedDateTime is formatted with milliseconds or nanoseconds according to the detail set in the instance. Type Resolution Example LocalDate Day of Month 2018-09-23 ZonedDateTime Any time unit 2018-09-23T14:21:41.123+01:00 Duration Any time unit P1DT13M Period Days P1Y2D Reading ZonedDateTime is configured to be tolerant so that added nanoseconds or missing milliseconds or missing seconds are supported. Please do not use @JsonFormat(pattern = \"...\") for customizing serialization because it breaks tolerant reading of formatting variants. If output should be customized, use @JsonSerializer . SDA-Commons provides two default serializers for ZonedDateTime to use a fixed resolution in the output. Iso8601Serializer is used to omit milliseconds and Iso8601Serializer.WithMillis is used to render the time with 3 digits for milliseconds. Usage: 1 2 3 4 5 6 7 8 9 class MyResource { @JsonSerialize ( using = org . sdase . commons . server . jackson . Iso8601Serializer . class ) private ZonedDateTime zonedDateTime ; @JsonSerialize ( using = org . sdase . commons . server . jackson . Iso8601Serializer . WithMillis . class ) private ZonedDateTime zonedDateTimeWithMillis ; // ... } Adding HAL links to resources \u00b6 Resources that should be processed for HAL links must be annotated as @Resource . Links are added directly in the resource class and are annotated as @Link . Embedded resources can be added as @EmbeddedResource . The Open API Tools are used to render them in appropriate _links and _embedded properties. Links are properly documented in Swagger when io.openapitools.hal:swagger-hal is added to the dependencies. io.openapitools.hal:swagger-hal is shipped with sda-commons-server-openapi . HAL link support may be disabled in the JacksonConfigurationBundle.builder() . Example: 1 2 3 4 5 6 7 @Resource public class Person { @Link private HALLink self ; private String name ; // ... } 1 2 3 GET / persons / 123 => { \"_links\" : { \"self\" : { \"href\" : \"/persons/123\" }}, \"name\" : \"John Doe\" } EmbedHelper \u00b6 To decide whether a resource is just linked or embedded, the EmbedHelper can be used. If query parameters for embedding are passed, like /api/cars?embed=drivers,owner , EmbedHelper.isEmbeddingOfRelationRequested(relationName) can be used to check whether a resource should be embedded: 1 2 3 4 5 6 7 EmbedHelper embedHelper = new EmbedHelper ( environment ); ... if ( embedHelper . isEmbeddingOfRelationRequested ( \"owner\" )) { carResource . setOwner ( createPerson ( ownerId )); } In an application that uses CDI the EmbedHelper should be instantiated the same way and provided by a producer method: 1 2 3 4 5 6 7 8 9 10 11 12 private EmbedHelper embedHelper ; @Override public void run ( Configuration config , Environment environment ) { // ... this . embedHelper = new EmbedHelper ( environment ); } @Produces public EmbedHelper embedHelper () { return this . embedHelper ; } Field filtering feature for resources \u00b6 The JacksonConfigurationBundle registers the FieldFilterModule to add the FieldFilterSerializerModifier . The modifier uses the JAX-RS request context to identify the query param fields . If such a query param is present, only the requested fields (comma separated) are rendered in the resource view. additionally HAL links and embedded resources are rendered as well. Field filtering support may be disabled in the JacksonConfigurationBundle.builder() . Example: 1 2 3 4 5 6 7 @EnableFieldFilter public class Person { private String firstName ; private String surName ; private String nickName ; // ... } 1 2 3 GET / persons / 123 ? fields = firstName , nickName => { \"firstName\" : \"John\" , \"nickName\" : \"Johnny\" } Configuration \u00b6 Disable HAL support \u00b6 The JacksonConfigurationBundle may be initialized without HAL support, if links are not needed or achieved in another way in the application: 1 JacksonConfigurationBundle . builder (). withoutHalSupport (). build (); Customize the ObjectMapper \u00b6 Custom configurations of the ObjectMapper can be achieved by adding a customization consumer which receives the used ObjectMapper instance: 1 2 3 JacksonConfigurationBundle . builder () . withCustomization ( om -> om . enable ( SerializationFeature . INDENT_OUTPUT )) . build (); YAML \u00b6 If the JacksonYAMLProvider is available in the classpath, it will be registered to support requests that Accept application/yaml . This is especially useful for Swagger which provides the swagger.json also as swagger.yaml . To activate YAML support, a dependency to com.fasterxml.jackson.jaxrs:jackson-jaxrs-yaml-provider has to be added. It is shipped in an appropriate version with sda-commons-server-openapi . Error Format \u00b6 Exceptions are mapped to a common error format that looks like the following example 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 422 Unprocessable Entity { \"title\" : \"Request parameters are not valid\" , \"invalidParams\" : [ { \"field\" : \"manufacture\" , \"reason\" : \"Audi has no Golf GTI model (not found)\" \"errorCode\" : \"FIELD_CORRELATION_ERROR\" }, { \"field\" : \"model\" , \"reason\" : \"Golf GTI is unkown by Audi (not found)\" \"errorCode\" : \"FIELD_CORRELATION_ERROR\" } ] } For validation errors, the invalidParams section is filled. For other errors, just a title is given. \"field\" defines the invalid field within the JSON structure \"reason\" gives a hint why the value is not valid. This is the error message of the validation. \"errorCode\" is the validation annotation given in uppercase underscore notation The reason might be in different language due to internationalization. Examples how exceptions and the error structure should be used, can be found within the example project sda-commons-server-errorhandling-example","title":"Server Jackson"},{"location":"server-jackson/#sda-commons-server-jackson","text":"The module sda-commons-server-jackson is used to configure the ObjectMapper with the recommended default settings of SDA SE services. It also provides support for linking resources with HAL and adds the ability to filter fields on client request. The JacksonConfigurationBundle is used to configure the JSON serializer. It adds various error mappers to support the SDA error message standard. These replace the default Dropwizard error mappers but also additional new mappers are added, e.g. mapping JaxRs Exceptions, such as NotFound and NotAuthorized. All mappers do log the errors when mapping. The framework already provides a predefined exception mapper for InvalidTypeException if the framework default for feature FAIL_ON_INVALID_SUBTYPE is customized. See section Customize the ObjectMapper below for details about customizing the default object mapper settings. If the feature is enabled the InvalidTypeIdExceptionMapper produces a response in the common error format. The ObjectMapperConfigurationUtil can be used to receive an ObjectMapper with the recommended settings for usage outside a Dropwizard application. The default ObjectMapper is configured to be fault-tolerant to avoid failures in deserialization. JSR-303 validations should be used to validate input data. For serialization the bundle disables FAIL_ON_EMPTY_BEANS , WRITE_DATES_AS_TIMESTAMPS , WRITE_DURATIONS_AS_TIMESTAMPS . For deserialization the bundle disables FAIL_ON_UNKNOWN_PROPERTIES , FAIL_ON_IGNORED_PROPERTIES , FAIL_ON_INVALID_SUBTYPE and enables ACCEPT_SINGLE_VALUE_AS_ARRAY , READ_UNKNOWN_ENUM_VALUES_AS_NULL , READ_UNKNOWN_ENUM_VALUES_USING_DEFAULT_VALUE . The FuzzyEnumModule from Dropwizard is removed as it lacks support of newer Jackson features for enumerations.","title":"SDA Commons Server Jackson"},{"location":"server-jackson/#usage","text":"In the application class, the bundle is added in the initialize method: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 import JacksonConfigurationBundle ; import io.dropwizard.core.Application ; public class MyApplication extends Application < MyConfiguration > { public static void main ( final String [] args ) { new MyApplication (). run ( args ); } @Override public void initialize ( Bootstrap < MyConfiguration > bootstrap ) { // ... bootstrap . addBundle ( JacksonConfigurationBundle . builder (). build ()); // ... } @Override public void run ( MyConfiguration configuration , Environment environment ) { // ... } }","title":"Usage"},{"location":"server-jackson/#date-and-time-formatting","text":"It is strongly recommended to use LocalDate for dates without time ZonedDateTime for date and times Duration for durations with time resolution Period for durations with day resolution All these types can be read and written in JSON as ISO 8601 formats. ZonedDateTime is formatted with milliseconds or nanoseconds according to the detail set in the instance. Type Resolution Example LocalDate Day of Month 2018-09-23 ZonedDateTime Any time unit 2018-09-23T14:21:41.123+01:00 Duration Any time unit P1DT13M Period Days P1Y2D Reading ZonedDateTime is configured to be tolerant so that added nanoseconds or missing milliseconds or missing seconds are supported. Please do not use @JsonFormat(pattern = \"...\") for customizing serialization because it breaks tolerant reading of formatting variants. If output should be customized, use @JsonSerializer . SDA-Commons provides two default serializers for ZonedDateTime to use a fixed resolution in the output. Iso8601Serializer is used to omit milliseconds and Iso8601Serializer.WithMillis is used to render the time with 3 digits for milliseconds. Usage: 1 2 3 4 5 6 7 8 9 class MyResource { @JsonSerialize ( using = org . sdase . commons . server . jackson . Iso8601Serializer . class ) private ZonedDateTime zonedDateTime ; @JsonSerialize ( using = org . sdase . commons . server . jackson . Iso8601Serializer . WithMillis . class ) private ZonedDateTime zonedDateTimeWithMillis ; // ... }","title":"Date and Time formatting"},{"location":"server-jackson/#adding-hal-links-to-resources","text":"Resources that should be processed for HAL links must be annotated as @Resource . Links are added directly in the resource class and are annotated as @Link . Embedded resources can be added as @EmbeddedResource . The Open API Tools are used to render them in appropriate _links and _embedded properties. Links are properly documented in Swagger when io.openapitools.hal:swagger-hal is added to the dependencies. io.openapitools.hal:swagger-hal is shipped with sda-commons-server-openapi . HAL link support may be disabled in the JacksonConfigurationBundle.builder() . Example: 1 2 3 4 5 6 7 @Resource public class Person { @Link private HALLink self ; private String name ; // ... } 1 2 3 GET / persons / 123 => { \"_links\" : { \"self\" : { \"href\" : \"/persons/123\" }}, \"name\" : \"John Doe\" }","title":"Adding HAL links to resources"},{"location":"server-jackson/#embedhelper","text":"To decide whether a resource is just linked or embedded, the EmbedHelper can be used. If query parameters for embedding are passed, like /api/cars?embed=drivers,owner , EmbedHelper.isEmbeddingOfRelationRequested(relationName) can be used to check whether a resource should be embedded: 1 2 3 4 5 6 7 EmbedHelper embedHelper = new EmbedHelper ( environment ); ... if ( embedHelper . isEmbeddingOfRelationRequested ( \"owner\" )) { carResource . setOwner ( createPerson ( ownerId )); } In an application that uses CDI the EmbedHelper should be instantiated the same way and provided by a producer method: 1 2 3 4 5 6 7 8 9 10 11 12 private EmbedHelper embedHelper ; @Override public void run ( Configuration config , Environment environment ) { // ... this . embedHelper = new EmbedHelper ( environment ); } @Produces public EmbedHelper embedHelper () { return this . embedHelper ; }","title":"EmbedHelper"},{"location":"server-jackson/#field-filtering-feature-for-resources","text":"The JacksonConfigurationBundle registers the FieldFilterModule to add the FieldFilterSerializerModifier . The modifier uses the JAX-RS request context to identify the query param fields . If such a query param is present, only the requested fields (comma separated) are rendered in the resource view. additionally HAL links and embedded resources are rendered as well. Field filtering support may be disabled in the JacksonConfigurationBundle.builder() . Example: 1 2 3 4 5 6 7 @EnableFieldFilter public class Person { private String firstName ; private String surName ; private String nickName ; // ... } 1 2 3 GET / persons / 123 ? fields = firstName , nickName => { \"firstName\" : \"John\" , \"nickName\" : \"Johnny\" }","title":"Field filtering feature for resources"},{"location":"server-jackson/#configuration","text":"","title":"Configuration"},{"location":"server-jackson/#disable-hal-support","text":"The JacksonConfigurationBundle may be initialized without HAL support, if links are not needed or achieved in another way in the application: 1 JacksonConfigurationBundle . builder (). withoutHalSupport (). build ();","title":"Disable HAL support"},{"location":"server-jackson/#customize-the-objectmapper","text":"Custom configurations of the ObjectMapper can be achieved by adding a customization consumer which receives the used ObjectMapper instance: 1 2 3 JacksonConfigurationBundle . builder () . withCustomization ( om -> om . enable ( SerializationFeature . INDENT_OUTPUT )) . build ();","title":"Customize the ObjectMapper"},{"location":"server-jackson/#yaml","text":"If the JacksonYAMLProvider is available in the classpath, it will be registered to support requests that Accept application/yaml . This is especially useful for Swagger which provides the swagger.json also as swagger.yaml . To activate YAML support, a dependency to com.fasterxml.jackson.jaxrs:jackson-jaxrs-yaml-provider has to be added. It is shipped in an appropriate version with sda-commons-server-openapi .","title":"YAML"},{"location":"server-jackson/#error-format","text":"Exceptions are mapped to a common error format that looks like the following example 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 422 Unprocessable Entity { \"title\" : \"Request parameters are not valid\" , \"invalidParams\" : [ { \"field\" : \"manufacture\" , \"reason\" : \"Audi has no Golf GTI model (not found)\" \"errorCode\" : \"FIELD_CORRELATION_ERROR\" }, { \"field\" : \"model\" , \"reason\" : \"Golf GTI is unkown by Audi (not found)\" \"errorCode\" : \"FIELD_CORRELATION_ERROR\" } ] } For validation errors, the invalidParams section is filled. For other errors, just a title is given. \"field\" defines the invalid field within the JSON structure \"reason\" gives a hint why the value is not valid. This is the error message of the validation. \"errorCode\" is the validation annotation given in uppercase underscore notation The reason might be in different language due to internationalization. Examples how exceptions and the error structure should be used, can be found within the example project sda-commons-server-errorhandling-example","title":"Error Format"},{"location":"server-kafka-example/","text":"SDA Dropwizard Commons Server Kafka Example \u00b6 This module presents two very basic example applications * one for showing the creation of MessageProducer and * one for showing the creation of MessageListener . The consumers are very simple and just store the retrieved values in a list. Each of the applications comprises two examples, a simple one and a more complex one using values from the Kafka section of the application configuration.","title":"Server Kafka Example"},{"location":"server-kafka-example/#sda-dropwizard-commons-server-kafka-example","text":"This module presents two very basic example applications * one for showing the creation of MessageProducer and * one for showing the creation of MessageListener . The consumers are very simple and just store the retrieved values in a list. Each of the applications comprises two examples, a simple one and a more complex one using values from the Kafka section of the application configuration.","title":"SDA Dropwizard Commons Server Kafka Example"},{"location":"server-kafka-testing/","text":"SDA Commons Server Kafka Testing \u00b6 The module sda-commons-server-kafka-testing is the base module to add unit and integration test for Kafka broker usage. It includes the dependencies to sda-commons-server-testing module. The kafka-junit5 library provides means for easily setting up a Kafka broker that can be reconfigured easily by using the following class extension: 1 2 3 @RegisterExtension @Order ( 0 ) // Start the broker before the app static final SharedKafkaTestResource KAFKA = new SharedKafkaTestResource (). withBrokers ( 2 ); Test support with random broker ports \u00b6 The usage of random ports allows to execute tests in parallel and reduce the probability of port conflicts, e.g. when local-infra is also started. The example above starts two Kafka brokers within a cluster. To test your application, you have to configure these servers as bootstrap servers. This is normally done via the configuration YAML file within the property kafka -> brokers . You can override these properties programmatically using config overrides when creating your DropwizardAppExtension : Example: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 import com.salesforce.kafka.test.junit5.SharedKafkaTestResource ; import io.dropwizard.testing.junit5.DropwizardAppExtension ; // ... class KafkaJUnit5IT { @RegisterExtension @Order ( 0 ) // Start the broker before the app static final SharedKafkaTestResource KAFKA = new SharedKafkaTestResource (). withBrokers ( 2 ); @RegisterExtension @Order ( 1 ) static final DropwizardAppExtension < KafkaTestConfiguration > DW = new DropwizardAppExtension <> ( KafkaTestApplication . class , resourceFilePath ( \"test-config.yaml\" ), config ( \"kafka.brokers\" , KAFKA :: getKafkaConnectString ) // Override the Kafka brokers ); } Create topics \u00b6 When setting up your test class you might have problems if producers or consumers want to register with a topic that does not exist. Do ease the creation of topics you can use the following Junit 5 extension. Make sure that this class extension is executed after the Kafka server was started but before your application starts up. Example: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 @RegisterExtension @Order ( 0 ) static final SharedKafkaTestResource KAFKA = new SharedKafkaTestResource (); @RegisterExtension @Order ( 1 ) static final CreateKafkaTopicsClassExtension TOPICS = new CreateKafkaTopicsClassExtension ( KAFKA , Arrays . asList ( \"foo\" , \"bar\" )); @RegisterExtension @Order ( 2 ) static final DropwizardAppExtension < KafkaExampleConfiguration > DW = new DropwizardAppExtension <> ( KafkaExampleProducerApplication . class , resourceFilePath ( \"test-config-producer.yml\" ), config ( \"kafka.brokers\" , KAFKA :: getKafkaConnectString ));","title":"Server Kafka Testing"},{"location":"server-kafka-testing/#sda-commons-server-kafka-testing","text":"The module sda-commons-server-kafka-testing is the base module to add unit and integration test for Kafka broker usage. It includes the dependencies to sda-commons-server-testing module. The kafka-junit5 library provides means for easily setting up a Kafka broker that can be reconfigured easily by using the following class extension: 1 2 3 @RegisterExtension @Order ( 0 ) // Start the broker before the app static final SharedKafkaTestResource KAFKA = new SharedKafkaTestResource (). withBrokers ( 2 );","title":"SDA Commons Server Kafka Testing"},{"location":"server-kafka-testing/#test-support-with-random-broker-ports","text":"The usage of random ports allows to execute tests in parallel and reduce the probability of port conflicts, e.g. when local-infra is also started. The example above starts two Kafka brokers within a cluster. To test your application, you have to configure these servers as bootstrap servers. This is normally done via the configuration YAML file within the property kafka -> brokers . You can override these properties programmatically using config overrides when creating your DropwizardAppExtension : Example: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 import com.salesforce.kafka.test.junit5.SharedKafkaTestResource ; import io.dropwizard.testing.junit5.DropwizardAppExtension ; // ... class KafkaJUnit5IT { @RegisterExtension @Order ( 0 ) // Start the broker before the app static final SharedKafkaTestResource KAFKA = new SharedKafkaTestResource (). withBrokers ( 2 ); @RegisterExtension @Order ( 1 ) static final DropwizardAppExtension < KafkaTestConfiguration > DW = new DropwizardAppExtension <> ( KafkaTestApplication . class , resourceFilePath ( \"test-config.yaml\" ), config ( \"kafka.brokers\" , KAFKA :: getKafkaConnectString ) // Override the Kafka brokers ); }","title":"Test support with random broker ports"},{"location":"server-kafka-testing/#create-topics","text":"When setting up your test class you might have problems if producers or consumers want to register with a topic that does not exist. Do ease the creation of topics you can use the following Junit 5 extension. Make sure that this class extension is executed after the Kafka server was started but before your application starts up. Example: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 @RegisterExtension @Order ( 0 ) static final SharedKafkaTestResource KAFKA = new SharedKafkaTestResource (); @RegisterExtension @Order ( 1 ) static final CreateKafkaTopicsClassExtension TOPICS = new CreateKafkaTopicsClassExtension ( KAFKA , Arrays . asList ( \"foo\" , \"bar\" )); @RegisterExtension @Order ( 2 ) static final DropwizardAppExtension < KafkaExampleConfiguration > DW = new DropwizardAppExtension <> ( KafkaExampleProducerApplication . class , resourceFilePath ( \"test-config-producer.yml\" ), config ( \"kafka.brokers\" , KAFKA :: getKafkaConnectString ));","title":"Create topics"},{"location":"server-kafka/","text":"SDA Dropwizard Commons Server Kafka \u00b6 This module provides a KafkaBundle adds convenient functionality to create Kafka consumers, producers, and topics via configuration or Java DSL. It additionally provides a default MessageListener that implements a polling loop for Kafka consumers. The user of this bundle must only implement the functional logic. Usage \u00b6 Add the following dependency: 1 compile 'org.sdase.commons:sda-commons-server-kafka:<current-version>' Add the Kafka Configuration to your configuration class: 1 2 3 4 5 6 7 public class YourConfiguration extends SdaPlatformConfiguration { // ... @NotNull @Valid private KafkaConfiguration kafka = new KafkaConfiguration (); // ... } Bootstrap the bundle (see below). Configuration Summary The following configuration can be used as base for a production-ready configuration. Not every service both consumes and produces , so the properties for producers , consumers , or listenerConfig can be removed if they don't apply to the service. Other defaults can be added, if needed by the service. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 kafka : # List of brokers to bootstrap consumer and provider connection. brokers : ${KAFKA_BROKERS:-[]} # Security information used for all consumers and providers to connect to kafka. security : user : \"${KAFKA_SECURITY_USER}\" password : \"${KAFKA_SECURITY_PASSWORD}\" protocol : ${KAFKA_SECURITY_PROTOCOL:-PLAINTEXT} saslMechanism : ${KAFKA_SECURITY_SASL_MECHANISM:-PLAIN} # Map with topic configurations. The key is the used as name to address the configuration within the code. topics : <your-topic-config-key> : name : ${KAFKA_TOPICS_<YOUR_CONFIG>_NAME:-default} # Map with producer configurations. The key is used as name to address the configuration within the code. # Can be removed if no producers are used. producers : <your-producer-config-key> : # Empty by default. Can be overridden by System Properties, so it must be part of this configuration. # Map with consumer configurations. The key is used as name/id to address the configuration within the code. # Can be removed if no consumers are used. consumers : <your-consumer-config-key> : group : ${KAFKA_CONSUMERS_<YOUR_CONFIG>_GROUP:-default} # Map with listener configurations that can be used within MessageListener creation. listenerConfig : <your-listener-config-key> : # Empty by default. Can be overridden by System Properties, so it must be part of this configuration. A possible documentation could look like this: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 ## Environment Variables The following environment variables can be used to configure the Docker container: // ... ### Kafka A short overview about what kind of messages the service consumes or produces. The following properties can be set: #### Broker connection * `KAFKA_BROKERS` _array_ * The list of Kafka brokers * Example: `[\"kafka-broker1:9092\", \"kafka-broker2:9092\"]` * Default: `[]` * `KAFKA_SECURITY_USER` _string_ * The username used to connect to Kafka. * Example: `\"\"` * `KAFKA_SECURITY_PASSWORD` _string_ * The password used to connect to Kafka. * Example: `\"\"` * `KAFKA_SECURITY_PROTOCOL` _string_ * The security protocol used by Kafka. * Example: `PLAINTEXT` or `SASL_SSL` * Default: `PLAINTEXT` * `KAFKA_SECURITY_SASL_MECHANISM` _string_ * The SASL mechanism to use to connect to the Kafka. * Example: `PLAIN, SCRAM-SHA-256, or SCRAM-SHA-512` * Default: `PLAIN` #### Topics * `KAFKA_TOPICS_<YOUR_CONFIG>_NAME` _string_ * Topic name where to read medical record updates. It is checked that the topic is configured in \"compact\" mode. * Example: `medicalRecordsTopic` * Default: `default` #### Consumers * `KAFKA_CONSUMERS_<YOUR_CONFIG>_GROUP` _string_ * Consumer group name for the consumer `YOUR_CONFIG` . * Example: `myConsumer` * Default: `default` Bootstrap The bundle got enhanced to allow more control and flexibility how Kafka messages are consumed and which commit strategy is used. The bundle should be added as field to the application since it provides methods for the creation of MessageProducer and MessageListener . The Builders for MessageListenerRegistration and ProducerRegistration supports the user in the creation of these complex configurable objects. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 public class DemoApplication { private final KafkaBundle < AppConfiguration > kafkaBundle = KafkaBundle . builder (). withConfigurationProvider ( AppConfiguration :: getKafka ). build (); private final MessageProducer < String , ProductBundle > producer ; public void initialize ( Bootstrap < AppConfiguration > bootstrap ) { bootstrap . addBundle ( kafkaBundle ); } public void run ( AppConfiguration configuration , Environment environment ) throws Exception { // register with default consumer and listener config // The handler implements the actual logic for the message processing // replace with your handler implementation MessageHandler < String , String > handler = record -> results . add ( record . value ()); ErrorHandler < Sting , String > errorHandler = new IgnoreAndProceedErrorHandler <> (); kafkaBundle . createMessageListener ( MessageListenerRegistration . < String , String > builder () . withDefaultListenerConfig () . forTopic ( topic ) // replace topic with your topic name . withDefaultConsumer () . withValueDeserializer ( new StringDeserializer ()) . withListenerStrategy ( new AutocommitMLS < String , String > ( handler , errorHandler )) . build ()); // register with custom consumer and listener configuration (e.g. 2 instances, poll every minute) // method returns list of listeners, one for each instance List < MessageListener > listener = kafkaBundle . createMessageListener ( MessageListenerRegistration . < String , String > builder () . withListenerConfig ( ListenerConfig . builder (). withPollInterval ( 60000 ). build ( 2 )) . forTopic ( \"topic\" ) // replace topic with your topic name . withConsumerConfig ( \"consumer2\" ) // use consumer config from config yaml . withListenerStrategy ( new AutocommitMLS < String , String > ( handler , errorHandler )) . build ()); // Create message producer with default KafkaProducer MessageProducer < String , ProductBundle > producer = kafkaBundle . registerProducerForTopic ( ProducerRegistration . < String , String > builder () . forTopic ( \"topic\" ) // simple topic definition (name only, partition and replication Factor 1) . withDefaultProducer () . build ()); // Create message with a detailed topic specification that checks the topic MessageProducer < String , String > producerConfigured = kafkaBundle . registerProducer ( ProducerRegistration . < String , String > builder () . forTopic ( TopicConfig . builder ( \"mytopic\" ). build ()) . withProducerConfig ( \"producer1\" ) // use producer config from config yaml . build ()); // JSON Example MessageHandler < String , SimpleEntity > jsonHandler = record -> jsonResults . add ( record . value ()); ErrorHandler < Sting , SimpleEntity > errorHandler = new IgnoreAndProceedErrorHandler <> (); List < MessageListener > jsonListener = kafkaBundle . createMessageListener ( MessageListenerRegistration . builder () . withDefaultListenerConfig () . forTopic ( topic ) . withDefaultConsumer () . withValueDeserializer ( new KafkaJsonDeserializer <> ( new ObjectMapper (), SimpleEntity . class )) . withListenerStrategy ( new AutocommitMLS < String , SimpleEntity > ( jsonHandler , errorHandler )) . build () ); MessageProducer < String , SimpleEntity > jsonProducer = kafkaBundle . registerProducer ( ProducerRegistration . builder () . forTopic ( topic ) . withDefaultProducer () . withKeySerializer ( new StringSerializer ()) . withValueSerializer ( new KafkaJsonSerializer <> ( new ObjectMapper ())). build ()); // plain consumer where the user has full control and take over responsibility to close te consumer // by name of a valid consumer configuration from config yaml KafkaConsumer < String , String > consumer = kafkaBundle . createConsumer ( new StringDeserializer (), new StringDeserializer (), \"consumer1\" ); // by given consumer configuration KafkaConsumer < String , String > consumer = kafkaBundle . createConsumer ( new StringDeserializer (), new StringDeserializer (), ConsumerConfig . < String , String > builder () . withGroup ( \"test-consumer\" ) . addConfig ( \"max.poll.records\" , \"100\" ) . addConfig ( \"enable.auto.commit\" , \"false\" ). build ()); // There are similar methods for producer creation } // Optional: make producer available via CDI @Produces annotation // Note: CDI is not included within the bundle. @Produces public MessageProducer < String , ProductBundle > produceKafkaProductBundleProducer () { return producer ; } } Connect to multiple Kafka Cluster in one application \u00b6 To connect to multiple Kafka Clusters in one application, one KafkaBundle for each Cluster must be used. A dedicated configuration for each bundle is needed as well. Only in this case, the name of the health check for each bundle must be customized. 1 2 3 4 5 6 public class YourConfiguration extends SdaPlatformConfiguration { private KafkaConfiguration kafkaExternal = new KafkaConfiguration (); private KafkaConfiguration kafkaInternal = new KafkaConfiguration (); // ... } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 public class DemoApplication { private final KafkaBundle < YourConfiguration > kafkaBundleExternal = KafkaBundle . builder () . withConfigurationProvider ( YourConfiguration :: getKafkaExternal ) . withHealthCheckName ( \"kafkaConnectionExternal\" ) . build (); private final KafkaBundle < YourConfiguration > kafkaBundleInternal = KafkaBundle . builder () . withConfigurationProvider ( YourConfiguration :: getKafkaInternal ) . withHealthCheckName ( \"kafkaConnectionInternal\" ) . build (); public void initialize ( Bootstrap < AppConfiguration > bootstrap ) { // ... bootstrap . addBundle ( kafkaBundleExternal ); bootstrap . addBundle ( kafkaBundleInternal ); } public void run ( AppConfiguration configuration , Environment environment ) throws Exception { // ... } } Known Kafka Problems \u00b6 There exists a known Kafka issue in the new consumer API KAFAK-4740 that throws potentially a org.apache.kafka.commons.errors.SerializationException when a record key/value can't be deserialized depending on deserializer implementation. This can result in an infinite loop because the poll is not committed and the next poll will throw this exception again. The wrapped deserialization approach offers a workaround where this exception is prevented and the processing can continue. But be aware that the key or value can be null in this case in both MessageHandler.handle() and ErrorHandler.handleError() methods. Another alternative is to implement your own Deserializer to have even more control and where you can potentially apply any fallback deserialization strategy. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 public class DemoApplication { private final KafkaBundle < AppConfiguration > kafkaBundle = KafkaBundle . builder () . withConfigurationProvider ( AppConfiguration :: getKafka ). build (); private final MessageProducer < String , ProductBundle > producer ; public void initialize ( Bootstrap < AppConfiguration > bootstrap ) { bootstrap . addBundle ( kafkaBundle ); } public void run ( AppConfiguration configuration , Environment environment ) throws Exception { // register with default consumer and listener config // The handler implements the actual logic for the message processing // ... // JSON Example with wrapped Deserializer to avoid DeseriliazationException, see Note below List < MessageListener > jsonListener = kafkaBundle . registerMessageHandler ( MessageHandlerRegistration . builder () . withDefaultListenerConfig () . forTopic ( topic ) . withDefaultConsumer () . withValueDeserializer ( new WrappedNoSerializationErrorDeserializer <> ( new KafkaJsonDeserializer <> ( new ObjectMapper (), SimpleEntity . class ))) . withListenerStrategy ( new AutocommitMLS < String , SimpleEntity > ( jsonHandler , errorHandler )) . build () ); } } Configuration \u00b6 To configure KafkaBundle add the following kafka block to your Dropwizard config.yml. The following config snippet shows an example configuration with descriptive comments: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 kafka : # For testing without a kafka in integration tests, the bundle api can be disabled. No consumers and providers will be created disabled : false # Admin client is used for checking and creating topics as well as for Health Checks adminConfig : # Timeout for request to the kafka admin url used by admin clients adminClientRequestTimeoutMs : 2000 # Admin Rest Api for accessing the accessing admin functionality adminEndpoint : - kafka-admin-api-1:9092 - kafka-admin-api-2:9092 # Admin Security information used for all calls against the Admin Rest API adminSecurity : user : user password : password protocol : SASL_SSL # List of brokers to bootstrap consumer and provider connection brokers : - kafka-broker-1:9092 - kafka-broker-2:9092 # Security information used for all consumers and providers to connect to kafka security : user : user password : password protocol : SASL_SSL saslMechanism : PLAIN # Additional configuration properties that are added to all consumers, producers, and the admin client # configuration key -> values as defined in the kafka documentation config : ssl.truststore.location : /my/truststore/location.jks # Map with consumer configurations. The key is used as name/id to address the configuration within the code. consumers : # id/name of the consumer configuration consumer1 : # name of the consumer group the consumer belongs to. If not defined, 'default' is the default group : newGroup config : # Deserializers can be set here within the configuration or within the builder DSL key.deserializer : org.apache.kafka.common.serialization.LongDeserializer value.deserializer : org.apache.kafka.common.serialization.LongDeserializer cons2 : # if no attribute \"group\" is given, a default is used config : fetch.min.bytes : 3000 auto.offset.reset : latest # Map with producer configurations. The key is used as name to address the configuration within the code. producers : # id/name of the producer config producer1 : # configuration key -> values as defined in the kafka documentation config : # Serializers can be set here within the configuration or within the builder DSL key.serializer : org.apache.kafka.common.serialization.LongSerializer value.serializer : org.apache.kafka.common.serialization.LongSerializer acks : all retries : 1000 # Map with topic configurations. The key is the name of the topic and is also used to address the configuration within the code topics : # id of the topic configuration topic1 : # topic key # topic name name : topic1-name topic2 : name : topic2-name # Map with listener configurations that can be used within MessageListener creation. listenerConfig : # id/name of the listener configuration listenerConfig1 : # Number of listener instances that will be generated. If > 1, several KafkaConsumer are generated. Kafka assigns these consumers # to different partitions of the consumed topic. Number instances should be smaller or equal to the number of partitions. instances : 1 # If the topic check is configured within the DSL, the listener waits this amount of ms before checking topic existence again. 0 will disable existence check even when configured in DSL topicMissingRetryMs : 60000 # Milliseconds to sleep between two poll intervals if no messages are available pollInterval : 200 You can disable the health check manually if Kafka is not essential for the functionality of your service, e.g. it's only used to invalidate a cache, notify about updates or other minor tasks that could fail without affecting the service so much it's no longer providing a viable functionality. This way your service can stay healthy even if the connection to Kafka is disrupted. 1 2 3 4 5 private KafkaBundle < KafkaTestConfiguration > bundle = KafkaBundle . builder () . withConfigurationProvider ( KafkaTestConfiguration :: getKafka ) . withoutHealthCheck () // disable kafkas health check . build (); Keep in mind that in this case a producer might fail if the broker is not available, so depending on the use case the producer should do appropriate error handling e.g. storing unprocessed messages in a queue until the broker is available again. If no error handling is done you might be better off not disabling the health check. Disabling the internal health check registers an external health check for monitoring purposes and to determine the fitness of the service. The connection to the broker can be monitored through Prometheus metrics without impacting the health of the service. Security Settings \u00b6 There are different configuration options to connect to a Kafka Broker. PLAINTEXT \u00b6 The server uses an unencrypted connection with no authentication. 1 2 security : protocol : PLAINTEXT # can be omitted, default value SSL \u00b6 The server uses an encrypted connection with no authentication. 1 2 security : protocol : SSL SASL_PLAINTEXT \u00b6 The server uses an unencrypted connection with PLAIN authentication. 1 2 3 4 5 security : protocol : SASL_PLAINTEXT saslMechanism : PLAIN # can be omitted, default value when username and password are specified user : user password : password SASL_SSL \u00b6 The server uses an encrypted connection with PLAIN authentication. 1 2 3 4 5 security : protocol : SASL_SSL saslMechanism : PLAIN # can be omitted, default value when username and password are specified user : user password : password Other SASL mechanisms \u00b6 Beside sasl.mechanism: PLAIN , the bundle also supports SCRAM-SHA-256 and SCRAM-SHA-512 . All mechanisms can be used with both SASL_PLAINTEXT and SASL_SSL . 1 2 3 4 5 security : protocol : SASL_PLAINTEXT # or SASL_SSL saslMechanism : SCRAM-SHA-512 # or SCRAM-SHA-256 user : user password : password Other mechanisms can be configured by overriding the config properties. Note that the properties can also be configured individually for each consumer, each producer, and the admin client. 1 2 3 config : sasl.mechanism : OTHER-VALUE sasl.jaas.config : \"org.apache.kafka.common.security.scram.ScramLoginModule required username='youruser' password='yourpassword';\" OR 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 consumers : yourConsumer : config : sasl.mechanism : OTHER-VALUE sasl.jaas.config : \"org.apache.kafka.common.security.scram.ScramLoginModule required username='youruser' password='yourpassword';\" producers : yourProducer : config : sasl.mechanism : OTHER-VALUE sasl.jaas.config : \"org.apache.kafka.common.security.scram.ScramLoginModule required username='youruser' password='yourpassword';\" adminConfig : config : sasl.mechanism : OTHER-VALUE sasl.jaas.config : \"org.apache.kafka.common.security.scram.ScramLoginModule required username='youruser' password='yourpassword';\" Custom Certificate Authority and Client Certificates \u00b6 SSL or SASL_SSL can also use Kafka brokers that have a self-signed or private-CA certificate. Use the Java-default system properties javax.net.ssl.trustStore and javax.net.ssl.trustStorePassword to provide the certificates (see KafkaBundleWithSslTruststoreIT ). For more control, configure the truststore only for the Kafka bundle and not for the complete JVM (see KafkaBundleWithSslIT ): 1 2 3 4 5 6 security : protocol : SSL config : ssl.truststore.location : /your/truststore/location.jks ssl.truststore.password : truststore-password This configuration option also supports providing client certificates in a custom keystore: 1 2 3 4 5 6 7 8 9 10 11 12 security : protocol : SSL config : # configure the truststore ssl.truststore.location : /your/truststore/location.jks ssl.truststore.password : truststore-password # configure the keystore with client-certificates ssl.keystore.location : /your/keystore/location.jks ssl.keystore.password : keystore-password ssl.key.password : cert-key-password Configuration value defaults (extending/changing the Kafka defaults) \u00b6 These are only the defaults that are explicitly set within the code of the bundle. All other properties depends on the actual broker configuration or the Kafka defaults are used. Key Value disabled false adminClientRequestTimeoutMs 5000 brokers \u00b6 No defaults security \u00b6 Key Value protocol PLAINTEXT consumers \u00b6 Key Value group default clientId Name of the consumer configuration. Sets Kafka's client.id . config -> enable.auto.commit true config -> auto.commit.interval.ms 1000 config -> auto.offset.reset earliest config -> key.deserializer org.apache.kafka.common.serialization.StringDeserializer config -> value.deserializer org.apache.kafka.common.serialization.StringDeserializer producers \u00b6 Key Value clientId Name of the producer configuration. Sets Kafka's client.id . config -> acks all config -> retries 0 config -> linger.ms 0 config -> key.serializer org.apache.kafka.common.serialization.StringSerializer config -> value.serializer org.apache.kafka.common.serialization.StringSerializer listenerConfig \u00b6 Key Value instances 1 topicMissingRetryMs 0 pollInterval 100 MessageListener \u00b6 A MessageListener MessageListener is a default poll loop implementation that correctly subscribes for some topics and includes additional features such as a graceful shutdown when the application stops. The message listener hands over the received consumer records to a MessageListenerStrategy that defines the message handling and the commit behavior. A strategy should use a MessageHandler and a ErrorHandler to separate business logic from commit logic as shown e.g. in AutocommitStrategy to make the strategy reusable Included MessageListenerStrategies \u00b6 The bundle provides some MessageListenerStrategy that can be reused in projects. A strategy is automatically inited with the Prometheus histogram class when using the builder methods. You may need to do that explicitly if you use strategies, e.g. in tests. Autocommit MessageListenerStrategy \u00b6 This strategy reads messages from the broker and passes the records to a message handler that must be implemented by the user of the bundle. The underlying consumer commits records periodically using the kafka config defaults. But, the MessageListener does not implement any extra logic in case of re-balancing. Therefore, the listener does not support an exact once semantic. It might occur that messages are redelivered after re-balance activities. SyncCommit MessageListenerStrategy \u00b6 This strategy reads messages from the broker and passes the records to a message handler that must be implemented by the user of the bundle. The strategy requires enable.auto.commit set to false and uses sync commit explicitly before polling a new chunk. Retry processing error MessageListenerStrategy \u00b6 This strategy reads messages from the broker and passes the records to a message handler that must be implemented by the user of the bundle. The strategy requires enable.auto.commit set to false and the underlying consumer commits records for each partition. In case of processing errors the handler should throw ProcessingErrorRetryException which is then delegated to the ErrorHandler where finally can be decided if the processing should be stopped or retried (handleError returns false ). In case of retry the consumer set the offset on the failing record and interrupt the processing of further records. The next poll will retry the records on this partition starting with the failing record. Create preconfigured consumers and producers \u00b6 To give the user more flexibility the bundle allows to create consumers and producers either by name of a valid configuration from the config YAML or by specifying a configuration in code. The user takes over the full responsibility and have to ensure that the consumer is closed when no longer used. Migration information (from kafka-commons) \u00b6 Compatibility with older broker versions Newer versions than kafka-commons v0.19.0 assumes at least version 1.0.0 of the broker, since some admin client commands are not supported in earlier broker versions If you use older versions of the broker, you MUST not use the options to check or create topics for MessageHandlers and MessageProducers. Now each MessageListener uses exactly one thread to handle the incoming consumer records. Commit is done after this thread returns from processing the implemented functional logic. In former versions of this bundle, there was a shared ThreadPool used by all MessageListener s. The business logic was handled in an own thread executed within the thread pool. Records has been committed directly after creating the threads and not after business logic execution. This hinder an ordered message processing as well as committing an offset after the business logic processing. This implementation must be considered when migrating older implementations since it might affect the performance. If you prefer the old behavior, you should create a thread pool within your MessageHandler implementation. Kafka Bundle with Managed Kafka \u00b6 See below for example configuration. Of course, you can configure further properties, analogously. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 kafka: brokers: ${KAFKA_BROKERS:-[]} security: user: ${SECRET_KAFKA_USERNAME} password: ${SECRET_KAFKA_PASSWORD} protocol: ${KAFKA_SECURITY_PROTOCOL} producers: producer1: config: key.serializer: org.apache.kafka.common.serialization.LongSerializer value.serializer: org.apache.kafka.common.serialization.LongSerializer acks: all ... Note : Do not use ; in passwords, as this will crash your application. In this case, the KAFKA_BROKERS variable should contain a JSON array with a list of broker 1 2 3 4 [ \"kafka-broker:12345\" , \"kafka-broker:54321\" ] Health check \u00b6 A health check with the name kafkaConnection is automatically registered to test the Kafka connection. The health check tries to list the topics available at the broker. Testing \u00b6 sda-commons-server-kafka-testing provides support for integration testing with Kafka with JUnit 4. Eventing \u00b6 If you want to use Kafka in the context of eventing when consuming or producing messages, you might check out our module sda-commons-server-cloudevents .","title":"Server Kafka"},{"location":"server-kafka/#sda-dropwizard-commons-server-kafka","text":"This module provides a KafkaBundle adds convenient functionality to create Kafka consumers, producers, and topics via configuration or Java DSL. It additionally provides a default MessageListener that implements a polling loop for Kafka consumers. The user of this bundle must only implement the functional logic.","title":"SDA Dropwizard Commons Server Kafka"},{"location":"server-kafka/#usage","text":"Add the following dependency: 1 compile 'org.sdase.commons:sda-commons-server-kafka:<current-version>' Add the Kafka Configuration to your configuration class: 1 2 3 4 5 6 7 public class YourConfiguration extends SdaPlatformConfiguration { // ... @NotNull @Valid private KafkaConfiguration kafka = new KafkaConfiguration (); // ... } Bootstrap the bundle (see below). Configuration Summary The following configuration can be used as base for a production-ready configuration. Not every service both consumes and produces , so the properties for producers , consumers , or listenerConfig can be removed if they don't apply to the service. Other defaults can be added, if needed by the service. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 kafka : # List of brokers to bootstrap consumer and provider connection. brokers : ${KAFKA_BROKERS:-[]} # Security information used for all consumers and providers to connect to kafka. security : user : \"${KAFKA_SECURITY_USER}\" password : \"${KAFKA_SECURITY_PASSWORD}\" protocol : ${KAFKA_SECURITY_PROTOCOL:-PLAINTEXT} saslMechanism : ${KAFKA_SECURITY_SASL_MECHANISM:-PLAIN} # Map with topic configurations. The key is the used as name to address the configuration within the code. topics : <your-topic-config-key> : name : ${KAFKA_TOPICS_<YOUR_CONFIG>_NAME:-default} # Map with producer configurations. The key is used as name to address the configuration within the code. # Can be removed if no producers are used. producers : <your-producer-config-key> : # Empty by default. Can be overridden by System Properties, so it must be part of this configuration. # Map with consumer configurations. The key is used as name/id to address the configuration within the code. # Can be removed if no consumers are used. consumers : <your-consumer-config-key> : group : ${KAFKA_CONSUMERS_<YOUR_CONFIG>_GROUP:-default} # Map with listener configurations that can be used within MessageListener creation. listenerConfig : <your-listener-config-key> : # Empty by default. Can be overridden by System Properties, so it must be part of this configuration. A possible documentation could look like this: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 ## Environment Variables The following environment variables can be used to configure the Docker container: // ... ### Kafka A short overview about what kind of messages the service consumes or produces. The following properties can be set: #### Broker connection * `KAFKA_BROKERS` _array_ * The list of Kafka brokers * Example: `[\"kafka-broker1:9092\", \"kafka-broker2:9092\"]` * Default: `[]` * `KAFKA_SECURITY_USER` _string_ * The username used to connect to Kafka. * Example: `\"\"` * `KAFKA_SECURITY_PASSWORD` _string_ * The password used to connect to Kafka. * Example: `\"\"` * `KAFKA_SECURITY_PROTOCOL` _string_ * The security protocol used by Kafka. * Example: `PLAINTEXT` or `SASL_SSL` * Default: `PLAINTEXT` * `KAFKA_SECURITY_SASL_MECHANISM` _string_ * The SASL mechanism to use to connect to the Kafka. * Example: `PLAIN, SCRAM-SHA-256, or SCRAM-SHA-512` * Default: `PLAIN` #### Topics * `KAFKA_TOPICS_<YOUR_CONFIG>_NAME` _string_ * Topic name where to read medical record updates. It is checked that the topic is configured in \"compact\" mode. * Example: `medicalRecordsTopic` * Default: `default` #### Consumers * `KAFKA_CONSUMERS_<YOUR_CONFIG>_GROUP` _string_ * Consumer group name for the consumer `YOUR_CONFIG` . * Example: `myConsumer` * Default: `default` Bootstrap The bundle got enhanced to allow more control and flexibility how Kafka messages are consumed and which commit strategy is used. The bundle should be added as field to the application since it provides methods for the creation of MessageProducer and MessageListener . The Builders for MessageListenerRegistration and ProducerRegistration supports the user in the creation of these complex configurable objects. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 public class DemoApplication { private final KafkaBundle < AppConfiguration > kafkaBundle = KafkaBundle . builder (). withConfigurationProvider ( AppConfiguration :: getKafka ). build (); private final MessageProducer < String , ProductBundle > producer ; public void initialize ( Bootstrap < AppConfiguration > bootstrap ) { bootstrap . addBundle ( kafkaBundle ); } public void run ( AppConfiguration configuration , Environment environment ) throws Exception { // register with default consumer and listener config // The handler implements the actual logic for the message processing // replace with your handler implementation MessageHandler < String , String > handler = record -> results . add ( record . value ()); ErrorHandler < Sting , String > errorHandler = new IgnoreAndProceedErrorHandler <> (); kafkaBundle . createMessageListener ( MessageListenerRegistration . < String , String > builder () . withDefaultListenerConfig () . forTopic ( topic ) // replace topic with your topic name . withDefaultConsumer () . withValueDeserializer ( new StringDeserializer ()) . withListenerStrategy ( new AutocommitMLS < String , String > ( handler , errorHandler )) . build ()); // register with custom consumer and listener configuration (e.g. 2 instances, poll every minute) // method returns list of listeners, one for each instance List < MessageListener > listener = kafkaBundle . createMessageListener ( MessageListenerRegistration . < String , String > builder () . withListenerConfig ( ListenerConfig . builder (). withPollInterval ( 60000 ). build ( 2 )) . forTopic ( \"topic\" ) // replace topic with your topic name . withConsumerConfig ( \"consumer2\" ) // use consumer config from config yaml . withListenerStrategy ( new AutocommitMLS < String , String > ( handler , errorHandler )) . build ()); // Create message producer with default KafkaProducer MessageProducer < String , ProductBundle > producer = kafkaBundle . registerProducerForTopic ( ProducerRegistration . < String , String > builder () . forTopic ( \"topic\" ) // simple topic definition (name only, partition and replication Factor 1) . withDefaultProducer () . build ()); // Create message with a detailed topic specification that checks the topic MessageProducer < String , String > producerConfigured = kafkaBundle . registerProducer ( ProducerRegistration . < String , String > builder () . forTopic ( TopicConfig . builder ( \"mytopic\" ). build ()) . withProducerConfig ( \"producer1\" ) // use producer config from config yaml . build ()); // JSON Example MessageHandler < String , SimpleEntity > jsonHandler = record -> jsonResults . add ( record . value ()); ErrorHandler < Sting , SimpleEntity > errorHandler = new IgnoreAndProceedErrorHandler <> (); List < MessageListener > jsonListener = kafkaBundle . createMessageListener ( MessageListenerRegistration . builder () . withDefaultListenerConfig () . forTopic ( topic ) . withDefaultConsumer () . withValueDeserializer ( new KafkaJsonDeserializer <> ( new ObjectMapper (), SimpleEntity . class )) . withListenerStrategy ( new AutocommitMLS < String , SimpleEntity > ( jsonHandler , errorHandler )) . build () ); MessageProducer < String , SimpleEntity > jsonProducer = kafkaBundle . registerProducer ( ProducerRegistration . builder () . forTopic ( topic ) . withDefaultProducer () . withKeySerializer ( new StringSerializer ()) . withValueSerializer ( new KafkaJsonSerializer <> ( new ObjectMapper ())). build ()); // plain consumer where the user has full control and take over responsibility to close te consumer // by name of a valid consumer configuration from config yaml KafkaConsumer < String , String > consumer = kafkaBundle . createConsumer ( new StringDeserializer (), new StringDeserializer (), \"consumer1\" ); // by given consumer configuration KafkaConsumer < String , String > consumer = kafkaBundle . createConsumer ( new StringDeserializer (), new StringDeserializer (), ConsumerConfig . < String , String > builder () . withGroup ( \"test-consumer\" ) . addConfig ( \"max.poll.records\" , \"100\" ) . addConfig ( \"enable.auto.commit\" , \"false\" ). build ()); // There are similar methods for producer creation } // Optional: make producer available via CDI @Produces annotation // Note: CDI is not included within the bundle. @Produces public MessageProducer < String , ProductBundle > produceKafkaProductBundleProducer () { return producer ; } }","title":"Usage"},{"location":"server-kafka/#connect-to-multiple-kafka-cluster-in-one-application","text":"To connect to multiple Kafka Clusters in one application, one KafkaBundle for each Cluster must be used. A dedicated configuration for each bundle is needed as well. Only in this case, the name of the health check for each bundle must be customized. 1 2 3 4 5 6 public class YourConfiguration extends SdaPlatformConfiguration { private KafkaConfiguration kafkaExternal = new KafkaConfiguration (); private KafkaConfiguration kafkaInternal = new KafkaConfiguration (); // ... } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 public class DemoApplication { private final KafkaBundle < YourConfiguration > kafkaBundleExternal = KafkaBundle . builder () . withConfigurationProvider ( YourConfiguration :: getKafkaExternal ) . withHealthCheckName ( \"kafkaConnectionExternal\" ) . build (); private final KafkaBundle < YourConfiguration > kafkaBundleInternal = KafkaBundle . builder () . withConfigurationProvider ( YourConfiguration :: getKafkaInternal ) . withHealthCheckName ( \"kafkaConnectionInternal\" ) . build (); public void initialize ( Bootstrap < AppConfiguration > bootstrap ) { // ... bootstrap . addBundle ( kafkaBundleExternal ); bootstrap . addBundle ( kafkaBundleInternal ); } public void run ( AppConfiguration configuration , Environment environment ) throws Exception { // ... } }","title":"Connect to multiple Kafka Cluster in one application"},{"location":"server-kafka/#known-kafka-problems","text":"There exists a known Kafka issue in the new consumer API KAFAK-4740 that throws potentially a org.apache.kafka.commons.errors.SerializationException when a record key/value can't be deserialized depending on deserializer implementation. This can result in an infinite loop because the poll is not committed and the next poll will throw this exception again. The wrapped deserialization approach offers a workaround where this exception is prevented and the processing can continue. But be aware that the key or value can be null in this case in both MessageHandler.handle() and ErrorHandler.handleError() methods. Another alternative is to implement your own Deserializer to have even more control and where you can potentially apply any fallback deserialization strategy. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 public class DemoApplication { private final KafkaBundle < AppConfiguration > kafkaBundle = KafkaBundle . builder () . withConfigurationProvider ( AppConfiguration :: getKafka ). build (); private final MessageProducer < String , ProductBundle > producer ; public void initialize ( Bootstrap < AppConfiguration > bootstrap ) { bootstrap . addBundle ( kafkaBundle ); } public void run ( AppConfiguration configuration , Environment environment ) throws Exception { // register with default consumer and listener config // The handler implements the actual logic for the message processing // ... // JSON Example with wrapped Deserializer to avoid DeseriliazationException, see Note below List < MessageListener > jsonListener = kafkaBundle . registerMessageHandler ( MessageHandlerRegistration . builder () . withDefaultListenerConfig () . forTopic ( topic ) . withDefaultConsumer () . withValueDeserializer ( new WrappedNoSerializationErrorDeserializer <> ( new KafkaJsonDeserializer <> ( new ObjectMapper (), SimpleEntity . class ))) . withListenerStrategy ( new AutocommitMLS < String , SimpleEntity > ( jsonHandler , errorHandler )) . build () ); } }","title":"Known Kafka Problems"},{"location":"server-kafka/#configuration","text":"To configure KafkaBundle add the following kafka block to your Dropwizard config.yml. The following config snippet shows an example configuration with descriptive comments: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 kafka : # For testing without a kafka in integration tests, the bundle api can be disabled. No consumers and providers will be created disabled : false # Admin client is used for checking and creating topics as well as for Health Checks adminConfig : # Timeout for request to the kafka admin url used by admin clients adminClientRequestTimeoutMs : 2000 # Admin Rest Api for accessing the accessing admin functionality adminEndpoint : - kafka-admin-api-1:9092 - kafka-admin-api-2:9092 # Admin Security information used for all calls against the Admin Rest API adminSecurity : user : user password : password protocol : SASL_SSL # List of brokers to bootstrap consumer and provider connection brokers : - kafka-broker-1:9092 - kafka-broker-2:9092 # Security information used for all consumers and providers to connect to kafka security : user : user password : password protocol : SASL_SSL saslMechanism : PLAIN # Additional configuration properties that are added to all consumers, producers, and the admin client # configuration key -> values as defined in the kafka documentation config : ssl.truststore.location : /my/truststore/location.jks # Map with consumer configurations. The key is used as name/id to address the configuration within the code. consumers : # id/name of the consumer configuration consumer1 : # name of the consumer group the consumer belongs to. If not defined, 'default' is the default group : newGroup config : # Deserializers can be set here within the configuration or within the builder DSL key.deserializer : org.apache.kafka.common.serialization.LongDeserializer value.deserializer : org.apache.kafka.common.serialization.LongDeserializer cons2 : # if no attribute \"group\" is given, a default is used config : fetch.min.bytes : 3000 auto.offset.reset : latest # Map with producer configurations. The key is used as name to address the configuration within the code. producers : # id/name of the producer config producer1 : # configuration key -> values as defined in the kafka documentation config : # Serializers can be set here within the configuration or within the builder DSL key.serializer : org.apache.kafka.common.serialization.LongSerializer value.serializer : org.apache.kafka.common.serialization.LongSerializer acks : all retries : 1000 # Map with topic configurations. The key is the name of the topic and is also used to address the configuration within the code topics : # id of the topic configuration topic1 : # topic key # topic name name : topic1-name topic2 : name : topic2-name # Map with listener configurations that can be used within MessageListener creation. listenerConfig : # id/name of the listener configuration listenerConfig1 : # Number of listener instances that will be generated. If > 1, several KafkaConsumer are generated. Kafka assigns these consumers # to different partitions of the consumed topic. Number instances should be smaller or equal to the number of partitions. instances : 1 # If the topic check is configured within the DSL, the listener waits this amount of ms before checking topic existence again. 0 will disable existence check even when configured in DSL topicMissingRetryMs : 60000 # Milliseconds to sleep between two poll intervals if no messages are available pollInterval : 200 You can disable the health check manually if Kafka is not essential for the functionality of your service, e.g. it's only used to invalidate a cache, notify about updates or other minor tasks that could fail without affecting the service so much it's no longer providing a viable functionality. This way your service can stay healthy even if the connection to Kafka is disrupted. 1 2 3 4 5 private KafkaBundle < KafkaTestConfiguration > bundle = KafkaBundle . builder () . withConfigurationProvider ( KafkaTestConfiguration :: getKafka ) . withoutHealthCheck () // disable kafkas health check . build (); Keep in mind that in this case a producer might fail if the broker is not available, so depending on the use case the producer should do appropriate error handling e.g. storing unprocessed messages in a queue until the broker is available again. If no error handling is done you might be better off not disabling the health check. Disabling the internal health check registers an external health check for monitoring purposes and to determine the fitness of the service. The connection to the broker can be monitored through Prometheus metrics without impacting the health of the service.","title":"Configuration"},{"location":"server-kafka/#security-settings","text":"There are different configuration options to connect to a Kafka Broker.","title":"Security Settings"},{"location":"server-kafka/#plaintext","text":"The server uses an unencrypted connection with no authentication. 1 2 security : protocol : PLAINTEXT # can be omitted, default value","title":"PLAINTEXT"},{"location":"server-kafka/#ssl","text":"The server uses an encrypted connection with no authentication. 1 2 security : protocol : SSL","title":"SSL"},{"location":"server-kafka/#sasl_plaintext","text":"The server uses an unencrypted connection with PLAIN authentication. 1 2 3 4 5 security : protocol : SASL_PLAINTEXT saslMechanism : PLAIN # can be omitted, default value when username and password are specified user : user password : password","title":"SASL_PLAINTEXT"},{"location":"server-kafka/#sasl_ssl","text":"The server uses an encrypted connection with PLAIN authentication. 1 2 3 4 5 security : protocol : SASL_SSL saslMechanism : PLAIN # can be omitted, default value when username and password are specified user : user password : password","title":"SASL_SSL"},{"location":"server-kafka/#other-sasl-mechanisms","text":"Beside sasl.mechanism: PLAIN , the bundle also supports SCRAM-SHA-256 and SCRAM-SHA-512 . All mechanisms can be used with both SASL_PLAINTEXT and SASL_SSL . 1 2 3 4 5 security : protocol : SASL_PLAINTEXT # or SASL_SSL saslMechanism : SCRAM-SHA-512 # or SCRAM-SHA-256 user : user password : password Other mechanisms can be configured by overriding the config properties. Note that the properties can also be configured individually for each consumer, each producer, and the admin client. 1 2 3 config : sasl.mechanism : OTHER-VALUE sasl.jaas.config : \"org.apache.kafka.common.security.scram.ScramLoginModule required username='youruser' password='yourpassword';\" OR 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 consumers : yourConsumer : config : sasl.mechanism : OTHER-VALUE sasl.jaas.config : \"org.apache.kafka.common.security.scram.ScramLoginModule required username='youruser' password='yourpassword';\" producers : yourProducer : config : sasl.mechanism : OTHER-VALUE sasl.jaas.config : \"org.apache.kafka.common.security.scram.ScramLoginModule required username='youruser' password='yourpassword';\" adminConfig : config : sasl.mechanism : OTHER-VALUE sasl.jaas.config : \"org.apache.kafka.common.security.scram.ScramLoginModule required username='youruser' password='yourpassword';\"","title":"Other SASL mechanisms"},{"location":"server-kafka/#custom-certificate-authority-and-client-certificates","text":"SSL or SASL_SSL can also use Kafka brokers that have a self-signed or private-CA certificate. Use the Java-default system properties javax.net.ssl.trustStore and javax.net.ssl.trustStorePassword to provide the certificates (see KafkaBundleWithSslTruststoreIT ). For more control, configure the truststore only for the Kafka bundle and not for the complete JVM (see KafkaBundleWithSslIT ): 1 2 3 4 5 6 security : protocol : SSL config : ssl.truststore.location : /your/truststore/location.jks ssl.truststore.password : truststore-password This configuration option also supports providing client certificates in a custom keystore: 1 2 3 4 5 6 7 8 9 10 11 12 security : protocol : SSL config : # configure the truststore ssl.truststore.location : /your/truststore/location.jks ssl.truststore.password : truststore-password # configure the keystore with client-certificates ssl.keystore.location : /your/keystore/location.jks ssl.keystore.password : keystore-password ssl.key.password : cert-key-password","title":"Custom Certificate Authority and Client Certificates"},{"location":"server-kafka/#configuration-value-defaults-extendingchanging-the-kafka-defaults","text":"These are only the defaults that are explicitly set within the code of the bundle. All other properties depends on the actual broker configuration or the Kafka defaults are used. Key Value disabled false adminClientRequestTimeoutMs 5000","title":"Configuration value defaults (extending/changing the Kafka defaults)"},{"location":"server-kafka/#brokers","text":"No defaults","title":"brokers"},{"location":"server-kafka/#security","text":"Key Value protocol PLAINTEXT","title":"security"},{"location":"server-kafka/#consumers","text":"Key Value group default clientId Name of the consumer configuration. Sets Kafka's client.id . config -> enable.auto.commit true config -> auto.commit.interval.ms 1000 config -> auto.offset.reset earliest config -> key.deserializer org.apache.kafka.common.serialization.StringDeserializer config -> value.deserializer org.apache.kafka.common.serialization.StringDeserializer","title":"consumers"},{"location":"server-kafka/#producers","text":"Key Value clientId Name of the producer configuration. Sets Kafka's client.id . config -> acks all config -> retries 0 config -> linger.ms 0 config -> key.serializer org.apache.kafka.common.serialization.StringSerializer config -> value.serializer org.apache.kafka.common.serialization.StringSerializer","title":"producers"},{"location":"server-kafka/#listenerconfig","text":"Key Value instances 1 topicMissingRetryMs 0 pollInterval 100","title":"listenerConfig"},{"location":"server-kafka/#messagelistener","text":"A MessageListener MessageListener is a default poll loop implementation that correctly subscribes for some topics and includes additional features such as a graceful shutdown when the application stops. The message listener hands over the received consumer records to a MessageListenerStrategy that defines the message handling and the commit behavior. A strategy should use a MessageHandler and a ErrorHandler to separate business logic from commit logic as shown e.g. in AutocommitStrategy to make the strategy reusable","title":"MessageListener"},{"location":"server-kafka/#included-messagelistenerstrategies","text":"The bundle provides some MessageListenerStrategy that can be reused in projects. A strategy is automatically inited with the Prometheus histogram class when using the builder methods. You may need to do that explicitly if you use strategies, e.g. in tests.","title":"Included MessageListenerStrategies"},{"location":"server-kafka/#autocommit-messagelistenerstrategy","text":"This strategy reads messages from the broker and passes the records to a message handler that must be implemented by the user of the bundle. The underlying consumer commits records periodically using the kafka config defaults. But, the MessageListener does not implement any extra logic in case of re-balancing. Therefore, the listener does not support an exact once semantic. It might occur that messages are redelivered after re-balance activities.","title":"Autocommit MessageListenerStrategy"},{"location":"server-kafka/#synccommit-messagelistenerstrategy","text":"This strategy reads messages from the broker and passes the records to a message handler that must be implemented by the user of the bundle. The strategy requires enable.auto.commit set to false and uses sync commit explicitly before polling a new chunk.","title":"SyncCommit MessageListenerStrategy"},{"location":"server-kafka/#retry-processing-error-messagelistenerstrategy","text":"This strategy reads messages from the broker and passes the records to a message handler that must be implemented by the user of the bundle. The strategy requires enable.auto.commit set to false and the underlying consumer commits records for each partition. In case of processing errors the handler should throw ProcessingErrorRetryException which is then delegated to the ErrorHandler where finally can be decided if the processing should be stopped or retried (handleError returns false ). In case of retry the consumer set the offset on the failing record and interrupt the processing of further records. The next poll will retry the records on this partition starting with the failing record.","title":"Retry processing error MessageListenerStrategy"},{"location":"server-kafka/#create-preconfigured-consumers-and-producers","text":"To give the user more flexibility the bundle allows to create consumers and producers either by name of a valid configuration from the config YAML or by specifying a configuration in code. The user takes over the full responsibility and have to ensure that the consumer is closed when no longer used.","title":"Create preconfigured consumers and producers"},{"location":"server-kafka/#migration-information-from-kafka-commons","text":"Compatibility with older broker versions Newer versions than kafka-commons v0.19.0 assumes at least version 1.0.0 of the broker, since some admin client commands are not supported in earlier broker versions If you use older versions of the broker, you MUST not use the options to check or create topics for MessageHandlers and MessageProducers. Now each MessageListener uses exactly one thread to handle the incoming consumer records. Commit is done after this thread returns from processing the implemented functional logic. In former versions of this bundle, there was a shared ThreadPool used by all MessageListener s. The business logic was handled in an own thread executed within the thread pool. Records has been committed directly after creating the threads and not after business logic execution. This hinder an ordered message processing as well as committing an offset after the business logic processing. This implementation must be considered when migrating older implementations since it might affect the performance. If you prefer the old behavior, you should create a thread pool within your MessageHandler implementation.","title":"Migration information (from kafka-commons)"},{"location":"server-kafka/#kafka-bundle-with-managed-kafka","text":"See below for example configuration. Of course, you can configure further properties, analogously. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 kafka: brokers: ${KAFKA_BROKERS:-[]} security: user: ${SECRET_KAFKA_USERNAME} password: ${SECRET_KAFKA_PASSWORD} protocol: ${KAFKA_SECURITY_PROTOCOL} producers: producer1: config: key.serializer: org.apache.kafka.common.serialization.LongSerializer value.serializer: org.apache.kafka.common.serialization.LongSerializer acks: all ... Note : Do not use ; in passwords, as this will crash your application. In this case, the KAFKA_BROKERS variable should contain a JSON array with a list of broker 1 2 3 4 [ \"kafka-broker:12345\" , \"kafka-broker:54321\" ]","title":"Kafka Bundle with Managed Kafka"},{"location":"server-kafka/#health-check","text":"A health check with the name kafkaConnection is automatically registered to test the Kafka connection. The health check tries to list the topics available at the broker.","title":"Health check"},{"location":"server-kafka/#testing","text":"sda-commons-server-kafka-testing provides support for integration testing with Kafka with JUnit 4.","title":"Testing"},{"location":"server-kafka/#eventing","text":"If you want to use Kafka in the context of eventing when consuming or producing messages, you might check out our module sda-commons-server-cloudevents .","title":"Eventing"},{"location":"server-key-mgmt/","text":"SDA Commons Server Key Management \u00b6 Bundle for key management in microservices. The main purpose is to provide configurable key management and mappings for keys and its values. This allows to define keys and mappings at deployment time and not at development time. So the actual keys can be adjusted according to the deployment scenario. The bundle also provides the annotation @PlatformKey(\"<keyName>\") to mark a String attribute to contain a valid key value. When defining keys at runtime, it might happen that keys used in business logic do not exist at runtime. This problem is not solved by the bundle. It must be considered as part of the key definition process. The bundle provides means to work with keys as well as to retrieve mapping values from or to a possible implementation depending on the keys. This includes * checking if a key is valid * checking if a value is valid for a given key * mapping of a key between API and implementation specific values API keys and values should be defined in snake case. For these two values, the bundle will be case tolerant, by mapping to uppercase internally. For implementations specific values, the bundle must not be case tolerant to provide keys as expected. The mapping between API and implementation of an API is necessary to define APIs completely independent of a concrete implementation. For example, when wrapping an API that fits into the platform around an existing implementation. Therefore, every value mapping consists of an api and an implementation value. The bundle provides a \"pass through\" default behavior for keys and mappings that are not known. This means, that the original value is just passed instead of being mapped. There is just a warning in the log files. This fail strategy can be configured as part of the builder. There is another fail strategy implemented ( FAIL_WITH_EXCEPTION ), that throws an IllegalArgumentException when no mapping can be found with no log message. Usage \u00b6 1 implementation 'org.sdase.commons:sda-commons-server-key-mgmt:<version>' Initialization of bundle: 1 2 3 4 5 private final KeyMgmtBundle < KeyMgmtBundleTestConfig > keyMgmt = KeyMgmtBundle . builder () . withKeyMgmtConfigProvider ( KeyMgmtBundleTestConfig :: getKeyMgmt ) . withFailStrategy ( FAIL_WITH_EXCEPTION ) . build (); The configuration includes the paths to the mapping and keys yaml files as well as the option to disable value validation. The following listing shows a yaml snippet for the keyMgmt configuration. 1 2 3 4 keyMgmt : apiKeysDefinitionPath : \"/keys\" # path to key definition yaml files mappingDefinitionPath : \"/mappings\" # path to mapping definition yaml files disableValidation : false # option to deactivate input value validation for keys. Default: false @PlatformKey \u00b6 The annotation @PlatformKey will trigger a validator to verify if the received value is a valid key for the referenced platform key. 1 2 @PlatformKey ( \"GENDER\" ) private String genderKey @PlatformKeys \u00b6 The annotation @PlatformKeys will trigger a validator to verify if the received value is a valid key within a list of referenced platform keys. 1 2 @PlatformKeys ( values = { \"GENDER, SALUTATION\" }) private String genderOrSalutationKey Key yaml file \u00b6 The yaml file or keys may contain one or more KeyDefinition documents. Example: 1 2 3 4 5 6 7 8 9 name : GENDER desciption : \"Gender of a human\" values : - value : MALE description : \"male gender\" - value : FEMALE description : \"female gender\" - value : Other description : \"other if male or female does not fit\" Mapping yaml file \u00b6 The yaml file for mappings may contain one or more KeyMappingModel documents. Example: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 name : GENDER mapping : apiToImplBidirectional : - api : \"MALE\" impl : \"m\" - api : \"FEMALE\" impl : \"F\" - api : \"OTHER\" impl : \"d\" --- name : SALUTATION mapping : apiToImplBidirectional : - api : \"MRS\" impl : \"1\" - api : \"MR\" impl : \"0\" implToApi : - impl : \"2\" api : \"MRS\"","title":"Server Key Management"},{"location":"server-key-mgmt/#sda-commons-server-key-management","text":"Bundle for key management in microservices. The main purpose is to provide configurable key management and mappings for keys and its values. This allows to define keys and mappings at deployment time and not at development time. So the actual keys can be adjusted according to the deployment scenario. The bundle also provides the annotation @PlatformKey(\"<keyName>\") to mark a String attribute to contain a valid key value. When defining keys at runtime, it might happen that keys used in business logic do not exist at runtime. This problem is not solved by the bundle. It must be considered as part of the key definition process. The bundle provides means to work with keys as well as to retrieve mapping values from or to a possible implementation depending on the keys. This includes * checking if a key is valid * checking if a value is valid for a given key * mapping of a key between API and implementation specific values API keys and values should be defined in snake case. For these two values, the bundle will be case tolerant, by mapping to uppercase internally. For implementations specific values, the bundle must not be case tolerant to provide keys as expected. The mapping between API and implementation of an API is necessary to define APIs completely independent of a concrete implementation. For example, when wrapping an API that fits into the platform around an existing implementation. Therefore, every value mapping consists of an api and an implementation value. The bundle provides a \"pass through\" default behavior for keys and mappings that are not known. This means, that the original value is just passed instead of being mapped. There is just a warning in the log files. This fail strategy can be configured as part of the builder. There is another fail strategy implemented ( FAIL_WITH_EXCEPTION ), that throws an IllegalArgumentException when no mapping can be found with no log message.","title":"SDA Commons Server Key Management"},{"location":"server-key-mgmt/#usage","text":"1 implementation 'org.sdase.commons:sda-commons-server-key-mgmt:<version>' Initialization of bundle: 1 2 3 4 5 private final KeyMgmtBundle < KeyMgmtBundleTestConfig > keyMgmt = KeyMgmtBundle . builder () . withKeyMgmtConfigProvider ( KeyMgmtBundleTestConfig :: getKeyMgmt ) . withFailStrategy ( FAIL_WITH_EXCEPTION ) . build (); The configuration includes the paths to the mapping and keys yaml files as well as the option to disable value validation. The following listing shows a yaml snippet for the keyMgmt configuration. 1 2 3 4 keyMgmt : apiKeysDefinitionPath : \"/keys\" # path to key definition yaml files mappingDefinitionPath : \"/mappings\" # path to mapping definition yaml files disableValidation : false # option to deactivate input value validation for keys. Default: false","title":"Usage"},{"location":"server-key-mgmt/#platformkey","text":"The annotation @PlatformKey will trigger a validator to verify if the received value is a valid key for the referenced platform key. 1 2 @PlatformKey ( \"GENDER\" ) private String genderKey","title":"@PlatformKey"},{"location":"server-key-mgmt/#platformkeys","text":"The annotation @PlatformKeys will trigger a validator to verify if the received value is a valid key within a list of referenced platform keys. 1 2 @PlatformKeys ( values = { \"GENDER, SALUTATION\" }) private String genderOrSalutationKey","title":"@PlatformKeys"},{"location":"server-key-mgmt/#key-yaml-file","text":"The yaml file or keys may contain one or more KeyDefinition documents. Example: 1 2 3 4 5 6 7 8 9 name : GENDER desciption : \"Gender of a human\" values : - value : MALE description : \"male gender\" - value : FEMALE description : \"female gender\" - value : Other description : \"other if male or female does not fit\"","title":"Key yaml file"},{"location":"server-key-mgmt/#mapping-yaml-file","text":"The yaml file for mappings may contain one or more KeyMappingModel documents. Example: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 name : GENDER mapping : apiToImplBidirectional : - api : \"MALE\" impl : \"m\" - api : \"FEMALE\" impl : \"F\" - api : \"OTHER\" impl : \"d\" --- name : SALUTATION mapping : apiToImplBidirectional : - api : \"MRS\" impl : \"1\" - api : \"MR\" impl : \"0\" implToApi : - impl : \"2\" api : \"MRS\"","title":"Mapping yaml file"},{"location":"server-mongo-testing/","text":"SDA Commons Server Mongo Testing \u00b6 This module provides the MongoDbClassExtension , a JUnit 5 test extension that is used to automatically bootstrap a MongoDB instance for integration tests. This is accomplished using Flapdoodle embedded MongoDB , that downloads and starts MongoDB in a separate process. Usage \u00b6 To create a MongoDB instance, add the MongoDB test extension to your test class: 1 2 3 4 5 6 7 8 @RegisterExtension @Order ( 0 ) static final MongoDbClassExtension MONGO_DB_EXTENSION = MongoDbClassExtension . builder () . withDatabase ( DATABASE_NAME ) . withUsername ( DATABASE_USERNAME ) . withPassword ( DATABASE_PASSWORD ) . build (); The test extension takes care to choose a free port for the database. You can access the database servers address using MONGO_DB_EXTENSION.getHosts() . Often one need to pass the server address to the constructor of another extension: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 import io.dropwizard.testing.junit5.DropwizardAppExtension ; public class PersistenceIT { @RegisterExtension @Order ( 0 ) static final MongoDbClassExtension MONGO_DB_EXTENSION = MongoDbClassExtension . builder (). build (); @RegisterExtension @Order ( 1 ) static final DropwizardAppExtension < MyApplicationConfiguration > DW = new DropwizardAppExtension <> ( MyApplication . class , ResourceHelpers . resourceFilePath ( \"test-config.yml\" ), ConfigOverride . config ( \"mongo.hosts\" , MONGO_DB_EXTENSION :: getHosts )); } The extension also provides a MONGO_DB_EXTENSION.clearDatabase() method to remove everything or the MONGO_DB_EXTENSION.clearCollections() method to remove all documents from the database between tests, without restarting the extension. To verify and modify the database during tests, MONGO_DB_EXTENSION.createClient() provides a way to access the database using the MongoClient . Scripting \u00b6 By default, scripting using JavaScript is disabled. You should avoid using it, as it can cause security issues. If you still need to use it, activate it using the build enableScripting() . MongoDB version \u00b6 Flapdoodles embedded MongoDB version is set to 4.4.x by default. If one needs a specific version the version can be set like this MongoDbClassExtension.builder().withVersion(specificMongoDbVersion).build() . Configuration in a special CI-environment \u00b6 Normally the mongod executable is downloaded directly from the MongoDB web page. However, in some CI-environments this behavior might be undesired, because of proxy servers, missing internet access, or to avoid downloading executables from untrusted sources. Therefore, it is possible to change the download location of the embedded mongod using the optional environment variable EMBEDDED_MONGO_DOWNLOAD_PATH . If EMBEDDED_MONGO_DOWNLOAD_PATH is set to http://example.com/download/ , the extension for example tries to download http://example.com/download/osx/mongodb-osx-ssl-x86_64-3.6.5.tgz . Use an Existing Database \u00b6 To test specific scenarios, e.g. a real database set up like in production, the extension can be configured to not bootstrap a database with Flapdoodle. A MongoDB Connection String must be set as environment variable TEST_MONGODB_CONNECTION_STRING in the build environment to reference the existing database: 1 2 3 # example for SDA SE internal MongoDB in local-infra export TEST_MONGODB_CONNECTION_STRING = mongodb://<user>:<password>@mongo-1:27118,mongo-2:27119,mongo-3:27120/testdb?authSource = admin ./gradlew check It may be required to create a Keystore and apply it to JVM that executes the tests if the database uses custom certificates. If such an environment variable is set, the MongoDbClassExtension will not start a local MongoDB but provides the configuration and an appropriate MongoClient to access the external database. To make this feature work, tests must provide all configuration to the application in test as shown in the example above must clean up the collections they modify because a single database is shared across tests cannot run in parallel because only a single database is shared across tests","title":"Server Mongo Testing"},{"location":"server-mongo-testing/#sda-commons-server-mongo-testing","text":"This module provides the MongoDbClassExtension , a JUnit 5 test extension that is used to automatically bootstrap a MongoDB instance for integration tests. This is accomplished using Flapdoodle embedded MongoDB , that downloads and starts MongoDB in a separate process.","title":"SDA Commons Server Mongo Testing"},{"location":"server-mongo-testing/#usage","text":"To create a MongoDB instance, add the MongoDB test extension to your test class: 1 2 3 4 5 6 7 8 @RegisterExtension @Order ( 0 ) static final MongoDbClassExtension MONGO_DB_EXTENSION = MongoDbClassExtension . builder () . withDatabase ( DATABASE_NAME ) . withUsername ( DATABASE_USERNAME ) . withPassword ( DATABASE_PASSWORD ) . build (); The test extension takes care to choose a free port for the database. You can access the database servers address using MONGO_DB_EXTENSION.getHosts() . Often one need to pass the server address to the constructor of another extension: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 import io.dropwizard.testing.junit5.DropwizardAppExtension ; public class PersistenceIT { @RegisterExtension @Order ( 0 ) static final MongoDbClassExtension MONGO_DB_EXTENSION = MongoDbClassExtension . builder (). build (); @RegisterExtension @Order ( 1 ) static final DropwizardAppExtension < MyApplicationConfiguration > DW = new DropwizardAppExtension <> ( MyApplication . class , ResourceHelpers . resourceFilePath ( \"test-config.yml\" ), ConfigOverride . config ( \"mongo.hosts\" , MONGO_DB_EXTENSION :: getHosts )); } The extension also provides a MONGO_DB_EXTENSION.clearDatabase() method to remove everything or the MONGO_DB_EXTENSION.clearCollections() method to remove all documents from the database between tests, without restarting the extension. To verify and modify the database during tests, MONGO_DB_EXTENSION.createClient() provides a way to access the database using the MongoClient .","title":"Usage"},{"location":"server-mongo-testing/#scripting","text":"By default, scripting using JavaScript is disabled. You should avoid using it, as it can cause security issues. If you still need to use it, activate it using the build enableScripting() .","title":"Scripting"},{"location":"server-mongo-testing/#mongodb-version","text":"Flapdoodles embedded MongoDB version is set to 4.4.x by default. If one needs a specific version the version can be set like this MongoDbClassExtension.builder().withVersion(specificMongoDbVersion).build() .","title":"MongoDB version"},{"location":"server-mongo-testing/#configuration-in-a-special-ci-environment","text":"Normally the mongod executable is downloaded directly from the MongoDB web page. However, in some CI-environments this behavior might be undesired, because of proxy servers, missing internet access, or to avoid downloading executables from untrusted sources. Therefore, it is possible to change the download location of the embedded mongod using the optional environment variable EMBEDDED_MONGO_DOWNLOAD_PATH . If EMBEDDED_MONGO_DOWNLOAD_PATH is set to http://example.com/download/ , the extension for example tries to download http://example.com/download/osx/mongodb-osx-ssl-x86_64-3.6.5.tgz .","title":"Configuration in a special CI-environment"},{"location":"server-mongo-testing/#use-an-existing-database","text":"To test specific scenarios, e.g. a real database set up like in production, the extension can be configured to not bootstrap a database with Flapdoodle. A MongoDB Connection String must be set as environment variable TEST_MONGODB_CONNECTION_STRING in the build environment to reference the existing database: 1 2 3 # example for SDA SE internal MongoDB in local-infra export TEST_MONGODB_CONNECTION_STRING = mongodb://<user>:<password>@mongo-1:27118,mongo-2:27119,mongo-3:27120/testdb?authSource = admin ./gradlew check It may be required to create a Keystore and apply it to JVM that executes the tests if the database uses custom certificates. If such an environment variable is set, the MongoDbClassExtension will not start a local MongoDB but provides the configuration and an appropriate MongoClient to access the external database. To make this feature work, tests must provide all configuration to the application in test as shown in the example above must clean up the collections they modify because a single database is shared across tests cannot run in parallel because only a single database is shared across tests","title":"Use an Existing Database"},{"location":"server-open-api/","text":"SDA Commons Server OpenAPI \u00b6 The module sda-commons-server-openapi is the base module to add OpenAPI support for applications in the SDA infrastructure. This package produces OpenApi 3.0 definitions . Usage \u00b6 In the application class, the bundle is added in the initialize method: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 public class ExampleApp extends Application < Configuration > { // ... @Override public void initialize ( Bootstrap < Configuration > bootstrap ) { // ... bootstrap . addBundle ( OpenApiBundle . builder () . addResourcePackageClass ( getClass ()) . build ()); // ... } } The above will scan resources in the package of the application class. Customize the OpenAPI definition with the OpenAPIDefinition on a class in the registered package or use a configuration file . Documentation Location \u00b6 The OpenAPI documentation base path is dependent on DropWizard's server.rootPath : as JSON: <server.rootPath>/openapi.json as YAML: <server.rootPath>/openapi.yaml Customization Options \u00b6 The packages scanned by OpenAPI: 1 2 3 4 5 OpenApiBundle . builder () //... . addResourcePackageClass ( getClass ()) . addResourcePackageClass ( Api . class ) . addResourcePackage ( \"my.package.containing.resources\" ) File Generation \u00b6 To automatically generate the OpenAPI spec and ensure that it is committed to version control, one can use a test like this: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 import static org.assertj.core.api.Assertions.assertThat ; import static org.sdase.commons.server.openapi.OpenApiFileHelper.normalizeOpenApiYaml ; import java.nio.file.Path ; import java.nio.file.Paths ; import org.junit.jupiter.api.Test ; import org.sdase.commons.server.testing.GoldenFileAssertions ; class OpenApiDocumentationTest { @Test void shouldHaveSameOpenApiInRepository () throws Exception { // receive the openapi.yaml from the OpenApiBundle var bundle = OpenApiBundle . builder () . addResourcePackage ( YourApplication . class . getPackageName ()) . build (); var openapi = bundle . generateOpenApiAsYaml (); assertThat ( openapi ). isNotNull (); // specify where you want your file to be stored Path filePath = Paths . get ( \"openapi.yaml\" ); // check and update the file GoldenFileAssertions . assertThat ( filePath ) . hasYamlContentAndUpdateGolden ( normalizeOpenApiYaml ( openapi )); } } This test uses the GoldenFileAssertions from sda-commons-server-testing and removes all contents that vary between tests (the servers key that contains random port numbers) with OpenApiFileHelper#nomalizeOpenApiYaml(String yaml) . Further Information \u00b6 Swagger-Core Annotations Best Practices in API Documentation Example \u00b6 config.yml - server.rootPath 1 2 server : rootPath : \"/api/*\" ExampleApp.java 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 package org.example.person.app ; import org.example.person.api.Api ; //... public class ExampleApp extends Application < Configuration > { // ... @Override public void initialize ( Bootstrap < Configuration > bootstrap ) { // ... bootstrap . addBundle ( OpenApiBundle . builder () . addResourcePackageClass ( Api . class ) . build ()); // ... } } Api.java - @OpenAPIDefinition 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 package org.example.person.api ; /// ... @OpenAPIDefinition ( info = @Info ( title = \"Example Api\" , version = \"1.2\" , description = \"Example Description\" , license = @License ( name = \"Apache License\" , url = \"https://www.apache.org/licenses/LICENSE-2.0.html\" ), contact = @Contact ( name = \"John Doe\" , email = \"john.doe@example.com\" ) ) ) @SecurityScheme ( type = SecuritySchemeType . HTTP , description = \"Passes the Bearer Token (SDA JWT) to the service class.\" , name = \"BEARER_TOKEN\" , scheme = \"bearer\" , bearerFormat = \"JWT\" ) public class Api {} PersonService.java - @Operation , @ApiResponse , @Content , @Schema @SecurityRequirement 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 package org.example.person.api ; //... @Path ( \"/persons\" ) public interface PersonService { @GET @Path ( \"/john-doe\" ) @Produces ( APPLICATION_JSON ) @Operation ( summary = \"Returns John Doe.\" , security = { @SecurityRequirement ( name = \"BEARER_TOKEN\" )}) @ApiResponse ( responseCode = \"200\" , description = \"Returns John Doe.\" , content = @Content ( schema = @Schema ( implementation = PersonResource . class ))) @ApiResponse ( responseCode = \"404\" , description = \"John Doe was not found.\" ) PersonResource getJohnDoe (); } PersonResource.java - @Schema 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 @Resource @Schema ( description = \"Person\" ) public class PersonResource { @Schema ( description = \"The person's first name.\" ) private final String firstName ; @Schema ( description = \"The person's last name.\" ) private final String lastName ; @Schema ( description = \"traits\" , example = \"[\\\"hipster\\\", \\\"generous\\\"]\" ) private final List < String > traits = new ArrayList <> (); @JsonCreator public PersonResource ( @JsonProperty ( \"firstName\" ) String firstName , @JsonProperty ( \"lastName\" ) String lastName , @JsonProperty ( \"traits\" ) List < String > traits ) { this . firstName = firstName ; this . lastName = lastName ; this . traits . addAll ( traits ); } public String getFirstName () { return firstName ; } public String getLastName () { return lastName ; } public List < String > getTraits () { return traits ; } } The generated documentation would be at: as JSON: /api/openapi.json as YAML: /api/openapi.yaml Handling example values \u00b6 The OpenApiBundle reads example annotations containing complex JSON instead of interpreting them as String. If the bundle encounters a value that could be interpreted as JSON, the value is parsed. If the value isn't JSON the value is interpreted as a string. If the example is supplied like example = \"{\\\"key\\\": false}\" the swagger definition will contain the example as example: {\"key\": false} . Use an existing OpenAPI file \u00b6 When working with the API first approach, it is possible to serve an existing OpenAPI file instead of generating it using Annotations. It is also possible to combine pre-existing and generated results into one file. custom-openapi.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 openapi : 3.0.1 info : title : A manually written OpenAPI file description : This is an example file that was written by hand contact : email : info@sda.se version : '1.1' paths : /house : # this path will be added put : summary : Update a house requestBody : content : application/json : schema : $ref : '#/components/schemas/House' responses : \"201\" : description : The house has been updated ... MyApplication.java 1 2 3 4 5 6 7 8 bootstrap . addBundle ( OpenApiBundle . builder () // optionally configure other resource packages. Note that the values from annotations will // override the settings from the imported openapi file. . addResourcePackageClass ( getClass ()) // provide the path to the existing openapi file (yaml or json) in the classpath . withExistingOpenAPIFromClasspathResource ( \"/custom-openapi.yaml\" ) . build ()); Note: Annotations such as @OpenAPIDefinition will override the contents of the provided OpenAPI file if they are found in a configured resource package.","title":"Server OpenApi"},{"location":"server-open-api/#sda-commons-server-openapi","text":"The module sda-commons-server-openapi is the base module to add OpenAPI support for applications in the SDA infrastructure. This package produces OpenApi 3.0 definitions .","title":"SDA Commons Server OpenAPI"},{"location":"server-open-api/#usage","text":"In the application class, the bundle is added in the initialize method: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 public class ExampleApp extends Application < Configuration > { // ... @Override public void initialize ( Bootstrap < Configuration > bootstrap ) { // ... bootstrap . addBundle ( OpenApiBundle . builder () . addResourcePackageClass ( getClass ()) . build ()); // ... } } The above will scan resources in the package of the application class. Customize the OpenAPI definition with the OpenAPIDefinition on a class in the registered package or use a configuration file .","title":"Usage"},{"location":"server-open-api/#documentation-location","text":"The OpenAPI documentation base path is dependent on DropWizard's server.rootPath : as JSON: <server.rootPath>/openapi.json as YAML: <server.rootPath>/openapi.yaml","title":"Documentation Location"},{"location":"server-open-api/#customization-options","text":"The packages scanned by OpenAPI: 1 2 3 4 5 OpenApiBundle . builder () //... . addResourcePackageClass ( getClass ()) . addResourcePackageClass ( Api . class ) . addResourcePackage ( \"my.package.containing.resources\" )","title":"Customization Options"},{"location":"server-open-api/#file-generation","text":"To automatically generate the OpenAPI spec and ensure that it is committed to version control, one can use a test like this: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 import static org.assertj.core.api.Assertions.assertThat ; import static org.sdase.commons.server.openapi.OpenApiFileHelper.normalizeOpenApiYaml ; import java.nio.file.Path ; import java.nio.file.Paths ; import org.junit.jupiter.api.Test ; import org.sdase.commons.server.testing.GoldenFileAssertions ; class OpenApiDocumentationTest { @Test void shouldHaveSameOpenApiInRepository () throws Exception { // receive the openapi.yaml from the OpenApiBundle var bundle = OpenApiBundle . builder () . addResourcePackage ( YourApplication . class . getPackageName ()) . build (); var openapi = bundle . generateOpenApiAsYaml (); assertThat ( openapi ). isNotNull (); // specify where you want your file to be stored Path filePath = Paths . get ( \"openapi.yaml\" ); // check and update the file GoldenFileAssertions . assertThat ( filePath ) . hasYamlContentAndUpdateGolden ( normalizeOpenApiYaml ( openapi )); } } This test uses the GoldenFileAssertions from sda-commons-server-testing and removes all contents that vary between tests (the servers key that contains random port numbers) with OpenApiFileHelper#nomalizeOpenApiYaml(String yaml) .","title":"File Generation"},{"location":"server-open-api/#further-information","text":"Swagger-Core Annotations Best Practices in API Documentation","title":"Further Information"},{"location":"server-open-api/#example","text":"config.yml - server.rootPath 1 2 server : rootPath : \"/api/*\" ExampleApp.java 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 package org.example.person.app ; import org.example.person.api.Api ; //... public class ExampleApp extends Application < Configuration > { // ... @Override public void initialize ( Bootstrap < Configuration > bootstrap ) { // ... bootstrap . addBundle ( OpenApiBundle . builder () . addResourcePackageClass ( Api . class ) . build ()); // ... } } Api.java - @OpenAPIDefinition 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 package org.example.person.api ; /// ... @OpenAPIDefinition ( info = @Info ( title = \"Example Api\" , version = \"1.2\" , description = \"Example Description\" , license = @License ( name = \"Apache License\" , url = \"https://www.apache.org/licenses/LICENSE-2.0.html\" ), contact = @Contact ( name = \"John Doe\" , email = \"john.doe@example.com\" ) ) ) @SecurityScheme ( type = SecuritySchemeType . HTTP , description = \"Passes the Bearer Token (SDA JWT) to the service class.\" , name = \"BEARER_TOKEN\" , scheme = \"bearer\" , bearerFormat = \"JWT\" ) public class Api {} PersonService.java - @Operation , @ApiResponse , @Content , @Schema @SecurityRequirement 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 package org.example.person.api ; //... @Path ( \"/persons\" ) public interface PersonService { @GET @Path ( \"/john-doe\" ) @Produces ( APPLICATION_JSON ) @Operation ( summary = \"Returns John Doe.\" , security = { @SecurityRequirement ( name = \"BEARER_TOKEN\" )}) @ApiResponse ( responseCode = \"200\" , description = \"Returns John Doe.\" , content = @Content ( schema = @Schema ( implementation = PersonResource . class ))) @ApiResponse ( responseCode = \"404\" , description = \"John Doe was not found.\" ) PersonResource getJohnDoe (); } PersonResource.java - @Schema 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 @Resource @Schema ( description = \"Person\" ) public class PersonResource { @Schema ( description = \"The person's first name.\" ) private final String firstName ; @Schema ( description = \"The person's last name.\" ) private final String lastName ; @Schema ( description = \"traits\" , example = \"[\\\"hipster\\\", \\\"generous\\\"]\" ) private final List < String > traits = new ArrayList <> (); @JsonCreator public PersonResource ( @JsonProperty ( \"firstName\" ) String firstName , @JsonProperty ( \"lastName\" ) String lastName , @JsonProperty ( \"traits\" ) List < String > traits ) { this . firstName = firstName ; this . lastName = lastName ; this . traits . addAll ( traits ); } public String getFirstName () { return firstName ; } public String getLastName () { return lastName ; } public List < String > getTraits () { return traits ; } } The generated documentation would be at: as JSON: /api/openapi.json as YAML: /api/openapi.yaml","title":"Example"},{"location":"server-open-api/#handling-example-values","text":"The OpenApiBundle reads example annotations containing complex JSON instead of interpreting them as String. If the bundle encounters a value that could be interpreted as JSON, the value is parsed. If the value isn't JSON the value is interpreted as a string. If the example is supplied like example = \"{\\\"key\\\": false}\" the swagger definition will contain the example as example: {\"key\": false} .","title":"Handling example values"},{"location":"server-open-api/#use-an-existing-openapi-file","text":"When working with the API first approach, it is possible to serve an existing OpenAPI file instead of generating it using Annotations. It is also possible to combine pre-existing and generated results into one file. custom-openapi.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 openapi : 3.0.1 info : title : A manually written OpenAPI file description : This is an example file that was written by hand contact : email : info@sda.se version : '1.1' paths : /house : # this path will be added put : summary : Update a house requestBody : content : application/json : schema : $ref : '#/components/schemas/House' responses : \"201\" : description : The house has been updated ... MyApplication.java 1 2 3 4 5 6 7 8 bootstrap . addBundle ( OpenApiBundle . builder () // optionally configure other resource packages. Note that the values from annotations will // override the settings from the imported openapi file. . addResourcePackageClass ( getClass ()) // provide the path to the existing openapi file (yaml or json) in the classpath . withExistingOpenAPIFromClasspathResource ( \"/custom-openapi.yaml\" ) . build ()); Note: Annotations such as @OpenAPIDefinition will override the contents of the provided OpenAPI file if they are found in a configured resource package.","title":"Use an existing OpenAPI file"},{"location":"server-openapi-example/","text":"SDA Commons Server OpenAPI Example \u00b6 This example module shows an application that uses the OpenApiBundle to describe REST endpoints with a OpenApi 3 documentation. Beside the initialization of the bundle via the SdaPlatformBundle , it includes a PersonService and a PersonResource to demonstrate some cases of API documentation. The integration test shows how the existence of a OpenAPI endpoint can be tested. The provided local-config.yaml allows to start the application without the need for authentication locally using a run configuration of the favourite IDE that defines the program arguments server sda-commons-server-openapi-example/local-config.yaml . Swagger documentation is available at GET /openapi.json or GET /openapi.yaml , you may use the Swagger Editor or Swagger UI to view the documentation. The config.yaml is an example how the application can be started in production. Such file should be copied in the Docker container so that the variables can be populated using the environment configured by the orchestration tool (e.g. Kubernetes).","title":"Server OpenApi Example"},{"location":"server-openapi-example/#sda-commons-server-openapi-example","text":"This example module shows an application that uses the OpenApiBundle to describe REST endpoints with a OpenApi 3 documentation. Beside the initialization of the bundle via the SdaPlatformBundle , it includes a PersonService and a PersonResource to demonstrate some cases of API documentation. The integration test shows how the existence of a OpenAPI endpoint can be tested. The provided local-config.yaml allows to start the application without the need for authentication locally using a run configuration of the favourite IDE that defines the program arguments server sda-commons-server-openapi-example/local-config.yaml . Swagger documentation is available at GET /openapi.json or GET /openapi.yaml , you may use the Swagger Editor or Swagger UI to view the documentation. The config.yaml is an example how the application can be started in production. Such file should be copied in the Docker container so that the variables can be populated using the environment configured by the orchestration tool (e.g. Kubernetes).","title":"SDA Commons Server OpenAPI Example"},{"location":"server-opentelemetry-example/","text":"SDA Commons Server OpenTelemetry Example \u00b6 This module is an example for a service with manual instrumentation using OpenTelemetry . It also provides an example for how to add custom manual instrumentation in case it is needed. See the OpenTelemetryTracingApp for the examples. How to run the example \u00b6 Start the example app and pass server config.yml as command line arguments and - otel.exporter.otlp.endpoint=http://jaeger-host:4317 . You also have to start Jaeger, for example using the Jaeger all-in-one image . Afterwards you can perform the following requests: http://localhost:8080/ http://localhost:8080/recursive http://localhost:8080/error http://localhost:8080/search http://localhost:8080/instrumented http://localhost:8080/param/value","title":"Server Open Telemetry Example"},{"location":"server-opentelemetry-example/#sda-commons-server-opentelemetry-example","text":"This module is an example for a service with manual instrumentation using OpenTelemetry . It also provides an example for how to add custom manual instrumentation in case it is needed. See the OpenTelemetryTracingApp for the examples.","title":"SDA Commons Server OpenTelemetry Example"},{"location":"server-opentelemetry-example/#how-to-run-the-example","text":"Start the example app and pass server config.yml as command line arguments and - otel.exporter.otlp.endpoint=http://jaeger-host:4317 . You also have to start Jaeger, for example using the Jaeger all-in-one image . Afterwards you can perform the following requests: http://localhost:8080/ http://localhost:8080/recursive http://localhost:8080/error http://localhost:8080/search http://localhost:8080/instrumented http://localhost:8080/param/value","title":"How to run the example"},{"location":"server-opentelemetry/","text":"SDA Commons Server OpenTelemetry \u00b6 This bundle is responsible for loading the creating an OpenTelemetry Sdk instance and registering it as a global instance ready to use everywhere in dependent applications. The module also creating server traces to insure proper context propagation. Docs \u00b6 An extensive documentation can be found in OpenTelemetry Java documentation . Migrating from OpenTracing \u00b6 Here you can find a migration guide , if you use OpenTracing on your project. Usage \u00b6 The bundle must be initialized before other bundles in the dependent applications, as it is responsible for initializing the openTelemetry Sdk and registering the created instance as global, so that dependent bundles can use it. 1 api project ( ':sda-commons-server-opentelemetry' ) Then the bundle is ready to be added to the application. 1 2 3 4 5 6 7 8 9 10 11 12 public class MyApp extends Application < Configuration > { @Override public void run ( Configuration configuration , Environment environment ) {} @Override public void initialize ( Bootstrap < Configuration > bootstrap ) { // use the autoConfigured module bootstrap . addBundle ( OpenTelemetryBundle . builder (). withAutoConfiguredTelemetryInstance (). build ()); // ... other bundles } } If the application is already using the starter bundle , no changes are needed. The module is already added and configured with environment variables. NOTE: Except for starter bundle , no other bundle must depend on this one. The bundle registers a global Telemetry instance that is used all across the application has a safety mechanism to prevent registering a new instance twice. Configuration \u00b6 The OpenTelemetry sdk is highly configurable! Many aspects of its behavior can be configured for your needs, such as exporter choice, exporter config (like where data is sent), trace context propagation headers... The configuration can be done either with environment variables or system properties. Please note that system properties will have a higher priority over environment variables. A full list of the configurable properties can be found in the autoconfigure module . It is recommended to use same configuration across all services in the same domain, to ensure the right context propagation, and export traces to the same monitoring backend. Service Name \u00b6 The service name is used to identify the source (the sender) of the received telemetry data in the monitoring backend. Therefore, it's important that every service has a unique name. To provide the application with a custom name for each deployment, the OTEL_SERVICE_NAME environment variable must be used. Basic configuration \u00b6 By default, the module exports traces in the otlp format, to a Jaeger collector . This can be very handy for less local setup overhead, where the All-in-one Jaeger image can be enough. - OTEL_SERVICE_NAME=my-service - OTEL_EXPORTER_OTLP_ENDPOINT=http://jaeger-collector-host:4317 Or in case if the collector is deployed as a sidecar: - OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4317 Using other exporters \u00b6 To export traces in a different format to an openTelemetry collector , configure the exporter and exporter endpoint accordingly e.g.: - OTEL_TRACES_EXPORTER=zipkin - OTEL_EXPORTER_ZIPKIN_ENDPOINT=http://zipkin-host:9411 Disable Tracing \u00b6 In order to disable tracing in the applications that are using this bundle, or the starter bundle , the environment variable TRACING_DISABLED=true can be used. Setting TRACING_DISABLED to false will force the instrumented modules provided by sda-commons to use a no-op instance. Manual instrumentation \u00b6 Sda commons already offers the necessary instrumentation for the server and some clients, to insure a better overview about service to service interaction. It is advised to avoid unnecessary tracing for interaction with external systems and expect/rely on and generic instrumentation provided by sda-commons. If additional internal behaviour should to be traced, an OpenTelemetry instance can be acquired using GlobalOpenTelemetry.get() . A very basic skeleton for a creating more traces: 1 2 3 4 5 6 7 8 9 10 11 12 13 public class Component { // ... public void doSomething () { // ... var tracer = GlobalTelemetry . get (). getTracer ( \"sda-commons.component\" ); Span span = tracer . spanBuilder ( \"doSomething\" ). startSpan (); try ( Scope ignored = span . makeCurrent ()) { // The actual work } finally { span . end (); } } } Some examples for manual tracing can be found in OpenTelemetry manual tracing example .","title":"Server Open Telemetry"},{"location":"server-opentelemetry/#sda-commons-server-opentelemetry","text":"This bundle is responsible for loading the creating an OpenTelemetry Sdk instance and registering it as a global instance ready to use everywhere in dependent applications. The module also creating server traces to insure proper context propagation.","title":"SDA Commons Server OpenTelemetry"},{"location":"server-opentelemetry/#docs","text":"An extensive documentation can be found in OpenTelemetry Java documentation .","title":"Docs"},{"location":"server-opentelemetry/#migrating-from-opentracing","text":"Here you can find a migration guide , if you use OpenTracing on your project.","title":"Migrating from OpenTracing"},{"location":"server-opentelemetry/#usage","text":"The bundle must be initialized before other bundles in the dependent applications, as it is responsible for initializing the openTelemetry Sdk and registering the created instance as global, so that dependent bundles can use it. 1 api project ( ':sda-commons-server-opentelemetry' ) Then the bundle is ready to be added to the application. 1 2 3 4 5 6 7 8 9 10 11 12 public class MyApp extends Application < Configuration > { @Override public void run ( Configuration configuration , Environment environment ) {} @Override public void initialize ( Bootstrap < Configuration > bootstrap ) { // use the autoConfigured module bootstrap . addBundle ( OpenTelemetryBundle . builder (). withAutoConfiguredTelemetryInstance (). build ()); // ... other bundles } } If the application is already using the starter bundle , no changes are needed. The module is already added and configured with environment variables. NOTE: Except for starter bundle , no other bundle must depend on this one. The bundle registers a global Telemetry instance that is used all across the application has a safety mechanism to prevent registering a new instance twice.","title":"Usage"},{"location":"server-opentelemetry/#configuration","text":"The OpenTelemetry sdk is highly configurable! Many aspects of its behavior can be configured for your needs, such as exporter choice, exporter config (like where data is sent), trace context propagation headers... The configuration can be done either with environment variables or system properties. Please note that system properties will have a higher priority over environment variables. A full list of the configurable properties can be found in the autoconfigure module . It is recommended to use same configuration across all services in the same domain, to ensure the right context propagation, and export traces to the same monitoring backend.","title":"Configuration"},{"location":"server-opentelemetry/#service-name","text":"The service name is used to identify the source (the sender) of the received telemetry data in the monitoring backend. Therefore, it's important that every service has a unique name. To provide the application with a custom name for each deployment, the OTEL_SERVICE_NAME environment variable must be used.","title":"Service Name"},{"location":"server-opentelemetry/#basic-configuration","text":"By default, the module exports traces in the otlp format, to a Jaeger collector . This can be very handy for less local setup overhead, where the All-in-one Jaeger image can be enough. - OTEL_SERVICE_NAME=my-service - OTEL_EXPORTER_OTLP_ENDPOINT=http://jaeger-collector-host:4317 Or in case if the collector is deployed as a sidecar: - OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4317","title":"Basic configuration"},{"location":"server-opentelemetry/#using-other-exporters","text":"To export traces in a different format to an openTelemetry collector , configure the exporter and exporter endpoint accordingly e.g.: - OTEL_TRACES_EXPORTER=zipkin - OTEL_EXPORTER_ZIPKIN_ENDPOINT=http://zipkin-host:9411","title":"Using other exporters"},{"location":"server-opentelemetry/#disable-tracing","text":"In order to disable tracing in the applications that are using this bundle, or the starter bundle , the environment variable TRACING_DISABLED=true can be used. Setting TRACING_DISABLED to false will force the instrumented modules provided by sda-commons to use a no-op instance.","title":"Disable Tracing"},{"location":"server-opentelemetry/#manual-instrumentation","text":"Sda commons already offers the necessary instrumentation for the server and some clients, to insure a better overview about service to service interaction. It is advised to avoid unnecessary tracing for interaction with external systems and expect/rely on and generic instrumentation provided by sda-commons. If additional internal behaviour should to be traced, an OpenTelemetry instance can be acquired using GlobalOpenTelemetry.get() . A very basic skeleton for a creating more traces: 1 2 3 4 5 6 7 8 9 10 11 12 13 public class Component { // ... public void doSomething () { // ... var tracer = GlobalTelemetry . get (). getTracer ( \"sda-commons.component\" ); Span span = tracer . spanBuilder ( \"doSomething\" ). startSpan (); try ( Scope ignored = span . makeCurrent ()) { // The actual work } finally { span . end (); } } } Some examples for manual tracing can be found in OpenTelemetry manual tracing example .","title":"Manual instrumentation"},{"location":"server-prometheus-example/","text":"SDA Commons Server Prometheus Example \u00b6 This example module shows a dummy implementation of a business service that records metrics in three different types: A Histogram tracks durations A Counter tracks the number of invocations or events (e.g. successful invocations) A Gauge tracks the current value of a state, e.g. used memory, free disk space or business data","title":"SDA Commons Server Prometheus Example"},{"location":"server-prometheus-example/#sda-commons-server-prometheus-example","text":"This example module shows a dummy implementation of a business service that records metrics in three different types: A Histogram tracks durations A Counter tracks the number of invocations or events (e.g. successful invocations) A Gauge tracks the current value of a state, e.g. used memory, free disk space or business data","title":"SDA Commons Server Prometheus Example"},{"location":"server-prometheus/","text":"SDA Commons Server Prometheus \u00b6 The module sda-commons-server-prometheus provides an admin endpoint to serve metrics and health check results in a format that Prometheus can read. The endpoint is available at the applications admin port at /metrics/prometheus . Provided metrics \u00b6 Default metrics that are provided at /metrics/prometheus : Metric name Labels Description Source Deprecated jvm_ * Multiple metrics about the JVM Bridged from Dropwizard Since v5.3.13 io_dropwizard_jetty_ Multiple metrics from the embedded Jetty server Bridged from Dropwizard io_dropwizard_db_ Multiple metrics from the database if a database is used Bridged from Dropwizard healthcheck_status name Metrics that represent the state of the health checks HealthCheckMetricsCollector A filter that extracts the consumer from the HTTP headers should add Consumer-Name to the request properties. That filter is not part of the PrometheusBundle . Micrometer Metrics \u00b6 We are slowly shifting over from Dropwizard-Metrics to Micrometer . The following metrics are available so far. JVM \u00b6 Metric Name Labels Description Source jvm_classes_loaded_classes The number of classes that are currently loaded in the Java virtual machine. Bridged from Micrometer jvm_classes_unloaded_classes The total number of classes unloaded since the Java virtual machine has started execution. Bridged from Micrometer jvm_buffer_count_buffers id An estimate of the number of buffers in the pool. Bridged from Micrometer jvm_buffer_memory_used_bytes id An estimate of the memory that the Java virtual machine is using for this buffer pool. Bridged from Micrometer jvm_buffer_total_capacity_bytes id An estimate of the total capacity of the buffers in this pool. Bridged from Micrometer jvm_memory_used_bytes id , area The amount of used memory. Bridged from Micrometer jvm_memory_committed_bytes id , area The amount of memory in bytes that is committed for the Java virtual machine to use. Bridged from Micrometer jvm_memory_max_bytes id , area The maximum amount of memory in bytes that can be used for memory management. Bridged from Micrometer jvm_gc_max_data_size_bytes Max size of long-lived heap memory pool. Bridged from Micrometer jvm_gc_live_data_size_bytes Size of long-lived heap memory pool after reclamation. Bridged from Micrometer jvm_gc_memory_allocated_bytes_total Incremented for an increase in the size of the (young) heap memory pool after one GC to before the next. Bridged from Micrometer jvm_gc_memory_promoted_bytes_total Count of positive increases in the size of the old generation memory pool before GC to after GC. Bridged from Micrometer jvm_gc_concurrent_phase_time_seconds_count gc , action , cause Time spent in concurrent phase. Bridged from Micrometer jvm_gc_concurrent_phase_time_seconds_sum gc , action , cause Bridged from Micrometer jvm_gc_concurrent_phase_time_seconds_max gc , action , cause Bridged from Micrometer jvm_gc_pause_seconds_count gc , action , cause Time spent in GC pause. Bridged from Micrometer jvm_gc_pause_seconds_sum gc , action , cause Bridged from Micrometer jvm_gc_pause_seconds_max gc , action , cause Bridged from Micrometer system_cpu_count The number of processors available to the Java virtual machine. Bridged from Micrometer system_load_average_1m The sum of the number of runnable entities queued to available processors and the number of runnable entities running on the available processors averaged over a period of time. Bridged from Micrometer system_cpu_usage The \"recent cpu usage\" of the system the application is running in. Bridged from Micrometer process_cpu_usage The \"recent cpu usage\" for the Java Virtual Machine process. Bridged from Micrometer jvm_threads_peak_threads The peak live thread count since the Java virtual machine started or peak was reset. Bridged from Micrometer jvm_threads_daemon_threads The current number of live daemon threads. Bridged from Micrometer jvm_threads_live The current number of live threads including both daemon and non-daemon threads. Bridged from Micrometer jvm_threads_started_threads_total The total number of application threads started in the JVM. Bridged from Micrometer jvm_threads_states_threads state The current number of threads. Bridged from Micrometer jvm_classes_loaded The number of classes that are currently loaded in the Java virtual machine. Bridged from Micrometer jvm_classes_unloaded The total number of classes unloaded since the Java virtual machine has started execution. Bridged from Micrometer Kafka \u00b6 Kafka internal producer metrics have been exposed Metric name Labels Description Source kafka_producer_batch_split_rate client.id The average number of batch splits per second kafka_producer_batch_split_total client.id The total number of batch splits kafka_producer_batch_size_max client.id The max number of bytes sent per partition per-request. kafka_producer_batch_size_avg client.id The average number of bytes sent per partition per-request. kafka_producer_buffer_exhausted_rate client.id The average per-second number of record sends that are dropped due to buffer exhaustion kafka_producer_buffer_total_bytes client.id The maximum amount of buffer memory the client can use (whether or not it is currently used). kafka_producer_buffer_available_bytes client.id The total amount of buffer memory that is not being used (either unallocated or in the free list). kafka_producer_buffer_exhausted_total client.id The total number of record sends that are dropped due to buffer exhaustion kafka_producer_bufferpool_wait_time_ns_total client.id The total time in nanoseconds an appender waits for space allocation. kafka_producer_bufferpool_wait_ratio client.id The fraction of time an appender waits for space allocation. kafka_producer_compression_rate_avg client.id The average compression rate of record batches, defined as the average ratio of the compressed batch size over the uncompressed size. kafka_producer_connection_creation_total client.id The total number of new connections established kafka_producer_connection_close_rate client.id The number of connections closed per second kafka_producer_connection_count client.id The current number of active connections. kafka_producer_connection_creation_rate client.id The number of new connections established per second kafka_producer_connection_close_total client.id The total number of connections closed kafka_producer_node_incoming_byte_total client.id , node.id The total number of incoming bytes kafka_producer_node_request_total client.id , node.id The total number of requests sent kafka_producer_node_request_size_max client.id , node.id The maximum size of any request sent. kafka_producer_node_outgoing_byte_total client.id , node.id The total number of outgoing bytes kafka_producer_node_response_total client.id , node.id The total number of responses received kafka_producer_node_request_size_avg client.id , node.id The average size of requests sent. kafka_producer_node_response_rate client.id , node.id The number of responses received per second kafka_producer_node_request_latency_avg client.id , node.id The average latency of a producer request kafka_producer_node_incoming_byte_rate client.id , node.id The number of incoming bytes per second kafka_producer_node_outgoing_byte_rate client.id , node.id The number of outgoing bytes per second kafka_producer_node_request_rate client.id , node.id The number of requests sent per second kafka_producer_node_request_latency_max client.id , node.id The maximum latency of a producer request kafka_producer_record_queue_time_avg client.id The average time in ms record batches spent in the send buffer. kafka_producer_record_send_total client.id The total number of records sent. kafka_producer_record_size_max client.id The maximum record size kafka_producer_record_error_rate client.id The average per-second number of record sends that resulted in errors kafka_producer_record_error_total client.id The total number of record sends that resulted in errors kafka_producer_records_per_request_avg client.id The average number of records per request. kafka_producer_record_size_avg client.id The average record size kafka_producer_record_queue_time_max client.id The maximum time in ms record batches spent in the send buffer. kafka_producer_record_send_rate client.id The average number of records sent per second. kafka_producer_record_retry_total client.id The total number of retried record sends kafka_producer_record_retry_rate client.id The average per-second number of retried record sends kafka_producer_requests_in_flight client.id The current number of in-flight requests awaiting a response. kafka_producer_request_size_avg client.id The average size of requests sent. kafka_producer_request_size_max client.id The maximum size of any request sent. kafka_producer_request_total client.id The total number of requests sent. kafka_producer_request_latency_avg client.id The average request latency in ms kafka_producer_request_latency_max client.id The maximum request latency in ms kafka_producer_request_rate client.id The number of requests sent per second kafka_producer_reauthentication_latency_max client.id The max latency observed due to re-authentication kafka_producer_produce_throttle_time_max client.id The maximum time in ms a request was throttled by a broker kafka_producer_produce_throttle_time_avg client.id The average time in ms a request was throttled by a broker kafka_producer_io_wait_time_ns_total client.id The total time the I/O thread spent waiting kafka_producer_incoming_byte_rate client.id The number of bytes read off all sockets per second kafka_producer_incoming_byte_total client.id The total number of bytes read off all sockets kafka_producer_io_time_ns_avg client.id The average length of time for I/O per select call in nanoseconds. kafka_producer_io_wait_time_ns_avg client.id The average length of time the I/O thread spent waiting for a socket ready for reads or writes in nanoseconds. kafka_producer_io_time_ns_total client.id The total time the I/O thread spent doing I/O kafka_producer_metadata_age client.id The age in seconds of the current producer metadata being used. kafka_producer_network_io_rate client.id The number of network operations (reads or writes) on all connections per second kafka_producer_network_io_total client.id The total number of network operations (reads or writes) on all connections kafka_producer_flush_time_ns_total client.id Total time producer has spent in flush in nanoseconds. kafka_producer_waiting_threads client.id The number of user threads blocked waiting for buffer memory to enqueue their records kafka_producer_reauthentication_latency_avg client.id The average latency observed due to re-authentication kafka_producer_metadata_wait_time_ns_total client.id Total time producer has spent waiting on topic metadata in nanoseconds. kafka_producer_outgoing_byte_rate client.id The number of outgoing bytes sent to all servers per second kafka_producer_outgoing_byte_total client.id The total number of outgoing bytes sent to all servers kafka_producer_response_rate client.id The number of responses received per second kafka_producer_response_total client.id The total number of responses received kafka_producer_txn_send_offsets_time_ns_total client.id Total time producer has spent in sendOffsetsToTransaction in nanoseconds. kafka_producer_txn_init_time_ns_total client.id Total time producer has spent in initTransactions in nanoseconds. kafka_producer_txn_abort_time_ns_total client.id Total time producer has spent in abortTransaction in nanoseconds. kafka_producer_txn_commit_time_ns_total client.id Total time producer has spent in commitTransaction in nanoseconds. kafka_producer_txn_begin_time_ns_total client.id Total time producer has spent in beginTransaction in nanoseconds. kafka_producer_failed_authentication_rate client.id The number of connections with failed authentication per second kafka_producer_failed_authentication_total client.id The total number of connections with failed authentication kafka_producer_failed_reauthentication_total client.id The total number of failed re-authentication of connections kafka_producer_failed_reauthentication_rate client.id The number of failed re-authentication of connections per second kafka_app_info_start_time_ms client.id Metric indicating start-time-ms kafka_producer_select_rate client.id The number of times the I/O layer checked for new I/O to perform per second kafka_producer_select_total client.id The total number of times the I/O layer checked for new I/O to perform kafka_producer_successful_authentication_rate client.id The number of connections with successful authentication per second kafka_producer_successful_authentication_no_reauth_total client.id The total number of connections with successful authentication where the client does not support re-authentication kafka_producer_successful_reauthentication_total client.id The total number of successful re-authentication of connections kafka_producer_successful_authentication_total client.id The total number of connections with successful authentication kafka_producer_successful_reauthentication_rate client.id The number of successful re-authentication of connections per second Kafka internal consumer metrics have been exposed Metric name Labels Description Source kafka_consumer_failed_reauthentication_rate client.id The number of failed re-authentication of connections per second kafka_consumer_connection_close_rate client.id The number of connections closed per second kafka_consumer_coordinator_partition_assigned_latency_max client.id The max time taken for a partition-assigned rebalance listener callback kafka_consumer_fetch_manager_records_consumed_rate client.id The average number of records consumed per second kafka_consumer_fetch_manager_fetch_latency_avg client.id The average time taken for a fetch request. kafka_consumer_io_wait_time_ns_total client.id The total time the I/O thread spent waiting kafka_consumer_successful_reauthentication_rate client.id The number of successful re-authentication of connections per second kafka_consumer_coordinator_last_rebalance_seconds_ago client.id The number of seconds since the last successful rebalance event kafka_consumer_failed_reauthentication_total client.id The total number of failed re-authentication of connections kafka_consumer_coordinator_commit_latency_max client.id The max time taken for a commit request kafka_consumer_last_poll_seconds_ago client.id The number of seconds since the last poll() invocation. kafka_consumer_outgoing_byte_total client.id The total number of outgoing bytes sent to all servers kafka_consumer_request_rate client.id The number of requests sent per second kafka_consumer_request_total client.id The total number of requests sent kafka_consumer_failed_authentication_rate client.id The number of connections with failed authentication per second kafka_consumer_response_rate client.id The number of responses received per second kafka_consumer_fetch_manager_records_lag_max client.id The maximum lag in terms of number of records for any partition in this window_ NOTE: This is based on current offset and not committed offset kafka_consumer_time_between_poll_max client.id The max delay between invocations of poll() in milliseconds kafka_consumer_coordinator_join_total client.id The total number of group joins kafka_consumer_coordinator_join_time_max client.id The max time taken for a group rejoin kafka_consumer_coordinator_rebalance_latency_avg client.id The average time taken for a group to complete a successful rebalance, which may be composed of several failed re-trials until it succeeded kafka_consumer_coordinator_sync_time_avg client.id The average time taken for a group sync kafka_consumer_network_io_total client.id The total number of network operations (reads or writes) on all connections kafka_consumer_reauthentication_latency_avg client.id The average latency observed due to re-authentication kafka_consumer_failed_authentication_total client.id The total number of connections with failed authentication kafka_consumer_fetch_manager_bytes_consumed_rate client.id The average number of bytes consumed per second kafka_consumer_coordinator_partition_revoked_latency_max client.id The max time taken for a partition-revoked rebalance listener callback kafka_consumer_coordinator_commit_total client.id The total number of commit calls kafka_consumer_fetch_manager_fetch_total client.id The total number of fetch requests kafka_consumer_io_time_ns_avg client.id The average length of time for I/O per select call in nanoseconds kafka_consumer_connection_count client.id The current number of active connections kafka_consumer_coordinator_rebalance_total client.id The total number of successful rebalance events, each event is composed of several failed re-trials until it succeeded kafka_consumer_incoming_byte_total client.id The total number of bytes read off all sockets kafka_consumer_fetch_manager_records_lead_min client.id The minimum lead in terms of number of records for any partition in this window kafka_consumer_committed_time_ns_total client.id The total time the consumer has spent in committed in nanoseconds kafka_consumer_coordinator_partition_assigned_latency_avg client.id The average time taken for a partition-assigned rebalance listener callback kafka_consumer_select_rate client.id The number of times the I/O layer checked for new I/O to perform per second kafka_consumer_coordinator_rebalance_latency_total client.id The total number of milliseconds this consumer has spent in successful rebalances since creation kafka_consumer_coordinator_failed_rebalance_rate_per_hour client.id The number of failed rebalance events per hour kafka_consumer_successful_authentication_rate client.id The number of connections with successful authentication per second kafka_consumer_fetch_manager_records_per_request_avg client.id The average number of records in each request kafka_consumer_outgoing_byte_rate client.id The number of outgoing bytes sent to all servers per second kafka_consumer_connection_close_total client.id The total number of connections closed kafka_consumer_coordinator_sync_time_max client.id The max time taken for a group sync kafka_consumer_network_io_rate client.id The number of network operations (reads or writes) on all connections per second kafka_consumer_coordinator_join_rate client.id The number of group joins per second kafka_consumer_fetch_manager_bytes_consumed_total client.id The total number of bytes consumed kafka_consumer_fetch_manager_fetch_size_avg client.id The average number of bytes fetched per request kafka_consumer_response_total client.id The total number of responses received kafka_consumer_coordinator_rebalance_latency_max client.id The max time taken for a group to complete a successful rebalance, which may be composed of several failed re-trials until it succeeded kafka_consumer_fetch_manager_records_consumed_total client.id The total number of records consumed kafka_consumer_fetch_manager_fetch_rate client.id The number of fetch requests per second kafka_consumer_commit_sync_time_ns_total client.id The total time the consumer has spent in commitSync in nanoseconds kafka_consumer_successful_authentication_no_reauth_total client.id The total number of connections with successful authentication where the client does not support re-authentication kafka_consumer_io_wait_time_ns_avg client.id The average length of time the I/O thread spent waiting for a socket ready for reads or writes in nanoseconds kafka_consumer_incoming_byte_rate client.id The number of bytes read off all sockets per second kafka_consumer_coordinator_heartbeat_response_time_max client.id The max time taken to receive a response to a heartbeat request kafka_consumer_coordinator_failed_rebalance_total client.id The total number of failed rebalance events kafka_consumer_coordinator_rebalance_rate_per_hour client.id The number of successful rebalance events per hour, each event is composed of several failed re-trials until it succeeded kafka_consumer_request_size_max client.id The maximum size of any request sent kafka_consumer_fetch_manager_fetch_latency_max client.id The max time taken for any fetch request kafka_consumer_fetch_manager_fetch_throttle_time_avg client.id The average throttle time in ms. kafka_consumer_coordinator_assigned_partitions client.id The number of partitions currently assigned to this consumer kafka_consumer_coordinator_heartbeat_rate client.id The number of heartbeats per second kafka_consumer_coordinator_sync_total | client.id` The total number of group syncs kafka_consumer_successful_reauthentication_total client.id The total number of successful re-authentication of connections kafka_consumer_fetch_manager_fetch_size_max client.id The maximum number of bytes fetched per request kafka_consumer_coordinator_partition_lost_latency_max client.id The max time taken for a partition-lost rebalance listener callback. kafka_consumer_coordinator_last_heartbeat_seconds_ago client.id The number of seconds since the last coordinator heartbeat was sent. kafka_consumer_io_time_ns_total client.id The total time the I/O thread spent doing I/O. kafka_consumer_coordinator_commit_latency_avg client.id The average time taken for a commit request. kafka_consumer_coordinator_partition_lost_latency_avg client.id The average time taken for a partition-lost rebalance listener callback kafka_consumer_poll_idle_ratio_avg client.id The average fraction of time the consumer's poll() is idle as opposed to waiting for the user code to process records. kafka_consumer_connection_creation_rate client.id The number of new connections established per second kafka_consumer_connection_creation_total client.id The total number of new connections established kafka_consumer_select_total client.id The total number of times the I/O layer checked for new I/O to perform kafka_consumer_reauthentication_latency_max client.id The max latency observed due to re-authentication kafka_consumer_request_size_avg client.id The average size of requests sent kafka_consumer_fetch_manager_fetch_throttle_time_max client.id The maximum throttle time in ms kafka_consumer_time_between_poll_avg client.id The average delay between invocations of poll() in milliseconds kafka_consumer_coordinator_join_time_avg client.id The average time taken for a group rejoin kafka_consumer_coordinator_sync_rate client.id The number of group syncs per second kafka_consumer_coordinator_partition_revoked_latency_avg client.id The average time taken for a partition-revoked rebalance listener callback kafka_consumer_successful_authentication_total client.id The total number of connections with successful authentication kafka_consumer_coordinator_heartbeat_total client.id The total number of heartbeats MongoDB \u00b6 Metric Name Labels Description Source mongodb.driver.pool.waitqueuesize cluster.id , server.address The current size of the wait queue for a connection from the pool Bridged from Micrometer mongodb.driver.pool.checkedout cluster.id , server.address The count of connections that are currently in use Bridged from Micrometer mongodb.driver.pool.size cluster.id , server.address The current size of the connection pool, including idle and and in-use members Bridged from Micrometer mongodb.driver.commands cluster.id , server.address collection , command , status Timer of mongodb commands Bridged from Micrometer Circuit Breaker \u00b6 More details about resilience4j circuit breaker metrics can be found here . Metric Name Labels Description Source resilience4j_circuitbreaker_buffered_calls kind The number of buffered failed calls stored in the ring buffer Bridged from Micrometer resilience4j_circuitbreaker_slow_calls kind The number of slow successful which were slower than a certain threshold Bridged from Micrometer resilience4j_circuitbreaker_calls_seconds kind Total number of successful calls by kind summary Bridged from Micrometer resilience4j_circuitbreaker_calls_seconds_max kind Total number of successful calls by kind Bridged from Micrometer resilience4j_circuitbreaker_state sate The states of the circuit breaker Bridged from Micrometer resilience4j_circuitbreaker_slow_call_rate The slow call of the circuit breaker Bridged from Micrometer resilience4j_circuitbreaker_calls_bucket kind Deprecated use resilience4j_circuitbreaker_calls_seconds Total number of calls by kind Prometheus resilience4j_circuitbreaker_failure_rate The failure rate of the circuit breaker Bridged from Micrometer resilience4j_circuitbreaker_not_permitted_calls_total kind Total number of not permitted calls Bridged from Micrometer resilience4j_circuitbreaker_calls_created kind Deprecated Total number of calls by kind Bridged from Micrometer Jetty \u00b6 Metric Name Labels Description Source jetty_connections_current connector_name The current number of open Jetty connections Bridged from Micrometer jetty_connections_max connector_name The maximum number of observed connections over a rolling 2-minute interval Bridged from Micrometer jetty_connections_messages_in connector_name Messages received by tracked connections Bridged from Micrometer jetty_connections_messages_out connector_name Messages sent by tracked connections Bridged from Micrometer jetty_connections_bytes_in connector_name Bytes received by tracked connections Bridged from Micrometer jetty_connections_bytes_out connector_name Bytes sent by tracked connections Bridged from Micrometer Request Metrics \u00b6 Metric Name Labels Description Source http_server_requests_seconds_count exception , method , outcome , status , uri Total number of requests your application received at this endpoint. Bridged from Micrometer http_server_requests_seconds_sum exception , method , outcome , status , uri Sum of the duration of every request your application received at this endpoint. Bridged from Micrometer http_server_requests_seconds_max exception , method , outcome , status , uri Maximum request duration during a time window. The value resets to 0 when a new time window starts. The default time window is 2 minutes. Bridged from Micrometer apache_http_client_request_duration_seconds manager , method , name , quantile Apache request duration summary. Bridged from Micrometer jetty_connections factory , port , quantile Jetty Connections summary. Bridged from Micrometer io_dropwizard_jetty_MutableServletContextHandler_get_requests quantile io_dropwizard_jetty_MutableServletContextHandler_get_requests summary. Bridged from Micrometer Health Checks \u00b6 All health checks are provided as a Gauge metric healthcheck_status and are included in the metrics endpoint. The name of the Health Check is used as label name . The metric value is 1.0 for healthy and 0.0 for unhealthy. Example: 1 2 healthcheck_status{name=\"hibernate\",} 1.0 healthcheck_status{name=\"disk_space\",} 0.0 Currently, health checks are evaluated when their status is requested. Slow health checks should be annotated with com.codahale.metrics.health.annotation.Async to avoid blocking collection of the results. Usage \u00b6 The PrometheusBundle has to be added in the application: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 import PrometheusBundle ; import io.dropwizard.core.Application ; public class MyApplication extends Application < MyConfiguration > { public static void main ( final String [] args ) { new MyApplication (). run ( args ); } @Override public void initialize ( Bootstrap < MyConfiguration > bootstrap ) { // ... bootstrap . addBundle ( PrometheusBundle . builder (). build ()); // ... } @Override public void run ( MyConfiguration configuration , Environment environment ) { // ... } }","title":"Server Prometheus"},{"location":"server-prometheus/#sda-commons-server-prometheus","text":"The module sda-commons-server-prometheus provides an admin endpoint to serve metrics and health check results in a format that Prometheus can read. The endpoint is available at the applications admin port at /metrics/prometheus .","title":"SDA Commons Server Prometheus"},{"location":"server-prometheus/#provided-metrics","text":"Default metrics that are provided at /metrics/prometheus : Metric name Labels Description Source Deprecated jvm_ * Multiple metrics about the JVM Bridged from Dropwizard Since v5.3.13 io_dropwizard_jetty_ Multiple metrics from the embedded Jetty server Bridged from Dropwizard io_dropwizard_db_ Multiple metrics from the database if a database is used Bridged from Dropwizard healthcheck_status name Metrics that represent the state of the health checks HealthCheckMetricsCollector A filter that extracts the consumer from the HTTP headers should add Consumer-Name to the request properties. That filter is not part of the PrometheusBundle .","title":"Provided metrics"},{"location":"server-prometheus/#micrometer-metrics","text":"We are slowly shifting over from Dropwizard-Metrics to Micrometer . The following metrics are available so far.","title":"Micrometer Metrics"},{"location":"server-prometheus/#jvm","text":"Metric Name Labels Description Source jvm_classes_loaded_classes The number of classes that are currently loaded in the Java virtual machine. Bridged from Micrometer jvm_classes_unloaded_classes The total number of classes unloaded since the Java virtual machine has started execution. Bridged from Micrometer jvm_buffer_count_buffers id An estimate of the number of buffers in the pool. Bridged from Micrometer jvm_buffer_memory_used_bytes id An estimate of the memory that the Java virtual machine is using for this buffer pool. Bridged from Micrometer jvm_buffer_total_capacity_bytes id An estimate of the total capacity of the buffers in this pool. Bridged from Micrometer jvm_memory_used_bytes id , area The amount of used memory. Bridged from Micrometer jvm_memory_committed_bytes id , area The amount of memory in bytes that is committed for the Java virtual machine to use. Bridged from Micrometer jvm_memory_max_bytes id , area The maximum amount of memory in bytes that can be used for memory management. Bridged from Micrometer jvm_gc_max_data_size_bytes Max size of long-lived heap memory pool. Bridged from Micrometer jvm_gc_live_data_size_bytes Size of long-lived heap memory pool after reclamation. Bridged from Micrometer jvm_gc_memory_allocated_bytes_total Incremented for an increase in the size of the (young) heap memory pool after one GC to before the next. Bridged from Micrometer jvm_gc_memory_promoted_bytes_total Count of positive increases in the size of the old generation memory pool before GC to after GC. Bridged from Micrometer jvm_gc_concurrent_phase_time_seconds_count gc , action , cause Time spent in concurrent phase. Bridged from Micrometer jvm_gc_concurrent_phase_time_seconds_sum gc , action , cause Bridged from Micrometer jvm_gc_concurrent_phase_time_seconds_max gc , action , cause Bridged from Micrometer jvm_gc_pause_seconds_count gc , action , cause Time spent in GC pause. Bridged from Micrometer jvm_gc_pause_seconds_sum gc , action , cause Bridged from Micrometer jvm_gc_pause_seconds_max gc , action , cause Bridged from Micrometer system_cpu_count The number of processors available to the Java virtual machine. Bridged from Micrometer system_load_average_1m The sum of the number of runnable entities queued to available processors and the number of runnable entities running on the available processors averaged over a period of time. Bridged from Micrometer system_cpu_usage The \"recent cpu usage\" of the system the application is running in. Bridged from Micrometer process_cpu_usage The \"recent cpu usage\" for the Java Virtual Machine process. Bridged from Micrometer jvm_threads_peak_threads The peak live thread count since the Java virtual machine started or peak was reset. Bridged from Micrometer jvm_threads_daemon_threads The current number of live daemon threads. Bridged from Micrometer jvm_threads_live The current number of live threads including both daemon and non-daemon threads. Bridged from Micrometer jvm_threads_started_threads_total The total number of application threads started in the JVM. Bridged from Micrometer jvm_threads_states_threads state The current number of threads. Bridged from Micrometer jvm_classes_loaded The number of classes that are currently loaded in the Java virtual machine. Bridged from Micrometer jvm_classes_unloaded The total number of classes unloaded since the Java virtual machine has started execution. Bridged from Micrometer","title":"JVM"},{"location":"server-prometheus/#kafka","text":"Kafka internal producer metrics have been exposed Metric name Labels Description Source kafka_producer_batch_split_rate client.id The average number of batch splits per second kafka_producer_batch_split_total client.id The total number of batch splits kafka_producer_batch_size_max client.id The max number of bytes sent per partition per-request. kafka_producer_batch_size_avg client.id The average number of bytes sent per partition per-request. kafka_producer_buffer_exhausted_rate client.id The average per-second number of record sends that are dropped due to buffer exhaustion kafka_producer_buffer_total_bytes client.id The maximum amount of buffer memory the client can use (whether or not it is currently used). kafka_producer_buffer_available_bytes client.id The total amount of buffer memory that is not being used (either unallocated or in the free list). kafka_producer_buffer_exhausted_total client.id The total number of record sends that are dropped due to buffer exhaustion kafka_producer_bufferpool_wait_time_ns_total client.id The total time in nanoseconds an appender waits for space allocation. kafka_producer_bufferpool_wait_ratio client.id The fraction of time an appender waits for space allocation. kafka_producer_compression_rate_avg client.id The average compression rate of record batches, defined as the average ratio of the compressed batch size over the uncompressed size. kafka_producer_connection_creation_total client.id The total number of new connections established kafka_producer_connection_close_rate client.id The number of connections closed per second kafka_producer_connection_count client.id The current number of active connections. kafka_producer_connection_creation_rate client.id The number of new connections established per second kafka_producer_connection_close_total client.id The total number of connections closed kafka_producer_node_incoming_byte_total client.id , node.id The total number of incoming bytes kafka_producer_node_request_total client.id , node.id The total number of requests sent kafka_producer_node_request_size_max client.id , node.id The maximum size of any request sent. kafka_producer_node_outgoing_byte_total client.id , node.id The total number of outgoing bytes kafka_producer_node_response_total client.id , node.id The total number of responses received kafka_producer_node_request_size_avg client.id , node.id The average size of requests sent. kafka_producer_node_response_rate client.id , node.id The number of responses received per second kafka_producer_node_request_latency_avg client.id , node.id The average latency of a producer request kafka_producer_node_incoming_byte_rate client.id , node.id The number of incoming bytes per second kafka_producer_node_outgoing_byte_rate client.id , node.id The number of outgoing bytes per second kafka_producer_node_request_rate client.id , node.id The number of requests sent per second kafka_producer_node_request_latency_max client.id , node.id The maximum latency of a producer request kafka_producer_record_queue_time_avg client.id The average time in ms record batches spent in the send buffer. kafka_producer_record_send_total client.id The total number of records sent. kafka_producer_record_size_max client.id The maximum record size kafka_producer_record_error_rate client.id The average per-second number of record sends that resulted in errors kafka_producer_record_error_total client.id The total number of record sends that resulted in errors kafka_producer_records_per_request_avg client.id The average number of records per request. kafka_producer_record_size_avg client.id The average record size kafka_producer_record_queue_time_max client.id The maximum time in ms record batches spent in the send buffer. kafka_producer_record_send_rate client.id The average number of records sent per second. kafka_producer_record_retry_total client.id The total number of retried record sends kafka_producer_record_retry_rate client.id The average per-second number of retried record sends kafka_producer_requests_in_flight client.id The current number of in-flight requests awaiting a response. kafka_producer_request_size_avg client.id The average size of requests sent. kafka_producer_request_size_max client.id The maximum size of any request sent. kafka_producer_request_total client.id The total number of requests sent. kafka_producer_request_latency_avg client.id The average request latency in ms kafka_producer_request_latency_max client.id The maximum request latency in ms kafka_producer_request_rate client.id The number of requests sent per second kafka_producer_reauthentication_latency_max client.id The max latency observed due to re-authentication kafka_producer_produce_throttle_time_max client.id The maximum time in ms a request was throttled by a broker kafka_producer_produce_throttle_time_avg client.id The average time in ms a request was throttled by a broker kafka_producer_io_wait_time_ns_total client.id The total time the I/O thread spent waiting kafka_producer_incoming_byte_rate client.id The number of bytes read off all sockets per second kafka_producer_incoming_byte_total client.id The total number of bytes read off all sockets kafka_producer_io_time_ns_avg client.id The average length of time for I/O per select call in nanoseconds. kafka_producer_io_wait_time_ns_avg client.id The average length of time the I/O thread spent waiting for a socket ready for reads or writes in nanoseconds. kafka_producer_io_time_ns_total client.id The total time the I/O thread spent doing I/O kafka_producer_metadata_age client.id The age in seconds of the current producer metadata being used. kafka_producer_network_io_rate client.id The number of network operations (reads or writes) on all connections per second kafka_producer_network_io_total client.id The total number of network operations (reads or writes) on all connections kafka_producer_flush_time_ns_total client.id Total time producer has spent in flush in nanoseconds. kafka_producer_waiting_threads client.id The number of user threads blocked waiting for buffer memory to enqueue their records kafka_producer_reauthentication_latency_avg client.id The average latency observed due to re-authentication kafka_producer_metadata_wait_time_ns_total client.id Total time producer has spent waiting on topic metadata in nanoseconds. kafka_producer_outgoing_byte_rate client.id The number of outgoing bytes sent to all servers per second kafka_producer_outgoing_byte_total client.id The total number of outgoing bytes sent to all servers kafka_producer_response_rate client.id The number of responses received per second kafka_producer_response_total client.id The total number of responses received kafka_producer_txn_send_offsets_time_ns_total client.id Total time producer has spent in sendOffsetsToTransaction in nanoseconds. kafka_producer_txn_init_time_ns_total client.id Total time producer has spent in initTransactions in nanoseconds. kafka_producer_txn_abort_time_ns_total client.id Total time producer has spent in abortTransaction in nanoseconds. kafka_producer_txn_commit_time_ns_total client.id Total time producer has spent in commitTransaction in nanoseconds. kafka_producer_txn_begin_time_ns_total client.id Total time producer has spent in beginTransaction in nanoseconds. kafka_producer_failed_authentication_rate client.id The number of connections with failed authentication per second kafka_producer_failed_authentication_total client.id The total number of connections with failed authentication kafka_producer_failed_reauthentication_total client.id The total number of failed re-authentication of connections kafka_producer_failed_reauthentication_rate client.id The number of failed re-authentication of connections per second kafka_app_info_start_time_ms client.id Metric indicating start-time-ms kafka_producer_select_rate client.id The number of times the I/O layer checked for new I/O to perform per second kafka_producer_select_total client.id The total number of times the I/O layer checked for new I/O to perform kafka_producer_successful_authentication_rate client.id The number of connections with successful authentication per second kafka_producer_successful_authentication_no_reauth_total client.id The total number of connections with successful authentication where the client does not support re-authentication kafka_producer_successful_reauthentication_total client.id The total number of successful re-authentication of connections kafka_producer_successful_authentication_total client.id The total number of connections with successful authentication kafka_producer_successful_reauthentication_rate client.id The number of successful re-authentication of connections per second Kafka internal consumer metrics have been exposed Metric name Labels Description Source kafka_consumer_failed_reauthentication_rate client.id The number of failed re-authentication of connections per second kafka_consumer_connection_close_rate client.id The number of connections closed per second kafka_consumer_coordinator_partition_assigned_latency_max client.id The max time taken for a partition-assigned rebalance listener callback kafka_consumer_fetch_manager_records_consumed_rate client.id The average number of records consumed per second kafka_consumer_fetch_manager_fetch_latency_avg client.id The average time taken for a fetch request. kafka_consumer_io_wait_time_ns_total client.id The total time the I/O thread spent waiting kafka_consumer_successful_reauthentication_rate client.id The number of successful re-authentication of connections per second kafka_consumer_coordinator_last_rebalance_seconds_ago client.id The number of seconds since the last successful rebalance event kafka_consumer_failed_reauthentication_total client.id The total number of failed re-authentication of connections kafka_consumer_coordinator_commit_latency_max client.id The max time taken for a commit request kafka_consumer_last_poll_seconds_ago client.id The number of seconds since the last poll() invocation. kafka_consumer_outgoing_byte_total client.id The total number of outgoing bytes sent to all servers kafka_consumer_request_rate client.id The number of requests sent per second kafka_consumer_request_total client.id The total number of requests sent kafka_consumer_failed_authentication_rate client.id The number of connections with failed authentication per second kafka_consumer_response_rate client.id The number of responses received per second kafka_consumer_fetch_manager_records_lag_max client.id The maximum lag in terms of number of records for any partition in this window_ NOTE: This is based on current offset and not committed offset kafka_consumer_time_between_poll_max client.id The max delay between invocations of poll() in milliseconds kafka_consumer_coordinator_join_total client.id The total number of group joins kafka_consumer_coordinator_join_time_max client.id The max time taken for a group rejoin kafka_consumer_coordinator_rebalance_latency_avg client.id The average time taken for a group to complete a successful rebalance, which may be composed of several failed re-trials until it succeeded kafka_consumer_coordinator_sync_time_avg client.id The average time taken for a group sync kafka_consumer_network_io_total client.id The total number of network operations (reads or writes) on all connections kafka_consumer_reauthentication_latency_avg client.id The average latency observed due to re-authentication kafka_consumer_failed_authentication_total client.id The total number of connections with failed authentication kafka_consumer_fetch_manager_bytes_consumed_rate client.id The average number of bytes consumed per second kafka_consumer_coordinator_partition_revoked_latency_max client.id The max time taken for a partition-revoked rebalance listener callback kafka_consumer_coordinator_commit_total client.id The total number of commit calls kafka_consumer_fetch_manager_fetch_total client.id The total number of fetch requests kafka_consumer_io_time_ns_avg client.id The average length of time for I/O per select call in nanoseconds kafka_consumer_connection_count client.id The current number of active connections kafka_consumer_coordinator_rebalance_total client.id The total number of successful rebalance events, each event is composed of several failed re-trials until it succeeded kafka_consumer_incoming_byte_total client.id The total number of bytes read off all sockets kafka_consumer_fetch_manager_records_lead_min client.id The minimum lead in terms of number of records for any partition in this window kafka_consumer_committed_time_ns_total client.id The total time the consumer has spent in committed in nanoseconds kafka_consumer_coordinator_partition_assigned_latency_avg client.id The average time taken for a partition-assigned rebalance listener callback kafka_consumer_select_rate client.id The number of times the I/O layer checked for new I/O to perform per second kafka_consumer_coordinator_rebalance_latency_total client.id The total number of milliseconds this consumer has spent in successful rebalances since creation kafka_consumer_coordinator_failed_rebalance_rate_per_hour client.id The number of failed rebalance events per hour kafka_consumer_successful_authentication_rate client.id The number of connections with successful authentication per second kafka_consumer_fetch_manager_records_per_request_avg client.id The average number of records in each request kafka_consumer_outgoing_byte_rate client.id The number of outgoing bytes sent to all servers per second kafka_consumer_connection_close_total client.id The total number of connections closed kafka_consumer_coordinator_sync_time_max client.id The max time taken for a group sync kafka_consumer_network_io_rate client.id The number of network operations (reads or writes) on all connections per second kafka_consumer_coordinator_join_rate client.id The number of group joins per second kafka_consumer_fetch_manager_bytes_consumed_total client.id The total number of bytes consumed kafka_consumer_fetch_manager_fetch_size_avg client.id The average number of bytes fetched per request kafka_consumer_response_total client.id The total number of responses received kafka_consumer_coordinator_rebalance_latency_max client.id The max time taken for a group to complete a successful rebalance, which may be composed of several failed re-trials until it succeeded kafka_consumer_fetch_manager_records_consumed_total client.id The total number of records consumed kafka_consumer_fetch_manager_fetch_rate client.id The number of fetch requests per second kafka_consumer_commit_sync_time_ns_total client.id The total time the consumer has spent in commitSync in nanoseconds kafka_consumer_successful_authentication_no_reauth_total client.id The total number of connections with successful authentication where the client does not support re-authentication kafka_consumer_io_wait_time_ns_avg client.id The average length of time the I/O thread spent waiting for a socket ready for reads or writes in nanoseconds kafka_consumer_incoming_byte_rate client.id The number of bytes read off all sockets per second kafka_consumer_coordinator_heartbeat_response_time_max client.id The max time taken to receive a response to a heartbeat request kafka_consumer_coordinator_failed_rebalance_total client.id The total number of failed rebalance events kafka_consumer_coordinator_rebalance_rate_per_hour client.id The number of successful rebalance events per hour, each event is composed of several failed re-trials until it succeeded kafka_consumer_request_size_max client.id The maximum size of any request sent kafka_consumer_fetch_manager_fetch_latency_max client.id The max time taken for any fetch request kafka_consumer_fetch_manager_fetch_throttle_time_avg client.id The average throttle time in ms. kafka_consumer_coordinator_assigned_partitions client.id The number of partitions currently assigned to this consumer kafka_consumer_coordinator_heartbeat_rate client.id The number of heartbeats per second kafka_consumer_coordinator_sync_total | client.id` The total number of group syncs kafka_consumer_successful_reauthentication_total client.id The total number of successful re-authentication of connections kafka_consumer_fetch_manager_fetch_size_max client.id The maximum number of bytes fetched per request kafka_consumer_coordinator_partition_lost_latency_max client.id The max time taken for a partition-lost rebalance listener callback. kafka_consumer_coordinator_last_heartbeat_seconds_ago client.id The number of seconds since the last coordinator heartbeat was sent. kafka_consumer_io_time_ns_total client.id The total time the I/O thread spent doing I/O. kafka_consumer_coordinator_commit_latency_avg client.id The average time taken for a commit request. kafka_consumer_coordinator_partition_lost_latency_avg client.id The average time taken for a partition-lost rebalance listener callback kafka_consumer_poll_idle_ratio_avg client.id The average fraction of time the consumer's poll() is idle as opposed to waiting for the user code to process records. kafka_consumer_connection_creation_rate client.id The number of new connections established per second kafka_consumer_connection_creation_total client.id The total number of new connections established kafka_consumer_select_total client.id The total number of times the I/O layer checked for new I/O to perform kafka_consumer_reauthentication_latency_max client.id The max latency observed due to re-authentication kafka_consumer_request_size_avg client.id The average size of requests sent kafka_consumer_fetch_manager_fetch_throttle_time_max client.id The maximum throttle time in ms kafka_consumer_time_between_poll_avg client.id The average delay between invocations of poll() in milliseconds kafka_consumer_coordinator_join_time_avg client.id The average time taken for a group rejoin kafka_consumer_coordinator_sync_rate client.id The number of group syncs per second kafka_consumer_coordinator_partition_revoked_latency_avg client.id The average time taken for a partition-revoked rebalance listener callback kafka_consumer_successful_authentication_total client.id The total number of connections with successful authentication kafka_consumer_coordinator_heartbeat_total client.id The total number of heartbeats","title":"Kafka"},{"location":"server-prometheus/#mongodb","text":"Metric Name Labels Description Source mongodb.driver.pool.waitqueuesize cluster.id , server.address The current size of the wait queue for a connection from the pool Bridged from Micrometer mongodb.driver.pool.checkedout cluster.id , server.address The count of connections that are currently in use Bridged from Micrometer mongodb.driver.pool.size cluster.id , server.address The current size of the connection pool, including idle and and in-use members Bridged from Micrometer mongodb.driver.commands cluster.id , server.address collection , command , status Timer of mongodb commands Bridged from Micrometer","title":"MongoDB"},{"location":"server-prometheus/#circuit-breaker","text":"More details about resilience4j circuit breaker metrics can be found here . Metric Name Labels Description Source resilience4j_circuitbreaker_buffered_calls kind The number of buffered failed calls stored in the ring buffer Bridged from Micrometer resilience4j_circuitbreaker_slow_calls kind The number of slow successful which were slower than a certain threshold Bridged from Micrometer resilience4j_circuitbreaker_calls_seconds kind Total number of successful calls by kind summary Bridged from Micrometer resilience4j_circuitbreaker_calls_seconds_max kind Total number of successful calls by kind Bridged from Micrometer resilience4j_circuitbreaker_state sate The states of the circuit breaker Bridged from Micrometer resilience4j_circuitbreaker_slow_call_rate The slow call of the circuit breaker Bridged from Micrometer resilience4j_circuitbreaker_calls_bucket kind Deprecated use resilience4j_circuitbreaker_calls_seconds Total number of calls by kind Prometheus resilience4j_circuitbreaker_failure_rate The failure rate of the circuit breaker Bridged from Micrometer resilience4j_circuitbreaker_not_permitted_calls_total kind Total number of not permitted calls Bridged from Micrometer resilience4j_circuitbreaker_calls_created kind Deprecated Total number of calls by kind Bridged from Micrometer","title":"Circuit Breaker"},{"location":"server-prometheus/#jetty","text":"Metric Name Labels Description Source jetty_connections_current connector_name The current number of open Jetty connections Bridged from Micrometer jetty_connections_max connector_name The maximum number of observed connections over a rolling 2-minute interval Bridged from Micrometer jetty_connections_messages_in connector_name Messages received by tracked connections Bridged from Micrometer jetty_connections_messages_out connector_name Messages sent by tracked connections Bridged from Micrometer jetty_connections_bytes_in connector_name Bytes received by tracked connections Bridged from Micrometer jetty_connections_bytes_out connector_name Bytes sent by tracked connections Bridged from Micrometer","title":"Jetty"},{"location":"server-prometheus/#request-metrics","text":"Metric Name Labels Description Source http_server_requests_seconds_count exception , method , outcome , status , uri Total number of requests your application received at this endpoint. Bridged from Micrometer http_server_requests_seconds_sum exception , method , outcome , status , uri Sum of the duration of every request your application received at this endpoint. Bridged from Micrometer http_server_requests_seconds_max exception , method , outcome , status , uri Maximum request duration during a time window. The value resets to 0 when a new time window starts. The default time window is 2 minutes. Bridged from Micrometer apache_http_client_request_duration_seconds manager , method , name , quantile Apache request duration summary. Bridged from Micrometer jetty_connections factory , port , quantile Jetty Connections summary. Bridged from Micrometer io_dropwizard_jetty_MutableServletContextHandler_get_requests quantile io_dropwizard_jetty_MutableServletContextHandler_get_requests summary. Bridged from Micrometer","title":"Request Metrics"},{"location":"server-prometheus/#health-checks","text":"All health checks are provided as a Gauge metric healthcheck_status and are included in the metrics endpoint. The name of the Health Check is used as label name . The metric value is 1.0 for healthy and 0.0 for unhealthy. Example: 1 2 healthcheck_status{name=\"hibernate\",} 1.0 healthcheck_status{name=\"disk_space\",} 0.0 Currently, health checks are evaluated when their status is requested. Slow health checks should be annotated with com.codahale.metrics.health.annotation.Async to avoid blocking collection of the results.","title":"Health Checks"},{"location":"server-prometheus/#usage","text":"The PrometheusBundle has to be added in the application: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 import PrometheusBundle ; import io.dropwizard.core.Application ; public class MyApplication extends Application < MyConfiguration > { public static void main ( final String [] args ) { new MyApplication (). run ( args ); } @Override public void initialize ( Bootstrap < MyConfiguration > bootstrap ) { // ... bootstrap . addBundle ( PrometheusBundle . builder (). build ()); // ... } @Override public void run ( MyConfiguration configuration , Environment environment ) { // ... } }","title":"Usage"},{"location":"server-s3-testing/","text":"SDA Commons Server S3 Testing \u00b6 This is accomplished using local-s3 , which provides an in memory object storage. Usage \u00b6 To use the test extension, a dependency to this module has to be added: 1 testCompile 'org.sdase.commons:sda-commons-server-s3-testing' JUnit 5 \u00b6 This module provides the S3ClassExtension , a JUnit 5 extension that is used to automatically bootstrap an AWS S3-compatible object storage instance for integration tests. To create a S3 Mock instance, annotate your test class with @LocalS3 and register the S3 test extension in your test class: 1 2 3 4 5 6 7 8 9 @LocalS3 class MyTest { @RegisterExtension @Order ( 0 ) static final S3ClassExtension S3 = S3ClassExtension . builder () . build (); } The extension takes care to choose a free port for the endpoint. You can access the URL using S3.getEndpoint() . Often one needs to pass the endpoint URL to the constructor of another extension: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 @LocalS3 class DropwizardIT { @RegisterExtension @Order ( 0 ) static final S3ClassExtension S3 = S3ClassExtension . builder () . build (); @RegisterExtension @Order ( 1 ) static DropwizardAppExtension < TestConfiguration > DW = new DropwizardAppExtension <> ( MyApplication . class , ResourceHelpers . resourceFilePath ( \"test-config.yml\" ), ConfigOverride . config ( \"objectstorage.endpoint\" , S3 :: getEndpoint )); } In case you need a pre-populated bucket in your tests, you might add files while building the extension. You can call S3.resetAll() to restore this state at any time. If you need to perform additional operations on the object storage S3.getClient() provides a full S3 storage client. 1 2 3 4 5 6 7 8 9 @RegisterExtension @Order ( 0 ) static final S3ClassExtension S3 = S3ClassExtension . builder () . createBucket ( \"bucket-of-water\" ) . putObject ( \"bucket\" , \"file.txt\" , new File ( ResourceHelpers . resourceFilePath ( \"test-file.txt\" ))) . putObject ( \"bucket\" , \"stream.txt\" , MyClass . class . getResourceAsStream ( \"/test-file.txt\" )) . putObject ( \"bucket\" , \"content.txt\" , \"RUN SDA\" ) . build ();","title":"Server S3 Testing"},{"location":"server-s3-testing/#sda-commons-server-s3-testing","text":"This is accomplished using local-s3 , which provides an in memory object storage.","title":"SDA Commons Server S3 Testing"},{"location":"server-s3-testing/#usage","text":"To use the test extension, a dependency to this module has to be added: 1 testCompile 'org.sdase.commons:sda-commons-server-s3-testing'","title":"Usage"},{"location":"server-s3-testing/#junit-5","text":"This module provides the S3ClassExtension , a JUnit 5 extension that is used to automatically bootstrap an AWS S3-compatible object storage instance for integration tests. To create a S3 Mock instance, annotate your test class with @LocalS3 and register the S3 test extension in your test class: 1 2 3 4 5 6 7 8 9 @LocalS3 class MyTest { @RegisterExtension @Order ( 0 ) static final S3ClassExtension S3 = S3ClassExtension . builder () . build (); } The extension takes care to choose a free port for the endpoint. You can access the URL using S3.getEndpoint() . Often one needs to pass the endpoint URL to the constructor of another extension: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 @LocalS3 class DropwizardIT { @RegisterExtension @Order ( 0 ) static final S3ClassExtension S3 = S3ClassExtension . builder () . build (); @RegisterExtension @Order ( 1 ) static DropwizardAppExtension < TestConfiguration > DW = new DropwizardAppExtension <> ( MyApplication . class , ResourceHelpers . resourceFilePath ( \"test-config.yml\" ), ConfigOverride . config ( \"objectstorage.endpoint\" , S3 :: getEndpoint )); } In case you need a pre-populated bucket in your tests, you might add files while building the extension. You can call S3.resetAll() to restore this state at any time. If you need to perform additional operations on the object storage S3.getClient() provides a full S3 storage client. 1 2 3 4 5 6 7 8 9 @RegisterExtension @Order ( 0 ) static final S3ClassExtension S3 = S3ClassExtension . builder () . createBucket ( \"bucket-of-water\" ) . putObject ( \"bucket\" , \"file.txt\" , new File ( ResourceHelpers . resourceFilePath ( \"test-file.txt\" ))) . putObject ( \"bucket\" , \"stream.txt\" , MyClass . class . getResourceAsStream ( \"/test-file.txt\" )) . putObject ( \"bucket\" , \"content.txt\" , \"RUN SDA\" ) . build ();","title":"JUnit 5"},{"location":"server-s3/","text":"SDA Commons Server S3 \u00b6 This module provides the S3Bundle , a Dropwizard bundle that is used to perform operations on an object storage. The bundle provides an S3 client based on the Amazon AWS SDK v2 . Usage \u00b6 The S3Bundle should be added as a field in the application class instead of being anonymously added in the initialize method like other bundles of this library. Implementations need to refer to the instance to access the client. The Dropwizard applications configuration class needs to provide a S3Configuration . The bundle builder requires to define the getter of the S3Configuration as method reference to access the configuration. Afterward, getClient() is used to access an instance of S3Client that is used to operate on the object storage. See S3BundleTest for a detailed usage example. Credentials \u00b6 Usually you can specify AWS credentials in the S3Configuration . But the bundle also supports AWS default credentials provider chain to retrieve credentials. Most prominently you can pass credentials via environment variables AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY or via system properties aws.accessKeyId and aws.secretAccessKey . If you want to use anonymous credentials, you can set useAnonymousCredentials to true in the S3Configuration . Tracing \u00b6 The bundle comes with OpenTelemetry instrumentation. Health Check \u00b6 The bundle supports the creation of both internal and external health checks. 1 2 3 4 5 private final S3Bundle<Config> s3Bundle = S3Bundle.builder() .withConfigurationProvider(Config::getS3Config) .withHealthCheck(Collections.singleton(Config::getS3Bucket)) .build(); For creation of an external health check, use .withExternalHealthCheck(Iterable<BucketNameProvider<C>>) respectively.","title":"Server S3"},{"location":"server-s3/#sda-commons-server-s3","text":"This module provides the S3Bundle , a Dropwizard bundle that is used to perform operations on an object storage. The bundle provides an S3 client based on the Amazon AWS SDK v2 .","title":"SDA Commons Server S3"},{"location":"server-s3/#usage","text":"The S3Bundle should be added as a field in the application class instead of being anonymously added in the initialize method like other bundles of this library. Implementations need to refer to the instance to access the client. The Dropwizard applications configuration class needs to provide a S3Configuration . The bundle builder requires to define the getter of the S3Configuration as method reference to access the configuration. Afterward, getClient() is used to access an instance of S3Client that is used to operate on the object storage. See S3BundleTest for a detailed usage example.","title":"Usage"},{"location":"server-s3/#credentials","text":"Usually you can specify AWS credentials in the S3Configuration . But the bundle also supports AWS default credentials provider chain to retrieve credentials. Most prominently you can pass credentials via environment variables AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY or via system properties aws.accessKeyId and aws.secretAccessKey . If you want to use anonymous credentials, you can set useAnonymousCredentials to true in the S3Configuration .","title":"Credentials"},{"location":"server-s3/#tracing","text":"The bundle comes with OpenTelemetry instrumentation.","title":"Tracing"},{"location":"server-s3/#health-check","text":"The bundle supports the creation of both internal and external health checks. 1 2 3 4 5 private final S3Bundle<Config> s3Bundle = S3Bundle.builder() .withConfigurationProvider(Config::getS3Config) .withHealthCheck(Collections.singleton(Config::getS3Bucket)) .build(); For creation of an external health check, use .withExternalHealthCheck(Iterable<BucketNameProvider<C>>) respectively.","title":"Health Check"},{"location":"server-security/","text":"SDA Commons Server Security \u00b6 The module sda-commons-server-security helps to configure a secure Dropwizard application according to the recommendations \"H\u00e4rtungsma\u00dfnahmen Dropwizard\" available at the internal wiki entry by Timo Pagel. Currently the SecurityBundle addresses Risk: Utilization of HTTP Methods -> HTTP offers several methods to perform actions on a web server. Some of the methods are designed to help developers deploy and test HTTP applications. If a Web server is incorrectly configured, attackers can take advantage of this. For example, cross-site tracing by attackers can be used to determine user data. Cross-site tracing involves cross-site scripting to capture advanced information via the HTTP TRACE method. For example, a cookie protected by HTTP-Only. Risk: Root Start -> If Dropwizard is started with extended privileges as root user, an attacker can attack more easily the operating system after taking over the container. Risk: Loss of source IP address -> After detecting an attack, the source IP address is often blocked first. To do this, the source IP address must be known. If a proxy (e.g. a load balancer) is used before the application, the IP address of the proxy is often specified as the source IP. Risk: Detection of confidential components -> Attackers attempt to determine the components of an application that are in use and then determine the versions that belong to them. Then vulnerabilities in the detected components of the found versions are exploited. The version number is often sent in error pages or via HTTP headers from servers and applications. Risk: Buffer Overflow -> Attackers use buffer overflow to corrupt the execution stack. Attackers send prepared packets to a web application to execute malicious code. Risk: Clickjacking -> Clickjacking integrates an existing website, such as www.sda-se.de , into another malicious website (e.g. www.sda-se.de.xx ). The malicious website inserts a transparent layer (technical IFrame) over the integration of the page www.sda-se.de . In addition, the malicious website installed a key logger that records all mouse movements and keystrokes of a victim and sends them to the attacker. URLs such as www.sda-se.de.xx are often distributed via phishing to obtain user access data. Risk: Interpretation of content by the browser -> Attackers can attempt to upload files to a server, which are then delivered to other users. If the browser of the user/victim interprets the uploaded file, the content type can be misinterpreted. For example, an attacker can hide and upload JavaScript code in images, which is then executed in the victim's browser. Risk: Cross Site Scripting (XSS) -> Attackers attempt to manipulate external resources, such as the content on content delivery services, to execute malicious code on users' browsers. In addition, attackers try to execute malicious JavaScript code via cross-site scripting (XSS) on users' browsers. Risk: Passing on visited URLs to third parties -> In the World Wide Web, referrer refers to the website via which the user accessed the current website or file. In the case of an HTTP request (e.g. a website or an image), the web browser sends the URL of the original website to the web server. The referrer is an optional part of the HTTP request sent to the web server. Although optional, the transmission is preset for all common browsers. Only if the current page is to be retrieved via HTTPS and the page to be retrieved is to be transmitted via HTTP should the referrer not be transmitted. If, on the other hand, the page to be retrieved is also transmitted via HTTPS, the referrer is transmitted independently of the host. Risk: Reloading content into Flash and PDFs -> Flash and PDFs can reload content from other domains. Attackers can use this to reload malicious code. Usage \u00b6 Just add the SecurityBundle to the application to avoid known risks: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 public class MyApp extends Application < MyConfiguration > { @Override public void initialize ( Bootstrap < MyConfiguration > bootstrap ) { // ... bootstrap . addBundle ( SecurityBundle . builder (). build ()); bootstrap . addBundle ( JacksonConfigurationBundle . builder (). build ()); // enables required custom error handlers // ... } @Override public void run ( MyConfiguration configuration , Environment environment ) { // ... } }","title":"Server Security"},{"location":"server-security/#sda-commons-server-security","text":"The module sda-commons-server-security helps to configure a secure Dropwizard application according to the recommendations \"H\u00e4rtungsma\u00dfnahmen Dropwizard\" available at the internal wiki entry by Timo Pagel. Currently the SecurityBundle addresses Risk: Utilization of HTTP Methods -> HTTP offers several methods to perform actions on a web server. Some of the methods are designed to help developers deploy and test HTTP applications. If a Web server is incorrectly configured, attackers can take advantage of this. For example, cross-site tracing by attackers can be used to determine user data. Cross-site tracing involves cross-site scripting to capture advanced information via the HTTP TRACE method. For example, a cookie protected by HTTP-Only. Risk: Root Start -> If Dropwizard is started with extended privileges as root user, an attacker can attack more easily the operating system after taking over the container. Risk: Loss of source IP address -> After detecting an attack, the source IP address is often blocked first. To do this, the source IP address must be known. If a proxy (e.g. a load balancer) is used before the application, the IP address of the proxy is often specified as the source IP. Risk: Detection of confidential components -> Attackers attempt to determine the components of an application that are in use and then determine the versions that belong to them. Then vulnerabilities in the detected components of the found versions are exploited. The version number is often sent in error pages or via HTTP headers from servers and applications. Risk: Buffer Overflow -> Attackers use buffer overflow to corrupt the execution stack. Attackers send prepared packets to a web application to execute malicious code. Risk: Clickjacking -> Clickjacking integrates an existing website, such as www.sda-se.de , into another malicious website (e.g. www.sda-se.de.xx ). The malicious website inserts a transparent layer (technical IFrame) over the integration of the page www.sda-se.de . In addition, the malicious website installed a key logger that records all mouse movements and keystrokes of a victim and sends them to the attacker. URLs such as www.sda-se.de.xx are often distributed via phishing to obtain user access data. Risk: Interpretation of content by the browser -> Attackers can attempt to upload files to a server, which are then delivered to other users. If the browser of the user/victim interprets the uploaded file, the content type can be misinterpreted. For example, an attacker can hide and upload JavaScript code in images, which is then executed in the victim's browser. Risk: Cross Site Scripting (XSS) -> Attackers attempt to manipulate external resources, such as the content on content delivery services, to execute malicious code on users' browsers. In addition, attackers try to execute malicious JavaScript code via cross-site scripting (XSS) on users' browsers. Risk: Passing on visited URLs to third parties -> In the World Wide Web, referrer refers to the website via which the user accessed the current website or file. In the case of an HTTP request (e.g. a website or an image), the web browser sends the URL of the original website to the web server. The referrer is an optional part of the HTTP request sent to the web server. Although optional, the transmission is preset for all common browsers. Only if the current page is to be retrieved via HTTPS and the page to be retrieved is to be transmitted via HTTP should the referrer not be transmitted. If, on the other hand, the page to be retrieved is also transmitted via HTTPS, the referrer is transmitted independently of the host. Risk: Reloading content into Flash and PDFs -> Flash and PDFs can reload content from other domains. Attackers can use this to reload malicious code.","title":"SDA Commons Server Security"},{"location":"server-security/#usage","text":"Just add the SecurityBundle to the application to avoid known risks: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 public class MyApp extends Application < MyConfiguration > { @Override public void initialize ( Bootstrap < MyConfiguration > bootstrap ) { // ... bootstrap . addBundle ( SecurityBundle . builder (). build ()); bootstrap . addBundle ( JacksonConfigurationBundle . builder (). build ()); // enables required custom error handlers // ... } @Override public void run ( MyConfiguration configuration , Environment environment ) { // ... } }","title":"Usage"},{"location":"server-spring-data-mongo/","text":"SDA Commons Server Spring Data Mongo \u00b6 The module sda-commons-server-spring-data-mongo is used to work with MongoDB using Spring Data Mongo . Initialization \u00b6 The SpringDataMongoBundle should be added as a field in the application class instead of being anonymously added in the initialize method like other bundles of this library. The Dropwizard application's config class needs to provide a MongoConfiguration . Please refer to the official documentation how to annotate your entity classes correctly, e.g. by adding @Document , @MongoId or @Indexed . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 public class MyApp extends Application < MyConfiguration > { private final SpringDataMongoBundle < MyConfiguration > springDataMongoBundle = SpringDataMongoBundle . builder () . withConfigurationProvider ( MyConfiguration :: getSpringDataMongo ) . withEntites ( MyEntity . class ) . build (); private PersonRepository personRepository ; @Override public void initialize ( Bootstrap < MyConfiguration > bootstrap ) { bootstrap . addBundle ( springDataMongoBundle ); } @Override public void run ( MyConfiguration configuration , Environment environment ) { personRepository = springDataMongoBundle . createRepository ( PersonRepository . class ); } public MongoOperations getMongoOperations () { return springDataMongoBundle . getMongoOperations (); } public GridFsOperations getGridFsOperations () { return springDataMongoBundle . getGridFsOperations (); } public PersonRepository getPersonRepository () { return personRepository ; } } Configuration \u00b6 The database connection is configured in the config.yaml of the application, specifically the connectionString . 1 2 mongo : connectionString : \"${MONGODB_CONNECTION_STRING:-}\" Example config for developer machines using local-infra : 1 2 mongo : connectionString : \"mongodb://mongo-1:27118,mongo-2:27119,mongo-3:27120/myAppName?replicaSet=sda-replica-set-1\" In tests the config is derived from the MongoDbClassExtension . See sda-commons-server-mongo-testing for details. Inheritance in Entities \u00b6 It is strongly recommended to annotate all types that are used in a field that does not exactly match the type with @TypeAlias . Using @TypeAlias will replace the default class name as discriminator with the given value in the annotation and gives you the ability for refactoring of the model classes. This rule applies for all types that are a subclass of an (abstract) super class, types that are stored in a field defined as Object and all types that are stored in a shared collection. The latter are usually a subtype of an abstract class to support a common repository. It is important to register each class that is annotated with @TypeAlias by using withEntities in the builder of the bundle. If not registered there, the mapping is unknown when reading entities. Health check \u00b6 A health check with the name mongo is automatically registered to test the mongo connection. A simple ping command to the database is used. Index creation \u00b6 The bundle will create indexes automatically by default. You can change the configuration using the builder: 1 2 3 4 5 6 private final SpringDataMongoBundle < MyConfiguration > springDataMongoBundle = SpringDataMongoBundle . builder () . withConfigurationProvider ( MyConfiguration :: getSpringDataMongo ) . withEntites ( MyEntity . class ) . disableAutoIndexCreation () . build (); Spring Data Mongo Repositories \u00b6 The bundle support creating Spring Data Mongo repositories that are defined by an interface. You can create an instance of your repository using the bundle's createRepository method that accepts the interface. 1 2 3 4 public interface PersonRepository extends PagingAndSortingRepository < Person , ObjectId > { // additional custom finder methods go here } 1 var personRepository = springDataMongoBundle . createRepository ( PersonRepository . class ); CA Certificates support \u00b6 Instead of providing caCertificate as an environment variable, mount the CA certificates in PEM format in the directory /var/trust/certificates . Certificates available in subdirectories will also be loaded. Note that this directory is also configurable through the Dropwizard config class. The config class should then provide a CaCertificateConfiguration to the bundle builder. See sda-commons-shared-certificates for details.","title":"Server Spring Data Mongo"},{"location":"server-spring-data-mongo/#sda-commons-server-spring-data-mongo","text":"The module sda-commons-server-spring-data-mongo is used to work with MongoDB using Spring Data Mongo .","title":"SDA Commons Server Spring Data Mongo"},{"location":"server-spring-data-mongo/#initialization","text":"The SpringDataMongoBundle should be added as a field in the application class instead of being anonymously added in the initialize method like other bundles of this library. The Dropwizard application's config class needs to provide a MongoConfiguration . Please refer to the official documentation how to annotate your entity classes correctly, e.g. by adding @Document , @MongoId or @Indexed . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 public class MyApp extends Application < MyConfiguration > { private final SpringDataMongoBundle < MyConfiguration > springDataMongoBundle = SpringDataMongoBundle . builder () . withConfigurationProvider ( MyConfiguration :: getSpringDataMongo ) . withEntites ( MyEntity . class ) . build (); private PersonRepository personRepository ; @Override public void initialize ( Bootstrap < MyConfiguration > bootstrap ) { bootstrap . addBundle ( springDataMongoBundle ); } @Override public void run ( MyConfiguration configuration , Environment environment ) { personRepository = springDataMongoBundle . createRepository ( PersonRepository . class ); } public MongoOperations getMongoOperations () { return springDataMongoBundle . getMongoOperations (); } public GridFsOperations getGridFsOperations () { return springDataMongoBundle . getGridFsOperations (); } public PersonRepository getPersonRepository () { return personRepository ; } }","title":"Initialization"},{"location":"server-spring-data-mongo/#configuration","text":"The database connection is configured in the config.yaml of the application, specifically the connectionString . 1 2 mongo : connectionString : \"${MONGODB_CONNECTION_STRING:-}\" Example config for developer machines using local-infra : 1 2 mongo : connectionString : \"mongodb://mongo-1:27118,mongo-2:27119,mongo-3:27120/myAppName?replicaSet=sda-replica-set-1\" In tests the config is derived from the MongoDbClassExtension . See sda-commons-server-mongo-testing for details.","title":"Configuration"},{"location":"server-spring-data-mongo/#inheritance-in-entities","text":"It is strongly recommended to annotate all types that are used in a field that does not exactly match the type with @TypeAlias . Using @TypeAlias will replace the default class name as discriminator with the given value in the annotation and gives you the ability for refactoring of the model classes. This rule applies for all types that are a subclass of an (abstract) super class, types that are stored in a field defined as Object and all types that are stored in a shared collection. The latter are usually a subtype of an abstract class to support a common repository. It is important to register each class that is annotated with @TypeAlias by using withEntities in the builder of the bundle. If not registered there, the mapping is unknown when reading entities.","title":"Inheritance in Entities"},{"location":"server-spring-data-mongo/#health-check","text":"A health check with the name mongo is automatically registered to test the mongo connection. A simple ping command to the database is used.","title":"Health check"},{"location":"server-spring-data-mongo/#index-creation","text":"The bundle will create indexes automatically by default. You can change the configuration using the builder: 1 2 3 4 5 6 private final SpringDataMongoBundle < MyConfiguration > springDataMongoBundle = SpringDataMongoBundle . builder () . withConfigurationProvider ( MyConfiguration :: getSpringDataMongo ) . withEntites ( MyEntity . class ) . disableAutoIndexCreation () . build ();","title":"Index creation"},{"location":"server-spring-data-mongo/#spring-data-mongo-repositories","text":"The bundle support creating Spring Data Mongo repositories that are defined by an interface. You can create an instance of your repository using the bundle's createRepository method that accepts the interface. 1 2 3 4 public interface PersonRepository extends PagingAndSortingRepository < Person , ObjectId > { // additional custom finder methods go here } 1 var personRepository = springDataMongoBundle . createRepository ( PersonRepository . class );","title":"Spring Data Mongo Repositories"},{"location":"server-spring-data-mongo/#ca-certificates-support","text":"Instead of providing caCertificate as an environment variable, mount the CA certificates in PEM format in the directory /var/trust/certificates . Certificates available in subdirectories will also be loaded. Note that this directory is also configurable through the Dropwizard config class. The config class should then provide a CaCertificateConfiguration to the bundle builder. See sda-commons-shared-certificates for details.","title":"CA Certificates support"},{"location":"server-testing/","text":"SDA Commons Server Testing \u00b6 The module sda-commons-server-testing is the base module to add unit and integrations test for applications in the SDA SE infrastructure. It provides JUnit test extensions that are helpful in integration tests. Add the module with test scope: 1 testCompile \"org.sdase.commons:sda-commons-server-testing\" In case you want to use JUnit 5 you also have to activate it in your build.gradle: 1 2 3 4 5 6 7 8 test { useJUnitPlatform () } // in case you use the integrationTest plugin: integrationTest { useJUnitPlatform () } Provided Assertions \u00b6 GoldenFileAssertions \u00b6 Special assertions for Path objects to check if a file matches the expected contents and updates them if needed. These assertions are helpful to check if certain files are stored in the repository (like OpenAPI or AsyncApi ). Use this assertion if you want to conveniently store the latest copy of a file in your repository, and let the CI fail if an update has not been committed. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 public class MyTestIT { @Test public void shouldHaveSameFileInRepository () throws IOException { // get expected content from a generator or rest endpoint String expected = ...; // get a path to the file that should be checked Path filePath = Paths . get ( \"my-file.yaml\" ); // assert the file and update the file afterwards GoldenFileAssertions . assertThat ( filePath ) . hasContentAndUpdateGolden ( expected ); } } There is also a assertThat(...).hasYamlContentAndUpdateGolden(...) variant that interprets the content as YAML or JSON and ignores the order of keys. If possible, prefer the other variant since the written content should always be reproducible. Note that the AsyncAPI and OpenAPI generations export reproducible content. SystemPropertyClassExtension \u00b6 The SystemPropertyClassExtension allows for overriding or unsetting system properties for (integration) tests and resets them to their original value when the tests have finished. To use the extension, register it to your test class via the JUnit5 @RegisterExtension : 1 2 3 4 5 6 @RegisterExtension public SystemPropertyClassExtension PROP = new SystemPropertyClassExtension () . setProperty ( PROP_TO_SET , VALUE ) . setProperty ( PROP_TO_SET_SUPPLIER , () -> VALUE ) . unsetProperty ( PROP_TO_UNSET ); Provided JUnit 5 Extensions \u00b6 SystemPropertyClassExtension \u00b6 This module provides the SystemPropertyClassExtension , a JUnit5 test extension to set and unset system properties before running an integration test. To use the extension register it to your test class via the JUnit5 @RegisterExtension : 1 2 3 4 5 6 @RegisterExtension static final SystemPropertyClassExtension PROP = new SystemPropertyClassExtension () . setProperty ( PROP_TO_SET , VALUE ) . setProperty ( PROP_TO_SET_SUPPLIER , () -> VALUE ) . unsetProperty ( PROP_TO_UNSET ); Set and overwritten values will be reset to their original value once the test has finished. JUnit Pioneer \u00b6 For JUnit 5 we will not supply a replacement for the EnvironmentRule and DropwizardRuleHelper . The EnvironmentRule can be replaced with JUnit Pioneer capabilities. Anyway please read the warning above for overriding environment variables. Use DropwizardAppExtension directly as a replacement for the DropwizardRuleHelper .","title":"Server Testing"},{"location":"server-testing/#sda-commons-server-testing","text":"The module sda-commons-server-testing is the base module to add unit and integrations test for applications in the SDA SE infrastructure. It provides JUnit test extensions that are helpful in integration tests. Add the module with test scope: 1 testCompile \"org.sdase.commons:sda-commons-server-testing\" In case you want to use JUnit 5 you also have to activate it in your build.gradle: 1 2 3 4 5 6 7 8 test { useJUnitPlatform () } // in case you use the integrationTest plugin: integrationTest { useJUnitPlatform () }","title":"SDA Commons Server Testing"},{"location":"server-testing/#provided-assertions","text":"","title":"Provided Assertions"},{"location":"server-testing/#goldenfileassertions","text":"Special assertions for Path objects to check if a file matches the expected contents and updates them if needed. These assertions are helpful to check if certain files are stored in the repository (like OpenAPI or AsyncApi ). Use this assertion if you want to conveniently store the latest copy of a file in your repository, and let the CI fail if an update has not been committed. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 public class MyTestIT { @Test public void shouldHaveSameFileInRepository () throws IOException { // get expected content from a generator or rest endpoint String expected = ...; // get a path to the file that should be checked Path filePath = Paths . get ( \"my-file.yaml\" ); // assert the file and update the file afterwards GoldenFileAssertions . assertThat ( filePath ) . hasContentAndUpdateGolden ( expected ); } } There is also a assertThat(...).hasYamlContentAndUpdateGolden(...) variant that interprets the content as YAML or JSON and ignores the order of keys. If possible, prefer the other variant since the written content should always be reproducible. Note that the AsyncAPI and OpenAPI generations export reproducible content.","title":"GoldenFileAssertions"},{"location":"server-testing/#systempropertyclassextension","text":"The SystemPropertyClassExtension allows for overriding or unsetting system properties for (integration) tests and resets them to their original value when the tests have finished. To use the extension, register it to your test class via the JUnit5 @RegisterExtension : 1 2 3 4 5 6 @RegisterExtension public SystemPropertyClassExtension PROP = new SystemPropertyClassExtension () . setProperty ( PROP_TO_SET , VALUE ) . setProperty ( PROP_TO_SET_SUPPLIER , () -> VALUE ) . unsetProperty ( PROP_TO_UNSET );","title":"SystemPropertyClassExtension"},{"location":"server-testing/#provided-junit-5-extensions","text":"","title":"Provided JUnit 5 Extensions"},{"location":"server-testing/#systempropertyclassextension_1","text":"This module provides the SystemPropertyClassExtension , a JUnit5 test extension to set and unset system properties before running an integration test. To use the extension register it to your test class via the JUnit5 @RegisterExtension : 1 2 3 4 5 6 @RegisterExtension static final SystemPropertyClassExtension PROP = new SystemPropertyClassExtension () . setProperty ( PROP_TO_SET , VALUE ) . setProperty ( PROP_TO_SET_SUPPLIER , () -> VALUE ) . unsetProperty ( PROP_TO_UNSET ); Set and overwritten values will be reset to their original value once the test has finished.","title":"SystemPropertyClassExtension"},{"location":"server-testing/#junit-pioneer","text":"For JUnit 5 we will not supply a replacement for the EnvironmentRule and DropwizardRuleHelper . The EnvironmentRule can be replaced with JUnit Pioneer capabilities. Anyway please read the warning above for overriding environment variables. Use DropwizardAppExtension directly as a replacement for the DropwizardRuleHelper .","title":"JUnit Pioneer"},{"location":"server-trace/","text":"SDA Commons Server Trace \u00b6 The module sda-commons-server-trace adds support to track or create a trace token. The trace token is used to correlate a set of service invocations that belongs to the same logically cohesive call of a higher level service offered by the SDA Platform, e.g. interaction service. Every service must forward the received trace token within calls to other services that are part of the SDA Platform. In HTTP REST calls, it's done within the HTTP Header as Trace-Token . If no token is provided within the request, it will be generated automatically and put into the request context. This should be only the case, if a service call firstly enters the SDA Platform. The calls can be correlated in logs and metrics using this trace token that is also added to the MDC . When using new threads for clients to invoke another service, the trace token is not transferred out-of-the-box. The same holds for mentioning the trace token in log entries of new threads. See the documentation about concurrency on how to transfer this context into another thread. Usage \u00b6 The trace token is loaded within a filter that is created and registered by the TraceTokenBundle which must be added to the Dropwizard application: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 public class MyApplication extends Application < MyConfiguration > { public static void main ( final String [] args ) { new MyApplication (). run ( args ); } @Override public void initialize ( Bootstrap < MyConfiguration > bootstrap ) { // ... bootstrap . addBundle ( TraceTokenBundle . builder (). build ()); // ... } @Override public void run ( MyConfiguration configuration , Environment environment ) { // ... } }","title":"Server Trace"},{"location":"server-trace/#sda-commons-server-trace","text":"The module sda-commons-server-trace adds support to track or create a trace token. The trace token is used to correlate a set of service invocations that belongs to the same logically cohesive call of a higher level service offered by the SDA Platform, e.g. interaction service. Every service must forward the received trace token within calls to other services that are part of the SDA Platform. In HTTP REST calls, it's done within the HTTP Header as Trace-Token . If no token is provided within the request, it will be generated automatically and put into the request context. This should be only the case, if a service call firstly enters the SDA Platform. The calls can be correlated in logs and metrics using this trace token that is also added to the MDC . When using new threads for clients to invoke another service, the trace token is not transferred out-of-the-box. The same holds for mentioning the trace token in log entries of new threads. See the documentation about concurrency on how to transfer this context into another thread.","title":"SDA Commons Server Trace"},{"location":"server-trace/#usage","text":"The trace token is loaded within a filter that is created and registered by the TraceTokenBundle which must be added to the Dropwizard application: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 public class MyApplication extends Application < MyConfiguration > { public static void main ( final String [] args ) { new MyApplication (). run ( args ); } @Override public void initialize ( Bootstrap < MyConfiguration > bootstrap ) { // ... bootstrap . addBundle ( TraceTokenBundle . builder (). build ()); // ... } @Override public void run ( MyConfiguration configuration , Environment environment ) { // ... } }","title":"Usage"},{"location":"server-weld-example/","text":"SDA Commons Server Weld Example \u00b6 This module shows an example how to start an application with Weld support and how beans can be created and used. The test shows how to start such an application within an integration test.","title":"Server Weld Example"},{"location":"server-weld-example/#sda-commons-server-weld-example","text":"This module shows an example how to start an application with Weld support and how beans can be created and used. The test shows how to start such an application within an integration test.","title":"SDA Commons Server Weld Example"},{"location":"server-weld-testing/","text":"SDA Commons Server Weld Testing \u00b6 sda-commons-server-weld-testing is used to bootstrap Dropwizard applications inside a Weld-SE container using the DropwizardAppExtension during testing and provides CDI support for Servlets, listeners and resources. Info: We at SDA SE do not use CDI in our microservices anymore. We believe that dependency injection is not helpful for small services. Therefore, this module is not actively maintained by SDA SE developers. Automated security upgrades are enabled. Contributions of new features and bug fixes are welcome. Usage \u00b6 Testing \u00b6 To start a Dropwizard application during testing the WeldAppExtension can be used: 1 2 3 4 5 6 7 8 public class WeldAppITest { @RegisterExtension static final WeldAppExtension < AppConfiguration > APP = new WeldAppExtension <> ( Application . class , ResourceHelpers . resourceFilePath ( \"config.yml\" )); // ... } The WeldAppExtension is a shortcut for creating a DropwizardAppExtension in combination with the WeldTestSupport .","title":"Server Weld Testing"},{"location":"server-weld-testing/#sda-commons-server-weld-testing","text":"sda-commons-server-weld-testing is used to bootstrap Dropwizard applications inside a Weld-SE container using the DropwizardAppExtension during testing and provides CDI support for Servlets, listeners and resources. Info: We at SDA SE do not use CDI in our microservices anymore. We believe that dependency injection is not helpful for small services. Therefore, this module is not actively maintained by SDA SE developers. Automated security upgrades are enabled. Contributions of new features and bug fixes are welcome.","title":"SDA Commons Server Weld Testing"},{"location":"server-weld-testing/#usage","text":"","title":"Usage"},{"location":"server-weld-testing/#testing","text":"To start a Dropwizard application during testing the WeldAppExtension can be used: 1 2 3 4 5 6 7 8 public class WeldAppITest { @RegisterExtension static final WeldAppExtension < AppConfiguration > APP = new WeldAppExtension <> ( Application . class , ResourceHelpers . resourceFilePath ( \"config.yml\" )); // ... } The WeldAppExtension is a shortcut for creating a DropwizardAppExtension in combination with the WeldTestSupport .","title":"Testing"},{"location":"server-weld/","text":"SDA Commons Server Weld \u00b6 sda-commons-server-weld is used to bootstrap Dropwizard applications inside a Weld-SE container and provides CDI support for servlets, listeners and resources. It allows to inject the application class or instances produced by the application class. Info: We at SDA SE do not use CDI in our microservices anymore. We believe that dependency injection is not helpful for small services. Therefore, this module is not actively maintained by SDA SE developers. Automated security upgrades are enabled. Contributions of new features and bug fixes are welcome. Usage \u00b6 Application Bootstrap \u00b6 To bootstrap a Dropwizard application inside a Weld-SE container, use the DropwizardWeldHelper : 1 2 3 public static void main ( final String [] args ) throws Exception { DropwizardWeldHelper . run ( MyApplication . class , args ); } Provided Bundles \u00b6 To optionally use CDI support inside of servlets, use the additional WeldBundle : 1 2 3 public void initialize ( final Bootstrap < AppConfiguration > bootstrap ) { bootstrap . addBundle ( new WeldBundle ()); } Testing \u00b6 See sda-commons-server-weld-testing .","title":"Server Weld"},{"location":"server-weld/#sda-commons-server-weld","text":"sda-commons-server-weld is used to bootstrap Dropwizard applications inside a Weld-SE container and provides CDI support for servlets, listeners and resources. It allows to inject the application class or instances produced by the application class. Info: We at SDA SE do not use CDI in our microservices anymore. We believe that dependency injection is not helpful for small services. Therefore, this module is not actively maintained by SDA SE developers. Automated security upgrades are enabled. Contributions of new features and bug fixes are welcome.","title":"SDA Commons Server Weld"},{"location":"server-weld/#usage","text":"","title":"Usage"},{"location":"server-weld/#application-bootstrap","text":"To bootstrap a Dropwizard application inside a Weld-SE container, use the DropwizardWeldHelper : 1 2 3 public static void main ( final String [] args ) throws Exception { DropwizardWeldHelper . run ( MyApplication . class , args ); }","title":"Application Bootstrap"},{"location":"server-weld/#provided-bundles","text":"To optionally use CDI support inside of servlets, use the additional WeldBundle : 1 2 3 public void initialize ( final Bootstrap < AppConfiguration > bootstrap ) { bootstrap . addBundle ( new WeldBundle ()); }","title":"Provided Bundles"},{"location":"server-weld/#testing","text":"See sda-commons-server-weld-testing .","title":"Testing"},{"location":"shared-asyncapi/","text":"SDA Commons Shared AsyncAPI \u00b6 \u26a0\ufe0f Experimental \u26a0\ufe0f \u00b6 Please be aware that SDA SE is likely to change or remove this artifact in the future. This module contains the AsyncApiGenerator to generate AsyncAPI specs from a template and model classes in a code first approach. The AsyncAPI specification is the industry standard for defining asynchronous APIs. Usage \u00b6 If the code first approach is used to create an AsyncAPI spec this module provides assistance. The suggested way to use this module is: A template file defining the general info rmation, channels and components.messages using the AsyncAPI spec. components.schemas should be omitted. The schema is defined and documented as Java classes in the code as they are used in message handlers and consumers. Jackson, Jakarta Validation and Swagger 2 annotations can be used for documentation. The root classes of messages are referenced in components.messages.YourMessage.payload.$ref as class://your.package.MessageModel . The AsyncApiGenerator is used to combine the template and the generated Json Schema of the models to a self-contained spec file. The generated AsyncAPI spec is committed into source control. This way, the commit history will show intended and unintended changes to the API and the API spec is accessible any time without executing any code. The API can be view in AsyncAPI Studio . It is suggested to use it as a test dependency, build the AsyncAPI in a unit test and verify that it is up-to-date. The GoldenFileAssertions from the test module help here. Example: Build AsyncAPI for Cars asyncapi_template.yaml CarManufactured \u2026 AsyncApiTest Generated asyncapi.yaml 1 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 package org.sdase.commons.shared.asyncapi.test.data.models ; import com.fasterxml.jackson.annotation.JsonPropertyDescription ; import io.swagger.v3.oas.annotations.media.Schema ; import jakarta.validation.constraints.NotBlank ; import jakarta.validation.constraints.NotNull ; import java.time.Instant ; @Schema ( title = \"Car manufactured\" , description = \"A new car was manufactured\" ) @SuppressWarnings ( \"unused\" ) public class CarManufactured extends BaseEvent { @NotBlank @Schema ( description = \"The registration of the vehicle\" , example = \"BB324A81\" ) private String vehicleRegistration ; @NotNull @JsonPropertyDescription ( \"The time of manufacturing\" ) private Instant date ; @NotNull @JsonPropertyDescription ( \"The model of the car\" ) private CarModel model ; public String getVehicleRegistration () { return vehicleRegistration ; } public CarManufactured setVehicleRegistration ( String vehicleRegistration ) { this . vehicleRegistration = vehicleRegistration ; return this ; } public Instant getDate () { return date ; } public CarManufactured setDate ( Instant date ) { this . date = date ; return this ; } public CarModel getModel () { return model ; } public CarManufactured setModel ( CarModel model ) { this . model = model ; return this ; } } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 package org.sdase.commons.shared.asyncapi.test.data.models ; import com.fasterxml.jackson.annotation.JsonClassDescription ; import com.fasterxml.jackson.annotation.JsonPropertyDescription ; import io.swagger.v3.oas.annotations.media.Schema ; import jakarta.validation.constraints.NotBlank ; import jakarta.validation.constraints.NotNull ; import java.time.Instant ; @JsonClassDescription ( \"A car was scrapped\" ) @SuppressWarnings ( \"unused\" ) public class CarScrapped extends BaseEvent { @NotBlank @JsonPropertyDescription ( \"The registration of the vehicle\" ) @Schema ( example = \"BB324A81\" ) private String vehicleRegistration ; @NotNull @JsonPropertyDescription ( \"The time of scrapping\" ) private Instant date ; @JsonPropertyDescription ( \"The location where the car was scrapped\" ) private String location ; public String getVehicleRegistration () { return vehicleRegistration ; } public CarScrapped setVehicleRegistration ( String vehicleRegistration ) { this . vehicleRegistration = vehicleRegistration ; return this ; } public Instant getDate () { return date ; } public CarScrapped setDate ( Instant date ) { this . date = date ; return this ; } public String getLocation () { return location ; } public CarScrapped setLocation ( String location ) { this . location = location ; return this ; } } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 package org.sdase.commons.shared.asyncapi ; import java.io.IOException ; import java.nio.file.Path ; import java.nio.file.Paths ; import org.junit.jupiter.api.Test ; import org.sdase.commons.server.testing.GoldenFileAssertions ; class AsyncApiTest { @Test void generateAndVerifySpec () throws IOException { // get template var template = getClass (). getResource ( \"/demo/asyncapi_template.yaml\" ); // generate AsyncAPI yaml String expected = AsyncApiGenerator . builder (). withAsyncApiBase ( template ). generateYaml (); // specify where to store the result, e.g. Path.of(\"asyncapi.yaml\") for the project root. Path filePath = Paths . get ( \"asyncapi.yaml\" ); // check and update the file GoldenFileAssertions . assertThat ( filePath ). hasYamlContentAndUpdateGolden ( expected ); } } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 --- asyncapi : \"2.5.0\" id : \"urn:org:sdase:example:cars\" defaultContentType : \"application/json\" info : title : \"Cars Example\" description : \"This example demonstrates how to define events around *cars*.\" version : \"1.0.0\" channels : car-events : publish : operationId : \"publishCarEvents\" summary : \"Car related events\" description : \"These are all events that are related to a car\" message : oneOf : - $ref : \"#/components/messages/CarManufactured\" - $ref : \"#/components/messages/CarScrapped\" components : messages : CarManufactured : title : \"Car Manufactured\" description : \"An event that represents when a new car is manufactured\" payload : $ref : \"#/components/schemas/CarManufactured\" CarScrapped : title : \"Car Scrapped\" description : \"An event that represents when a car is scrapped\" payload : $ref : \"#/components/schemas/CarScrapped\" schemas : CarManufactured : allOf : - type : \"object\" properties : id : type : \"string\" minLength : 1 pattern : \"^.*\\\\S+.*$\" description : \"The id of the message\" examples : - \"626A0F21-D940-4B44-BD36-23F0F567B0D0\" type : allOf : - $ref : \"#/components/schemas/Type\" - description : \"The type of message\" vehicleRegistration : type : \"string\" minLength : 1 pattern : \"^.*\\\\S+.*$\" description : \"The registration of the vehicle\" examples : - \"BB324A81\" date : type : \"string\" format : \"date-time\" description : \"The time of manufacturing\" model : allOf : - $ref : \"#/components/schemas/CarModel\" - description : \"The model of the car\" required : - \"id\" - \"vehicleRegistration\" - \"date\" - \"model\" title : \"Car manufactured\" description : \"A new car was manufactured\" - type : \"object\" properties : type : const : \"CAR_MANUFACTURED\" required : - \"type\" CarModel : anyOf : - $ref : \"#/components/schemas/Electrical\" - $ref : \"#/components/schemas/Combustion\" CarScrapped : allOf : - type : \"object\" properties : id : type : \"string\" minLength : 1 pattern : \"^.*\\\\S+.*$\" description : \"The id of the message\" examples : - \"626A0F21-D940-4B44-BD36-23F0F567B0D0\" type : allOf : - $ref : \"#/components/schemas/Type\" - description : \"The type of message\" vehicleRegistration : type : \"string\" minLength : 1 pattern : \"^.*\\\\S+.*$\" description : \"The registration of the vehicle\" examples : - \"BB324A81\" date : type : \"string\" format : \"date-time\" description : \"The time of scrapping\" location : type : \"string\" description : \"The location where the car was scrapped\" required : - \"id\" - \"vehicleRegistration\" - \"date\" description : \"A car was scrapped\" - type : \"object\" properties : type : const : \"CAR_SCRAPPED\" required : - \"type\" Combustion : allOf : - type : \"object\" properties : name : type : \"string\" description : \"The name of the car model\" examples : - \"Tesla Roadster\" engineType : type : \"string\" description : \"The type of engine\" tankVolume : type : \"integer\" description : \"The capacity of the tank in liter\" examples : - 95 required : - \"tankVolume\" title : \"Combustion engine\" description : \"An car model with a combustion engine\" - type : \"object\" properties : engineType : const : \"COMBUSTION\" required : - \"engineType\" Electrical : allOf : - type : \"object\" properties : name : type : \"string\" description : \"The name of the car model\" examples : - \"Tesla Roadster\" engineType : type : \"string\" description : \"The type of engine\" batteryCapacity : type : \"integer\" description : \"The capacity of the battery in kwH\" examples : - 200 required : - \"batteryCapacity\" title : \"Electrical engine\" description : \"An car model with an electrical engine\" - type : \"object\" properties : engineType : const : \"ELECTRICAL\" required : - \"engineType\" Type : type : \"string\" enum : - \"CAR_MANUFACTURED\" - \"CAR_SCRAPPED\" Usage with Existing Schemas \u00b6 In some cases it is not possible to generate a schema with appropriate documentation, e.g. when a framework requires to use classes from dependencies that do not contain the expected annotations. In this case the schema may be added to the template. This should be used as fallback only, because the schema is not connected to the actual code, it may diverge over time. Example: Build AsyncAPI with handcrafted schema template_with_schema.yaml Created ApiWithSchemaTest Generated asyncapi-schema.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 asyncapi : '2.5.0' id : 'urn:org:sdase:example' defaultContentType : application/json info : title : Example description : This example demonstrates how to define messages with hand crafted schemas. version : '1.0.0' channels : 'car-events' : publish : summary : An entity stream description : What happens to an entity message : oneOf : - $ref : '#/components/messages/Created' - $ref : '#/components/messages/Deleted' components : messages : Created : title : Entity created payload : # referencing the full name of the Class $ref : 'class://org.sdase.commons.shared.asyncapi.test.data.models.Created' Deleted : title : Entity deleted description : Deletion of the entity is represented by an external tombstone message. payload : # referencing the existing schema $ref : '#/components/schemas/Tombstone' schemas : Tombstone : type : object description : | The tombstone event is published to indicate that the entity has been deleted. All copies of data related to the entity must be deleted. properties : id : type : string tombstone : type : boolean const : true 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 package org.sdase.commons.shared.asyncapi.test.data.models ; import jakarta.validation.constraints.NotBlank ; import jakarta.validation.constraints.NotNull ; import jakarta.validation.constraints.Pattern ; @SuppressWarnings ( \"unused\" ) public class Created { @NotNull @Pattern ( regexp = \"[a-zA-Z0-9-_]{10,}\" ) private String id ; @NotBlank private String name ; public String getId () { return id ; } public Created setId ( String id ) { this . id = id ; return this ; } public String getName () { return name ; } public Created setName ( String name ) { this . name = name ; return this ; } } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 package org.sdase.commons.shared.asyncapi ; import java.io.IOException ; import java.nio.file.Path ; import java.nio.file.Paths ; import org.junit.jupiter.api.Test ; import org.sdase.commons.server.testing.GoldenFileAssertions ; class ApiWithSchemaTest { @Test void generateAndVerifySpec () throws IOException { // get template var template = getClass (). getResource ( \"/demo/template_with_schema.yaml\" ); // generate AsyncAPI yaml String expected = AsyncApiGenerator . builder (). withAsyncApiBase ( template ). generateYaml (); // specify where to store the result, e.g. Path.of(\"asyncapi.yaml\") for the project root. Path filePath = Paths . get ( \"asyncapi-schema.yaml\" ); // check and update the file GoldenFileAssertions . assertThat ( filePath ). hasYamlContentAndUpdateGolden ( expected ); } } 1 Generating Schema Files \u00b6 If desired, the module also allows to generate the JSON schema files, for example to use them to validate test data. Use JsonSchemaGenerator to create standalone JSON schemas: 1 2 3 4 5 String expected = JsonSchemaGenerator . builder () . forClass ( BaseEvent . class ) . withFailOnUnknownProperties ( true ) . generateYaml (); Document Models \u00b6 You can document the models using annotations like JsonPropertyDescription from Jackson or JsonSchemaExamples from mbknor-jackson-jsonSchema . See the tests of this module for example model classes . Note that this requires to add the module as compile time dependency.","title":"Shared Async API"},{"location":"shared-asyncapi/#sda-commons-shared-asyncapi","text":"","title":"SDA Commons Shared AsyncAPI"},{"location":"shared-asyncapi/#experimental","text":"Please be aware that SDA SE is likely to change or remove this artifact in the future. This module contains the AsyncApiGenerator to generate AsyncAPI specs from a template and model classes in a code first approach. The AsyncAPI specification is the industry standard for defining asynchronous APIs.","title":"\u26a0\ufe0f Experimental \u26a0\ufe0f"},{"location":"shared-asyncapi/#usage","text":"If the code first approach is used to create an AsyncAPI spec this module provides assistance. The suggested way to use this module is: A template file defining the general info rmation, channels and components.messages using the AsyncAPI spec. components.schemas should be omitted. The schema is defined and documented as Java classes in the code as they are used in message handlers and consumers. Jackson, Jakarta Validation and Swagger 2 annotations can be used for documentation. The root classes of messages are referenced in components.messages.YourMessage.payload.$ref as class://your.package.MessageModel . The AsyncApiGenerator is used to combine the template and the generated Json Schema of the models to a self-contained spec file. The generated AsyncAPI spec is committed into source control. This way, the commit history will show intended and unintended changes to the API and the API spec is accessible any time without executing any code. The API can be view in AsyncAPI Studio . It is suggested to use it as a test dependency, build the AsyncAPI in a unit test and verify that it is up-to-date. The GoldenFileAssertions from the test module help here. Example: Build AsyncAPI for Cars asyncapi_template.yaml CarManufactured \u2026 AsyncApiTest Generated asyncapi.yaml 1 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 package org.sdase.commons.shared.asyncapi.test.data.models ; import com.fasterxml.jackson.annotation.JsonPropertyDescription ; import io.swagger.v3.oas.annotations.media.Schema ; import jakarta.validation.constraints.NotBlank ; import jakarta.validation.constraints.NotNull ; import java.time.Instant ; @Schema ( title = \"Car manufactured\" , description = \"A new car was manufactured\" ) @SuppressWarnings ( \"unused\" ) public class CarManufactured extends BaseEvent { @NotBlank @Schema ( description = \"The registration of the vehicle\" , example = \"BB324A81\" ) private String vehicleRegistration ; @NotNull @JsonPropertyDescription ( \"The time of manufacturing\" ) private Instant date ; @NotNull @JsonPropertyDescription ( \"The model of the car\" ) private CarModel model ; public String getVehicleRegistration () { return vehicleRegistration ; } public CarManufactured setVehicleRegistration ( String vehicleRegistration ) { this . vehicleRegistration = vehicleRegistration ; return this ; } public Instant getDate () { return date ; } public CarManufactured setDate ( Instant date ) { this . date = date ; return this ; } public CarModel getModel () { return model ; } public CarManufactured setModel ( CarModel model ) { this . model = model ; return this ; } } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 package org.sdase.commons.shared.asyncapi.test.data.models ; import com.fasterxml.jackson.annotation.JsonClassDescription ; import com.fasterxml.jackson.annotation.JsonPropertyDescription ; import io.swagger.v3.oas.annotations.media.Schema ; import jakarta.validation.constraints.NotBlank ; import jakarta.validation.constraints.NotNull ; import java.time.Instant ; @JsonClassDescription ( \"A car was scrapped\" ) @SuppressWarnings ( \"unused\" ) public class CarScrapped extends BaseEvent { @NotBlank @JsonPropertyDescription ( \"The registration of the vehicle\" ) @Schema ( example = \"BB324A81\" ) private String vehicleRegistration ; @NotNull @JsonPropertyDescription ( \"The time of scrapping\" ) private Instant date ; @JsonPropertyDescription ( \"The location where the car was scrapped\" ) private String location ; public String getVehicleRegistration () { return vehicleRegistration ; } public CarScrapped setVehicleRegistration ( String vehicleRegistration ) { this . vehicleRegistration = vehicleRegistration ; return this ; } public Instant getDate () { return date ; } public CarScrapped setDate ( Instant date ) { this . date = date ; return this ; } public String getLocation () { return location ; } public CarScrapped setLocation ( String location ) { this . location = location ; return this ; } } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 package org.sdase.commons.shared.asyncapi ; import java.io.IOException ; import java.nio.file.Path ; import java.nio.file.Paths ; import org.junit.jupiter.api.Test ; import org.sdase.commons.server.testing.GoldenFileAssertions ; class AsyncApiTest { @Test void generateAndVerifySpec () throws IOException { // get template var template = getClass (). getResource ( \"/demo/asyncapi_template.yaml\" ); // generate AsyncAPI yaml String expected = AsyncApiGenerator . builder (). withAsyncApiBase ( template ). generateYaml (); // specify where to store the result, e.g. Path.of(\"asyncapi.yaml\") for the project root. Path filePath = Paths . get ( \"asyncapi.yaml\" ); // check and update the file GoldenFileAssertions . assertThat ( filePath ). hasYamlContentAndUpdateGolden ( expected ); } } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 --- asyncapi : \"2.5.0\" id : \"urn:org:sdase:example:cars\" defaultContentType : \"application/json\" info : title : \"Cars Example\" description : \"This example demonstrates how to define events around *cars*.\" version : \"1.0.0\" channels : car-events : publish : operationId : \"publishCarEvents\" summary : \"Car related events\" description : \"These are all events that are related to a car\" message : oneOf : - $ref : \"#/components/messages/CarManufactured\" - $ref : \"#/components/messages/CarScrapped\" components : messages : CarManufactured : title : \"Car Manufactured\" description : \"An event that represents when a new car is manufactured\" payload : $ref : \"#/components/schemas/CarManufactured\" CarScrapped : title : \"Car Scrapped\" description : \"An event that represents when a car is scrapped\" payload : $ref : \"#/components/schemas/CarScrapped\" schemas : CarManufactured : allOf : - type : \"object\" properties : id : type : \"string\" minLength : 1 pattern : \"^.*\\\\S+.*$\" description : \"The id of the message\" examples : - \"626A0F21-D940-4B44-BD36-23F0F567B0D0\" type : allOf : - $ref : \"#/components/schemas/Type\" - description : \"The type of message\" vehicleRegistration : type : \"string\" minLength : 1 pattern : \"^.*\\\\S+.*$\" description : \"The registration of the vehicle\" examples : - \"BB324A81\" date : type : \"string\" format : \"date-time\" description : \"The time of manufacturing\" model : allOf : - $ref : \"#/components/schemas/CarModel\" - description : \"The model of the car\" required : - \"id\" - \"vehicleRegistration\" - \"date\" - \"model\" title : \"Car manufactured\" description : \"A new car was manufactured\" - type : \"object\" properties : type : const : \"CAR_MANUFACTURED\" required : - \"type\" CarModel : anyOf : - $ref : \"#/components/schemas/Electrical\" - $ref : \"#/components/schemas/Combustion\" CarScrapped : allOf : - type : \"object\" properties : id : type : \"string\" minLength : 1 pattern : \"^.*\\\\S+.*$\" description : \"The id of the message\" examples : - \"626A0F21-D940-4B44-BD36-23F0F567B0D0\" type : allOf : - $ref : \"#/components/schemas/Type\" - description : \"The type of message\" vehicleRegistration : type : \"string\" minLength : 1 pattern : \"^.*\\\\S+.*$\" description : \"The registration of the vehicle\" examples : - \"BB324A81\" date : type : \"string\" format : \"date-time\" description : \"The time of scrapping\" location : type : \"string\" description : \"The location where the car was scrapped\" required : - \"id\" - \"vehicleRegistration\" - \"date\" description : \"A car was scrapped\" - type : \"object\" properties : type : const : \"CAR_SCRAPPED\" required : - \"type\" Combustion : allOf : - type : \"object\" properties : name : type : \"string\" description : \"The name of the car model\" examples : - \"Tesla Roadster\" engineType : type : \"string\" description : \"The type of engine\" tankVolume : type : \"integer\" description : \"The capacity of the tank in liter\" examples : - 95 required : - \"tankVolume\" title : \"Combustion engine\" description : \"An car model with a combustion engine\" - type : \"object\" properties : engineType : const : \"COMBUSTION\" required : - \"engineType\" Electrical : allOf : - type : \"object\" properties : name : type : \"string\" description : \"The name of the car model\" examples : - \"Tesla Roadster\" engineType : type : \"string\" description : \"The type of engine\" batteryCapacity : type : \"integer\" description : \"The capacity of the battery in kwH\" examples : - 200 required : - \"batteryCapacity\" title : \"Electrical engine\" description : \"An car model with an electrical engine\" - type : \"object\" properties : engineType : const : \"ELECTRICAL\" required : - \"engineType\" Type : type : \"string\" enum : - \"CAR_MANUFACTURED\" - \"CAR_SCRAPPED\"","title":"Usage"},{"location":"shared-asyncapi/#usage-with-existing-schemas","text":"In some cases it is not possible to generate a schema with appropriate documentation, e.g. when a framework requires to use classes from dependencies that do not contain the expected annotations. In this case the schema may be added to the template. This should be used as fallback only, because the schema is not connected to the actual code, it may diverge over time. Example: Build AsyncAPI with handcrafted schema template_with_schema.yaml Created ApiWithSchemaTest Generated asyncapi-schema.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 asyncapi : '2.5.0' id : 'urn:org:sdase:example' defaultContentType : application/json info : title : Example description : This example demonstrates how to define messages with hand crafted schemas. version : '1.0.0' channels : 'car-events' : publish : summary : An entity stream description : What happens to an entity message : oneOf : - $ref : '#/components/messages/Created' - $ref : '#/components/messages/Deleted' components : messages : Created : title : Entity created payload : # referencing the full name of the Class $ref : 'class://org.sdase.commons.shared.asyncapi.test.data.models.Created' Deleted : title : Entity deleted description : Deletion of the entity is represented by an external tombstone message. payload : # referencing the existing schema $ref : '#/components/schemas/Tombstone' schemas : Tombstone : type : object description : | The tombstone event is published to indicate that the entity has been deleted. All copies of data related to the entity must be deleted. properties : id : type : string tombstone : type : boolean const : true 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 package org.sdase.commons.shared.asyncapi.test.data.models ; import jakarta.validation.constraints.NotBlank ; import jakarta.validation.constraints.NotNull ; import jakarta.validation.constraints.Pattern ; @SuppressWarnings ( \"unused\" ) public class Created { @NotNull @Pattern ( regexp = \"[a-zA-Z0-9-_]{10,}\" ) private String id ; @NotBlank private String name ; public String getId () { return id ; } public Created setId ( String id ) { this . id = id ; return this ; } public String getName () { return name ; } public Created setName ( String name ) { this . name = name ; return this ; } } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 package org.sdase.commons.shared.asyncapi ; import java.io.IOException ; import java.nio.file.Path ; import java.nio.file.Paths ; import org.junit.jupiter.api.Test ; import org.sdase.commons.server.testing.GoldenFileAssertions ; class ApiWithSchemaTest { @Test void generateAndVerifySpec () throws IOException { // get template var template = getClass (). getResource ( \"/demo/template_with_schema.yaml\" ); // generate AsyncAPI yaml String expected = AsyncApiGenerator . builder (). withAsyncApiBase ( template ). generateYaml (); // specify where to store the result, e.g. Path.of(\"asyncapi.yaml\") for the project root. Path filePath = Paths . get ( \"asyncapi-schema.yaml\" ); // check and update the file GoldenFileAssertions . assertThat ( filePath ). hasYamlContentAndUpdateGolden ( expected ); } } 1","title":"Usage with Existing Schemas"},{"location":"shared-asyncapi/#generating-schema-files","text":"If desired, the module also allows to generate the JSON schema files, for example to use them to validate test data. Use JsonSchemaGenerator to create standalone JSON schemas: 1 2 3 4 5 String expected = JsonSchemaGenerator . builder () . forClass ( BaseEvent . class ) . withFailOnUnknownProperties ( true ) . generateYaml ();","title":"Generating Schema Files"},{"location":"shared-asyncapi/#document-models","text":"You can document the models using annotations like JsonPropertyDescription from Jackson or JsonSchemaExamples from mbknor-jackson-jsonSchema . See the tests of this module for example model classes . Note that this requires to add the module as compile time dependency.","title":"Document Models"},{"location":"shared-certificates/","text":"SDA Commons Shared Certificates \u00b6 This module is responsible for looking CA certificates in PEM format in a default (but configurable) directory and putting the parsed certificates into the truststore. These certificates are used to verify SSL connections to the database. Usage \u00b6 The CaCertificatesBundle should be added as a field in the bundle class instead of being anonymously added in the initialize method like other bundles of this library, so we can use it to get the SSLContext in the run method. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 public class MyBundle < C extends Configuration > implements ConfiguredBundle < C > { private final CaCertificatesBundle . FinalBuilder < C > caCertificatesBundleBuilder ; private CaCertificatesBundle < C > caCertificatesBundle ; private SSLContext sslContext ; private MyBundle ( //... CaCertificatesBundle . FinalBuilder < C > caCertificatesBundleBuilder ) { // ... this . caCertificatesBundleBuilder = caCertificatesBundleBuilder ; } @Override public void initialize ( Bootstrap <?> bootstrap ) { this . caCertificatesBundle = caCertificatesBundleBuilder . build (); bootstrap . addBundle (( ConfiguredBundle ) this . caCertificatesBundle ); } // ... @Override public void run ( C configuration , Environment environment ) { //... // get the sslContext instance produced by the caCertificateBundle this . sslContext = this . caCertificatesBundle . getSslContext (); } public static class Builder < T extends Configuration > { private CaCertificatesBundle . FinalBuilder < T > caCertificatesBundleBuilder = CaCertificatesBundle . builder (); public Builder < T > withCaCertificateConfigProvider ( CaCertificateConfigurationProvider < T > configProvider ) { this . caCertificatesBundleBuilder = CaCertificatesBundle . builder (). withCaCertificateConfigProvider ( configProvider ); return this ; } //... public MyBundle < T > build (){ return new MyBundle <> ( //..., caCertificatesBundleBuilder ); } } } Configuration \u00b6 The Dropwizard applications config class needs to provide a CaCertificateConfiguration . The directory that contains CA certificates in PEM format is configured in the config.yaml of the final application config. Example config for production to be used with environment variables of the cluster configuration: 1 2 caCertificate : customCaCertificateDir : \"${CA_CERTIFICATE_CUSTOM_DIR:-/var/trust/certificates}\" If no configuration is used in a service, the default path /var/trust/certificates is checked for PEM certificates. This is the preferred approach to keep all services using this library consistent. But configuration may be needed if unit tests must cover the use of a custom SSLContext.","title":"Shared Certificates"},{"location":"shared-certificates/#sda-commons-shared-certificates","text":"This module is responsible for looking CA certificates in PEM format in a default (but configurable) directory and putting the parsed certificates into the truststore. These certificates are used to verify SSL connections to the database.","title":"SDA Commons Shared Certificates"},{"location":"shared-certificates/#usage","text":"The CaCertificatesBundle should be added as a field in the bundle class instead of being anonymously added in the initialize method like other bundles of this library, so we can use it to get the SSLContext in the run method. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 public class MyBundle < C extends Configuration > implements ConfiguredBundle < C > { private final CaCertificatesBundle . FinalBuilder < C > caCertificatesBundleBuilder ; private CaCertificatesBundle < C > caCertificatesBundle ; private SSLContext sslContext ; private MyBundle ( //... CaCertificatesBundle . FinalBuilder < C > caCertificatesBundleBuilder ) { // ... this . caCertificatesBundleBuilder = caCertificatesBundleBuilder ; } @Override public void initialize ( Bootstrap <?> bootstrap ) { this . caCertificatesBundle = caCertificatesBundleBuilder . build (); bootstrap . addBundle (( ConfiguredBundle ) this . caCertificatesBundle ); } // ... @Override public void run ( C configuration , Environment environment ) { //... // get the sslContext instance produced by the caCertificateBundle this . sslContext = this . caCertificatesBundle . getSslContext (); } public static class Builder < T extends Configuration > { private CaCertificatesBundle . FinalBuilder < T > caCertificatesBundleBuilder = CaCertificatesBundle . builder (); public Builder < T > withCaCertificateConfigProvider ( CaCertificateConfigurationProvider < T > configProvider ) { this . caCertificatesBundleBuilder = CaCertificatesBundle . builder (). withCaCertificateConfigProvider ( configProvider ); return this ; } //... public MyBundle < T > build (){ return new MyBundle <> ( //..., caCertificatesBundleBuilder ); } } }","title":"Usage"},{"location":"shared-certificates/#configuration","text":"The Dropwizard applications config class needs to provide a CaCertificateConfiguration . The directory that contains CA certificates in PEM format is configured in the config.yaml of the final application config. Example config for production to be used with environment variables of the cluster configuration: 1 2 caCertificate : customCaCertificateDir : \"${CA_CERTIFICATE_CUSTOM_DIR:-/var/trust/certificates}\" If no configuration is used in a service, the default path /var/trust/certificates is checked for PEM certificates. This is the preferred approach to keep all services using this library consistent. But configuration may be needed if unit tests must cover the use of a custom SSLContext.","title":"Configuration"},{"location":"shared-error/","text":"SDA Commons Shared Error \u00b6 This module contains basic classes for a common error handling.","title":"Shared Error"},{"location":"shared-error/#sda-commons-shared-error","text":"This module contains basic classes for a common error handling.","title":"SDA Commons Shared Error"},{"location":"shared-forms/","text":"SDA Commons Shared Forms \u00b6 The module sda-commons-shared-forms adds all required dependencies to support multipart/* in Dropwizard applications. Clients will be automatically configured to support multipart/* if this module is available. To support multipart/* as a server, Dropwizard's io.dropwizard.forms.MultiPartBundle has to be added: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 public class MyApplication extends Application < MyConfiguration > { public static void main ( final String [] args ) { new MyApplication (). run ( args ); } @Override public void initialize ( Bootstrap < MyConfiguration > bootstrap ) { // ... bootstrap . addBundle ( new io . dropwizard . forms . MultiPartBundle ()); // ... } @Override public void run ( MyConfiguration configuration , Environment environment ) { // ... } }","title":"Shared Forms"},{"location":"shared-forms/#sda-commons-shared-forms","text":"The module sda-commons-shared-forms adds all required dependencies to support multipart/* in Dropwizard applications. Clients will be automatically configured to support multipart/* if this module is available. To support multipart/* as a server, Dropwizard's io.dropwizard.forms.MultiPartBundle has to be added: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 public class MyApplication extends Application < MyConfiguration > { public static void main ( final String [] args ) { new MyApplication (). run ( args ); } @Override public void initialize ( Bootstrap < MyConfiguration > bootstrap ) { // ... bootstrap . addBundle ( new io . dropwizard . forms . MultiPartBundle ()); // ... } @Override public void run ( MyConfiguration configuration , Environment environment ) { // ... } }","title":"SDA Commons Shared Forms"},{"location":"shared-tracing/","text":"SDA Commons Shared Tracing \u00b6 This module contains some conventions about tracing requests and consumers and should be used by all modules that are involved in tracing. These modules may be implemented for servers or clients.","title":"Shared Tracing"},{"location":"shared-tracing/#sda-commons-shared-tracing","text":"This module contains some conventions about tracing requests and consumers and should be used by all modules that are involved in tracing. These modules may be implemented for servers or clients.","title":"SDA Commons Shared Tracing"},{"location":"shared-yaml/","text":"SDA Commons Shared YAML \u00b6 This module contains a class YamlUtil providing static methods for YAML-file handling. Usage \u00b6 The YamlUtil provides an overloaded method for loading YAML-files and one for serialization. A single object \u00b6 Given the following YAML-file 1 2 message : \"Hello\" attribute : \"attribute1\" it's possible to load and serialize a single object: 1 2 3 4 5 6 7 8 9 10 11 12 13 import org.sdase.commons.shared.yaml.YamlUtil ; class MyClass { public static void main ( final String [] args ) { InputStream resource = this . getClass (). getClassLoader (). getResourceAsStream ( \"sample.yml\" ); TestBean tb = YamlUtil . load ( resource , TestBean . class ); // ... String serializedClass = YamlUtil . writeValueAsString ( tb ); } } List of objects \u00b6 To load a list of objects from a YAML-file like 1 2 3 4 5 6 - message : Hello World! attribute : Foo id : 123 - message : Hello Universe! attribute : Bar id : 456 use a TypeReference<T> as the second parameter: 1 List < TestBean > beans = YamlUtil . load ( resource , new TypeReference < List < TestBean >> () {}); Multiple documents \u00b6 To load a YAML-file that contains multiple YAML-documents, like 1 2 3 4 5 6 7 message : Hello World! attribute : Foo id : 123 --- message : Hello Universe! attribute : Bar id : 456 use YamlUtil.loadList() : 1 List < TestBean > beans = YamlUtil . loadList ( resource , TestBean . class );","title":"Shared Yaml"},{"location":"shared-yaml/#sda-commons-shared-yaml","text":"This module contains a class YamlUtil providing static methods for YAML-file handling.","title":"SDA Commons Shared YAML"},{"location":"shared-yaml/#usage","text":"The YamlUtil provides an overloaded method for loading YAML-files and one for serialization.","title":"Usage"},{"location":"shared-yaml/#a-single-object","text":"Given the following YAML-file 1 2 message : \"Hello\" attribute : \"attribute1\" it's possible to load and serialize a single object: 1 2 3 4 5 6 7 8 9 10 11 12 13 import org.sdase.commons.shared.yaml.YamlUtil ; class MyClass { public static void main ( final String [] args ) { InputStream resource = this . getClass (). getClassLoader (). getResourceAsStream ( \"sample.yml\" ); TestBean tb = YamlUtil . load ( resource , TestBean . class ); // ... String serializedClass = YamlUtil . writeValueAsString ( tb ); } }","title":"A single object"},{"location":"shared-yaml/#list-of-objects","text":"To load a list of objects from a YAML-file like 1 2 3 4 5 6 - message : Hello World! attribute : Foo id : 123 - message : Hello Universe! attribute : Bar id : 456 use a TypeReference<T> as the second parameter: 1 List < TestBean > beans = YamlUtil . load ( resource , new TypeReference < List < TestBean >> () {});","title":"List of objects"},{"location":"shared-yaml/#multiple-documents","text":"To load a YAML-file that contains multiple YAML-documents, like 1 2 3 4 5 6 7 message : Hello World! attribute : Foo id : 123 --- message : Hello Universe! attribute : Bar id : 456 use YamlUtil.loadList() : 1 List < TestBean > beans = YamlUtil . loadList ( resource , TestBean . class );","title":"Multiple documents"},{"location":"starter-example/","text":"SDA Commons Starter Example \u00b6 This example module shows an application that uses the SdaPlatformBundle to bootstrap a Dropwizard application for use in the SDA Platform. Beside the initialization of the bundle, it includes a REST endpoint to demonstrate registration of endpoints to map resources. The integration test shows how the application is bootstrapped in tests. The tests show the capabilities of a standard platform application. The provided local-config.yaml allows to start the application without the need for authentication locally using a run configuration of the favourite IDE that defines the program arguments server sda-commons-starter-example/local-config.yaml . Note that there will be no data available and the example application does not provide POST endpoints. All that is available is an empty array at GET /people and the simple Swagger documentation at GET /swagger.json or GET /swagger.yaml The config.yaml is an example how the application can be started in production. Such file should be copied in the Docker container so that the variables can be populated using the environment configured by the orchestration tool (e.g. Kubernetes).","title":"Starter Example"},{"location":"starter-example/#sda-commons-starter-example","text":"This example module shows an application that uses the SdaPlatformBundle to bootstrap a Dropwizard application for use in the SDA Platform. Beside the initialization of the bundle, it includes a REST endpoint to demonstrate registration of endpoints to map resources. The integration test shows how the application is bootstrapped in tests. The tests show the capabilities of a standard platform application. The provided local-config.yaml allows to start the application without the need for authentication locally using a run configuration of the favourite IDE that defines the program arguments server sda-commons-starter-example/local-config.yaml . Note that there will be no data available and the example application does not provide POST endpoints. All that is available is an empty array at GET /people and the simple Swagger documentation at GET /swagger.json or GET /swagger.yaml The config.yaml is an example how the application can be started in production. Such file should be copied in the Docker container so that the variables can be populated using the environment configured by the orchestration tool (e.g. Kubernetes).","title":"SDA Commons Starter Example"},{"location":"starter/","text":"SDA Commons Starter \u00b6 The module sda-commons-starter provides all basics required to build a service for the SDA Platform with Dropwizard. Apps built with the SdaPlatformBundle automatically contain Support for environment variables in configuration files and default console appender configuration Trace Token support a tolerant ObjectMapper , HAL support and a field filter Security checks on startup Authentication support Prometheus metrics OpenApi documentation Open Telemetry support for the Metadata Context They may be configured easily to allow cross-origin resource sharing use the Open Policy Agent for authorization Using the SdaPlatformBundle is the easiest and fastest way to create a service for the SDA Platform. To bootstrap a Dropwizard application for the SDA Platform only the SdaPlatformBundle has to be added. The API should be documented with Swagger annotations: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 import io.dropwizard.core.Application ; import io.dropwizard.core.setup.Bootstrap ; import io.dropwizard.core.setup.Environment ; import io.swagger.v3.oas.annotations.OpenAPIDefinition ; import SdaPlatformBundle ; import SdaPlatformConfiguration ; @OpenAPIDefinition ( info = @Info ( title = \"My First App\" , description = \"The description of my first application\" , version = \"1.0.0\" , contact = @Contact ( name = \"John Doe\" , email = \"info@example.com\" , url = \"j.doe@example.com\" ), license = @License ( name = \"Sample License\" ))) public class MyFirstApp extends Application < SdaPlatformConfiguration > { public static void main ( String [] args ) throws Exception { new MyFirstApp (). run ( args ); } @Override public void initialize ( Bootstrap < SdaPlatformConfiguration > bootstrap ) { bootstrap . addBundle ( SdaPlatformBundle . builder () . usingSdaPlatformConfiguration () // more Open API data that may also be added with annotations . addOpenApiResourcePackageClass ( this . getClass ()) // or use an existing OpenApi definition . withExistingOpenAPI ( \"openApiJsonOrYaml\" ) . build ()); } @Override public void run ( SdaPlatformConfiguration configuration , Environment environment ) { environment . jersey (). register ( MyFirstEndpoint . class ); } } The SdaPlatformConfiguration may be extended to add application specific configuration properties. The config.yaml of the SdaPlatformConfiguration supports configuration of authentication and CORS additionally to the defaults of Dropwizard's Configuration : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 # See sda-commons-server-auth auth : disableAuth : ${DISABLE_AUTH:-false} leeway : ${AUTH_LEEWAY:-0} keys : ${AUTH_KEYS:-[]} opa : disableOpa : ${OPA_DISABLE:-false} baseUrl : ${OPA_URL:-http://localhost:8181} policyPackage : ${OPA_POLICY_PACKAGE:-<your_package>} readTimeout : ${OPA_READ_TIMEOUT:-500} # See sda-commons-server-cors cors : # List of origins that are allowed to use the service. \"*\" allows all origins allowedOrigins : - \"*\" # Alternative: If the origins should be restricted, you should add the pattern # allowedOrigins: # - https://*.sdase.com # - https://*test.sdase.com # To use configurable patterns per environment the Json in Yaml syntax may be used with an environment placeholder: # allowedOrigins: ${CORS_ALLOWED_ORIGINS:-[\"*\"]} By using .usingSdaPlatformConfiguration() or .usingSdaPlatformConfiguration(MyCustomConfiguration.class) the authorization including the open policy agent are automatically enabled as well as the cors settings. Instead of .usingSdaPlatformConfiguration() and .usingSdaPlatformConfiguration(MyCustomConfiguration.class) , the configuration may be fully customized using .usingCustomConfig(MyCustomConfiguration.class) to support configurations that do not extend SdaPlatformConfiguration . This may also be needed to disable some features of the starter module or add special features such as Authorization. Please note that .withOpaAuthorization(MyConfiguration::getAuth, MyConfiguration::getOpa) will configure the AuthBundle to use .withExternalAuthorization() . Please read the documentation of the Auth Bundle carefully before using this option.","title":"Starter"},{"location":"starter/#sda-commons-starter","text":"The module sda-commons-starter provides all basics required to build a service for the SDA Platform with Dropwizard. Apps built with the SdaPlatformBundle automatically contain Support for environment variables in configuration files and default console appender configuration Trace Token support a tolerant ObjectMapper , HAL support and a field filter Security checks on startup Authentication support Prometheus metrics OpenApi documentation Open Telemetry support for the Metadata Context They may be configured easily to allow cross-origin resource sharing use the Open Policy Agent for authorization Using the SdaPlatformBundle is the easiest and fastest way to create a service for the SDA Platform. To bootstrap a Dropwizard application for the SDA Platform only the SdaPlatformBundle has to be added. The API should be documented with Swagger annotations: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 import io.dropwizard.core.Application ; import io.dropwizard.core.setup.Bootstrap ; import io.dropwizard.core.setup.Environment ; import io.swagger.v3.oas.annotations.OpenAPIDefinition ; import SdaPlatformBundle ; import SdaPlatformConfiguration ; @OpenAPIDefinition ( info = @Info ( title = \"My First App\" , description = \"The description of my first application\" , version = \"1.0.0\" , contact = @Contact ( name = \"John Doe\" , email = \"info@example.com\" , url = \"j.doe@example.com\" ), license = @License ( name = \"Sample License\" ))) public class MyFirstApp extends Application < SdaPlatformConfiguration > { public static void main ( String [] args ) throws Exception { new MyFirstApp (). run ( args ); } @Override public void initialize ( Bootstrap < SdaPlatformConfiguration > bootstrap ) { bootstrap . addBundle ( SdaPlatformBundle . builder () . usingSdaPlatformConfiguration () // more Open API data that may also be added with annotations . addOpenApiResourcePackageClass ( this . getClass ()) // or use an existing OpenApi definition . withExistingOpenAPI ( \"openApiJsonOrYaml\" ) . build ()); } @Override public void run ( SdaPlatformConfiguration configuration , Environment environment ) { environment . jersey (). register ( MyFirstEndpoint . class ); } } The SdaPlatformConfiguration may be extended to add application specific configuration properties. The config.yaml of the SdaPlatformConfiguration supports configuration of authentication and CORS additionally to the defaults of Dropwizard's Configuration : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 # See sda-commons-server-auth auth : disableAuth : ${DISABLE_AUTH:-false} leeway : ${AUTH_LEEWAY:-0} keys : ${AUTH_KEYS:-[]} opa : disableOpa : ${OPA_DISABLE:-false} baseUrl : ${OPA_URL:-http://localhost:8181} policyPackage : ${OPA_POLICY_PACKAGE:-<your_package>} readTimeout : ${OPA_READ_TIMEOUT:-500} # See sda-commons-server-cors cors : # List of origins that are allowed to use the service. \"*\" allows all origins allowedOrigins : - \"*\" # Alternative: If the origins should be restricted, you should add the pattern # allowedOrigins: # - https://*.sdase.com # - https://*test.sdase.com # To use configurable patterns per environment the Json in Yaml syntax may be used with an environment placeholder: # allowedOrigins: ${CORS_ALLOWED_ORIGINS:-[\"*\"]} By using .usingSdaPlatformConfiguration() or .usingSdaPlatformConfiguration(MyCustomConfiguration.class) the authorization including the open policy agent are automatically enabled as well as the cors settings. Instead of .usingSdaPlatformConfiguration() and .usingSdaPlatformConfiguration(MyCustomConfiguration.class) , the configuration may be fully customized using .usingCustomConfig(MyCustomConfiguration.class) to support configurations that do not extend SdaPlatformConfiguration . This may also be needed to disable some features of the starter module or add special features such as Authorization. Please note that .withOpaAuthorization(MyConfiguration::getAuth, MyConfiguration::getOpa) will configure the AuthBundle to use .withExternalAuthorization() . Please read the documentation of the Auth Bundle carefully before using this option.","title":"SDA Commons Starter"},{"location":"migration-guides/MIGRATION_GUIDE_v2_to_v3/","text":"Migration Guide from v2 to v3 \u00b6 The following modules contain changes: sda-commons-server-testing sda-commons-server-auth sda-commons-server-auth-testing sda-commons-server-opentracing sda-commons-server-morphia sda-commons-server-kafka Deployment and Release of upgraded services 1 sda-commons-server-testing \u00b6 Does not provide any JUnit 4 rules anymore. You should find JUnit 5 extensions for all of your rules. We recommend migrating all your JUnit 4 tests to JUnit 5. 2 sda-commons-server-auth \u00b6 The deprecated field readTimeout was removed from the class OpaConfig . Please set the timeout in the opaClient configuration instead. Example: 1 2 3 opa : opaClient : timeout : 500ms 3 sda-commons-server-auth-testing \u00b6 Please change your test-config.yaml if they use ${AUTH_RULE} as placeholder. We wanted to get rid of all references to old JUnit 4 rules. 1 2 - config: ${AUTH_RULE} + config: ${AUTH_CONFIG_KEYS} 4 sda-commons-server-opentracing \u00b6 Migrate from OpenTracing to OpenTelemetry. Starter Bundle \u00b6 If you do not use sda-commons-starter with SdaPlatformBundle , you need to remove the Jaeger bundle and OpenTracing bundle and add the OpenTelemetry bundle. 1 2 3 4 5 6 7 8 9 - bootstrap.addBundle(JaegerBundle.builder().build()); - bootstrap.addBundle(OpenTracingBundle.builder().build()); + bootstrap.addBundle( + OpenTelemetryBundle.builder() + .withAutoConfiguredTelemetryInstance() + .withExcludedUrlsPattern(Pattern.compile(String.join(\"|\", Arrays.asList( + \"/ping\", \"/healthcheck\", \"/healthcheck/internal\", \"/metrics\", + \"/metrics/prometheus\")))) + .build()); Replace OpenTracing dependencies to OpenTelemetry dependencies \u00b6 1 2 - testImplementation 'io.opentracing:opentracing-mock' + testImplementation 'io.opentelemetry:opentelemetry-sdk-testing' Executors need to be instrumented to follow traces when parts of the request are handled asynchronously. \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 - return new AsyncEmailSendManager( - new InMemoryAsyncTaskRepository<>(limits.getMaxAttempts()), - environment - .lifecycle() - .executorService(\"email-sender-%d\") - .minThreads(1) - .maxThreads(limits.getMaxParallel()) - .build(), + return new AsyncEmailSendManager( + new InMemoryAsyncTaskRepository<>(limits.getMaxAttempts()), + io.opentelemetry.context.Context.taskWrapping( + environment + .lifecycle() + .executorService(\"email-sender-%d\") + .minThreads(1) + .maxThreads(limits.getMaxParallel()) + .build()), Environment variables \u00b6 To disable tracing, you must set the variable TRACING_DISABLED to true . The legacy Jaeger environment variables are still supported, but they will be removed in later versions. 1 2 3 - JAEGER_SAMPLER_TYPE=const - JAEGER_SAMPLER_PARAM=0 + TRACING_DISABLED=true New environment variables \u00b6 In order to configure Open Telemetry, you can set some environment variables: Name Default value Description OTEL_PROPAGATORS jaeger,b3,tracecontext,baggage Propagators to be used as a comma-separated list OTEL_SERVICE_NAME value from env JAEGER_SERVICE_NAME (if set) The service name that will appear on tracing OTEL_EXPORTER_JAEGER_ENDPOINT http://jaeger-collector.jaeger:14250 Full URL of the Jaeger HTTP endpoint. The URL must point to the jaeger collector OTEL_TRACES_EXPORTER jaeger Trace exporter to be used (otlp, jaeger, zipkin, none) A full list of the configurable properties can be found in the General SDK Configuration . New API for instrumentation \u00b6 If you use the OpenTracing API for manual instrumentation, you will have to replace it with the OpenTelemetry API. You can find the API Definition to see all the possible methods. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 - public void sendEmail(To recipient, String subject, EmailBody body) { - Span span = GlobalTracer.get().buildSpan(\"sendSmtpMail\").start(); - try (Scope ignored = GlobalTracer.get().scopeManager().activate(span)) { - delegate.sendEmail(recipient, subject, body); - } finally { - span.finish(); - } - } + public void sendEmail(To recipient, String subject, EmailBody body) { + var tracer = GlobalOpenTelemetry.get().getTracer(getClass().getName()); + var span = tracer.spanBuilder(\"sendSmtpMail\").startSpan(); + try (var ignored = span.makeCurrent()) { + delegate.sendEmail(recipient, subject, body); + } finally { + span.end(); + } + } Test Setup \u00b6 Add the opentelemetry test dependency 1 testImplementation 'io.opentelemetry:opentelemetry-sdk-testing' Replace io.opentracing.mock.MockTracer with io.opentelemetry.sdk.testing.junit5.OpenTelemetryExtension 1 2 3 4 - private final MockTracer mockTracer = new MockTracer(); + @RegisterExtension + @Order(0) + static final OpenTelemetryExtension OTEL = OpenTelemetryExtension.create(); - Clear metrics and spans before each test using OpenTelemetryExtension 1 2 3 4 5 6 7 8 9 10 - @BeforeEach - void setupTestData() throws FolderException { - GlobalTracerTestUtil.setGlobalTracerUnconditionally(mockTracer); - mockTracer.reset(); - } + @BeforeEach + void setupTestData() throws FolderException { + OTEL.clearMetrics(); // OTEL is the OpenTelemetryExtension instance + OTEL.clearSpans(); + } Capture spans using OpenTelemetryExtension and io.opentelemetry.sdk.trace.data.SpanData 1 2 3 - assertThat(mockTracer.finishedSpans().stream() - .filter(s -> s.operationName().equals(\"expectedTracing\"))); + assertThat(OTEL.getSpans().stream().filter(s -> s.getName().equals(\"expectedTracing\"))); 5 sda-commons-server-morphia \u00b6 Migrate to Spring Data Mongo . The spring-data-mongo package is available at the following coordinates: 1 implementation \"org.sdase.commons:sda-commons-server-spring-data-mongo\" Replacing the MorphiaBundle: \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 - private final MorphiaBundle<AppConfiguration> morphiaBundle = - MorphiaBundle.builder() - .withConfigurationProvider(AppConfiguration::getMongo) - .withEntityScanPackageClass(ConsentConfigurationEntity.class) - .withValidation() - .build(); + private final SpringDataMongoBundle<AppConfiguration> mongoBundle = + SpringDataMongoBundle.builder() + .withConfigurationProvider(AppConfiguration::getMongo) + .withEntities(ConsentConfigurationEntity.class) + .build(); Morphia's Datastore can be replaced by Spring Data Mongo's MongoOperations : 1 2 3 public MongoOperations getMongoOperations () { return this . mongoBundle . getMongoOperations (); } Mongo Configuration Options \u00b6 The database connection can be configured in the config.yaml of the application. Please consult the README of the Spring Data Mongo module for further information on how to configure your database connection. Please note that we now prefer to configure the MongoDB connection using MongoDB's connection string. All other configuration options like hosts, options, etc. are still available but deprecated and will be removed in the next major release. There is only one exception: The deprecated option caCertificate was removed. For more information how to mount a certificate please read the module's documentation . Morphia compatibility \u00b6 When upgrading a service from 2.x.x a set of converters is provided to stay compatible with a database that was initialized with Morphia. To enable these converters, .withMorphiaCompatibility() can be used when building the SpringDataMongoBundle . 1 2 3 4 SpringDataMongoBundle . builder () . withConfigurationProvider ( AppConfiguration :: getSpringDataMongo ) . withMorphiaCompatibility () build (); Validation \u00b6 Automatic JSR validation is no longer provided in v3. If you still want to validate your models you can do so manually by using the ValidationFactory : 1 2 3 4 5 boolean isValid = Validation . buildDefaultValidatorFactory () . getValidator () . validate ( myEntity ) . isEmpty (); Queries \u00b6 Building queries with the datastore \u00b6 Simple operations can be realised by the MongoOperations directly. Saving an entity: 1 2 3 public void save ( SampleEntity entity ){ mongoOperations . insert ( entity ); } Find by id: 1 2 3 public Optional < SampleEntity > findById ( String id ){ return Optional . ofNullable ( mongoOperations . findById ( id , SampleEntity . class )); } More complex queries can be realised by building a compound Query : 1 2 3 4 5 6 7 8 public Optional < SampleEntity > findByArbitraryFields ( String identifier , String value ) { return Optional . ofNullable ( mongoOperations . findOne ( new Query ( where ( EntityFields . ARBITRARY_FIELD ). is ( identifier )) . and ( EntityFields . OTHER_FIELD ). ne ( value ), SampleEntity . class )); } See the official documentation for further information. Auto-generate queries out of method names with the MongoRepository interface \u00b6 Depending on the complexity, queries can also be auto-generated based on the method names by using the MongoRepository Interface provided by spring-data-mongo. Imagine the following simple entity class: 1 2 3 4 public class Person { private String Name ; private int age ; } Now we can define queries directly through the method names (see the official documentation for the specific syntax). 1 2 3 4 5 6 7 public interface PersonMongoRepository extends MongoRepository < Person , String > { Optional < Person > findByName ( String name ); List < Person > findAllByAgeIsLessThanEqual ( int age ); List < Person > findAllByNameIsNot ( String name ); } See the official documentation for further details. Query functions \u00b6 As Spring Data Mongo doesn't support/provide many query functions provided by Morphia, below are some replacements/alternatives. EqualIgnoreCase \u00b6 Morphia supports usage of equalIgnoreCase() . Use regex() in Spring Data Mongo. For example Morphia - query.criteria(\"fieldName\").equalIgnoreCase(entity.getFieldName()); Spring Data Mongo - query.addCriteria(where(\"fieldName\").regex(Pattern.compile(\"^\" + Pattern.quote(entity.getFieldName()), Pattern.CASE_INSENSITIVE))); HasAnyOf \u00b6 Morphia supports hasAnyOf() method. Use in() in Spring Data Mongo. For example Morphia - query.field(\"id\").hasAnyOf(getIds()); Spring Data Mongo - query.addCriteria(where(\"id\").in(getIds())); Filter \u00b6 Morphia supports filter() method. Use is() in Spring Data Mongo. For example Morphia - query.filter(\"id\", getId()); Spring Data Mongo - query.addCriteria(where(\"id\").is(getId())); Contains \u00b6 Morphia supports contains() method. Use regex() in Spring Data Mongo. For example Morphia - query.criteria(\"fieldName\").contains(entity.getFieldName()); Spring Data Mongo - query.addCriteria(where(\"fieldName\").regex(Pattern.compile(\".*\" + Pattern.quote(entity.getFieldName()) + \".*\"))); HasAllOf \u00b6 Morphia supports contains() method. Use regex() in Spring Data Mongo. For example Morphia - query.field(\"fieldName\").hasAllOf(getFieldNameValues()); Spring Data Mongo - query.addCriteria(where(\"fieldName\").all(getFieldNameValues())); Error handling \u00b6 Morphia raised ValidationException s or subclasses of it when validation errors like duplicate unique keys occur at database level. Spring Data Mongo will throw more specific exceptions like org.springframework.dao.DuplicateKeyException or more generic org.springframework.dao.DataAccessException . Error handling must be updated. Indexes \u00b6 While all @Indexes can be created at the main entity class in Morphia, each field must be @Indexed , @TextIndexed or @WildcardIndexed where it is defined in Spring Data Mongo. A @CompoundIndex can be declared at the entity class. It is important to set the index name in the annotations to match the previous name created by Morphia. MongoDB makes it quite difficult to rename an index. It is highly recommended validating the indexes that are generated by Spring Data MongoDB in comparison with the existing indexes of a database populated with the latest version of a service. Custom converters \u00b6 Custom converters can be added directly to the Entity class in Morphia by using annotation like @Converters(value = {ExampleConverter.class}) . In Spring Data Mongo the custom converters can be added to the builder in your Application.class like below: 1 2 3 4 5 6 7 SpringDataMongoBundle . builder () . withConfigurationProvider ( AppConfiguration :: getSpringDataMongo ) . withEntities ( ExampleClass . class ) . addCustomConverters ( new ExampleReadingConverter (), new ExampleWritingConverter ()) build (); Implementing the converter changes from Morphia interface to using the Spring interface can be done like here ZonedDateTimeReadConverter.java . Annotations \u00b6 Morphia Spring Data Mongo @Entity(noClassnameStored = true, name=\"exampleEntity\") @Document(\"exampleEntity\") . There is no property similar to noClassnameStored as the type/class can't be excluded with Spring Data. @PrePersist There is no replacement annotation for this. If you are using this on any fields, please set the field before save(). One very common example is to set the creation date like this entity.setCreated(ZonedDateTime.now(ZoneOffset.UTC)); @Embedded No replacement available as Spring Data already embeds the document. @Converters() Replaced with @ReadingConverter and @WritingConverter 6 sda-commons-server-kafka \u00b6 Some deprecated code was removed. Especially we removed the feature createTopicIfMissing because we don't recommend services to do that. You usually need different privileges for topic creation that you don't want to give to your services. We also removed the topicana classes in package org.sdase.commons.server.kafka.topicana . For this reason the following methods were removed or changed on MessageListenerRegistration and ProducerRegistration : checkTopicConfiguration This method compared the current topics with the configured ones and threw a org.sdase.commons.server.kafka.topicana.MismatchedTopicConfigException in case they did not match. It was removed in this version. forTopicConfigs This method accepted a list of org.sdase.commons.server.kafka.topicana.ExpectedTopicConfiguration to compare against the current topic configuration. Now it accepts a list of TopicConfig , with only the name of the topic, but it does not perform any check in the configuration. forTopic(ExpectedTopicConfiguration topic) This method accepted an instance of org.sdase.commons.server.kafka.topicana.ExpectedTopicConfiguration to compare against the current topic configuration. Now it accepts a TopicConfig instance, with only the name of the topic, but it does not perform any check in the configuration. Kafka Configuration \u00b6 As topic configuration is not defined in the service anymore, all properties but name have been removed from the topic configuration. 1 2 3 4 5 6 7 8 # example changes in config.yml kafka: topics: event-topic: name: ${KAFKA_CONSUMER_EVENT_TOPIC:-partner-ods-event-topic} - partitions: ${KAFKA_CONSUMER_EVENT_TOPIC_PARTITIONS:-2} - replicationFactor: ${KAFKA_CONSUMER_EVENT_TOPIC_REPLICATION_FACTOR:-1} 7. Deployment and Release of upgraded services \u00b6 The changes mentioned above also have an impact on the deployment of a containerized service. This section summarizes notable changes of deployments, that are derived from the required migration in a service. Services that upgrade to SDA Dropwizard Commons v3 should mention this in their release notes. It may be required to introduce the upgrade as breaking change (aka new major release) if deployments are incompatible to the previous version. Additional service specific changes may apply as well. Tracing \u00b6 Any Jaeger related configuration should be migrated to the Open Telemetry variant. The release notes of a service should provide respective information. Changes with backward compatible fallback: To identify the service, JAEGER_SERVICE_NAME is still supported. Changing to OTEL_SERVICE_NAME is recommended. To disable tracing, JAEGER_SAMPLER_TYPE=const in combination with JAEGER_SAMPLER_PARAM=0 is still supported. Changing to TRACING_DISABLED=true is recommended. Incompatible changes: JAEGER_AGENT_HOST is not supported anymore. The official documentation provides all available options. In an environment that previously used Jaeger, OTEL_TRACES_EXPORTER=jaeger and OTEL_EXPORTER_JAEGER_ENDPOINT=http://jaeger-collector.jaeger:14250 may be suitable. Note that the collector endpoint may not be the same as the agent endpoint configured for Jaeger. Kafka \u00b6 All topic configurations for topic creation and validation are not considered anymore. Users of the service should be informed that the service will not create any topic and does not validate existing topics. Therefore, some configuration options of a service may be removed. MongoDB \u00b6 Configuration of individual parts of the MongoDB Connection String is deprecated. Each service should provide a configuration option for the full MongoDB Connection String and inform about the deprecation in the release notes.","title":"Migrate from v2 to v2"},{"location":"migration-guides/MIGRATION_GUIDE_v2_to_v3/#migration-guide-from-v2-to-v3","text":"The following modules contain changes: sda-commons-server-testing sda-commons-server-auth sda-commons-server-auth-testing sda-commons-server-opentracing sda-commons-server-morphia sda-commons-server-kafka Deployment and Release of upgraded services","title":"Migration Guide from v2 to v3"},{"location":"migration-guides/MIGRATION_GUIDE_v2_to_v3/#1-sda-commons-server-testing","text":"Does not provide any JUnit 4 rules anymore. You should find JUnit 5 extensions for all of your rules. We recommend migrating all your JUnit 4 tests to JUnit 5.","title":"1 sda-commons-server-testing"},{"location":"migration-guides/MIGRATION_GUIDE_v2_to_v3/#2-sda-commons-server-auth","text":"The deprecated field readTimeout was removed from the class OpaConfig . Please set the timeout in the opaClient configuration instead. Example: 1 2 3 opa : opaClient : timeout : 500ms","title":"2 sda-commons-server-auth"},{"location":"migration-guides/MIGRATION_GUIDE_v2_to_v3/#3-sda-commons-server-auth-testing","text":"Please change your test-config.yaml if they use ${AUTH_RULE} as placeholder. We wanted to get rid of all references to old JUnit 4 rules. 1 2 - config: ${AUTH_RULE} + config: ${AUTH_CONFIG_KEYS}","title":"3 sda-commons-server-auth-testing"},{"location":"migration-guides/MIGRATION_GUIDE_v2_to_v3/#4-sda-commons-server-opentracing","text":"Migrate from OpenTracing to OpenTelemetry.","title":"4 sda-commons-server-opentracing"},{"location":"migration-guides/MIGRATION_GUIDE_v2_to_v3/#starter-bundle","text":"If you do not use sda-commons-starter with SdaPlatformBundle , you need to remove the Jaeger bundle and OpenTracing bundle and add the OpenTelemetry bundle. 1 2 3 4 5 6 7 8 9 - bootstrap.addBundle(JaegerBundle.builder().build()); - bootstrap.addBundle(OpenTracingBundle.builder().build()); + bootstrap.addBundle( + OpenTelemetryBundle.builder() + .withAutoConfiguredTelemetryInstance() + .withExcludedUrlsPattern(Pattern.compile(String.join(\"|\", Arrays.asList( + \"/ping\", \"/healthcheck\", \"/healthcheck/internal\", \"/metrics\", + \"/metrics/prometheus\")))) + .build());","title":"Starter Bundle"},{"location":"migration-guides/MIGRATION_GUIDE_v2_to_v3/#replace-opentracing-dependencies-to-opentelemetry-dependencies","text":"1 2 - testImplementation 'io.opentracing:opentracing-mock' + testImplementation 'io.opentelemetry:opentelemetry-sdk-testing'","title":"Replace OpenTracing dependencies to OpenTelemetry dependencies"},{"location":"migration-guides/MIGRATION_GUIDE_v2_to_v3/#executors-need-to-be-instrumented-to-follow-traces-when-parts-of-the-request-are-handled-asynchronously","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 - return new AsyncEmailSendManager( - new InMemoryAsyncTaskRepository<>(limits.getMaxAttempts()), - environment - .lifecycle() - .executorService(\"email-sender-%d\") - .minThreads(1) - .maxThreads(limits.getMaxParallel()) - .build(), + return new AsyncEmailSendManager( + new InMemoryAsyncTaskRepository<>(limits.getMaxAttempts()), + io.opentelemetry.context.Context.taskWrapping( + environment + .lifecycle() + .executorService(\"email-sender-%d\") + .minThreads(1) + .maxThreads(limits.getMaxParallel()) + .build()),","title":"Executors need to be instrumented to follow traces when parts of the request are handled asynchronously."},{"location":"migration-guides/MIGRATION_GUIDE_v2_to_v3/#environment-variables","text":"To disable tracing, you must set the variable TRACING_DISABLED to true . The legacy Jaeger environment variables are still supported, but they will be removed in later versions. 1 2 3 - JAEGER_SAMPLER_TYPE=const - JAEGER_SAMPLER_PARAM=0 + TRACING_DISABLED=true","title":"Environment variables"},{"location":"migration-guides/MIGRATION_GUIDE_v2_to_v3/#new-environment-variables","text":"In order to configure Open Telemetry, you can set some environment variables: Name Default value Description OTEL_PROPAGATORS jaeger,b3,tracecontext,baggage Propagators to be used as a comma-separated list OTEL_SERVICE_NAME value from env JAEGER_SERVICE_NAME (if set) The service name that will appear on tracing OTEL_EXPORTER_JAEGER_ENDPOINT http://jaeger-collector.jaeger:14250 Full URL of the Jaeger HTTP endpoint. The URL must point to the jaeger collector OTEL_TRACES_EXPORTER jaeger Trace exporter to be used (otlp, jaeger, zipkin, none) A full list of the configurable properties can be found in the General SDK Configuration .","title":"New environment variables"},{"location":"migration-guides/MIGRATION_GUIDE_v2_to_v3/#new-api-for-instrumentation","text":"If you use the OpenTracing API for manual instrumentation, you will have to replace it with the OpenTelemetry API. You can find the API Definition to see all the possible methods. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 - public void sendEmail(To recipient, String subject, EmailBody body) { - Span span = GlobalTracer.get().buildSpan(\"sendSmtpMail\").start(); - try (Scope ignored = GlobalTracer.get().scopeManager().activate(span)) { - delegate.sendEmail(recipient, subject, body); - } finally { - span.finish(); - } - } + public void sendEmail(To recipient, String subject, EmailBody body) { + var tracer = GlobalOpenTelemetry.get().getTracer(getClass().getName()); + var span = tracer.spanBuilder(\"sendSmtpMail\").startSpan(); + try (var ignored = span.makeCurrent()) { + delegate.sendEmail(recipient, subject, body); + } finally { + span.end(); + } + }","title":"New API for instrumentation"},{"location":"migration-guides/MIGRATION_GUIDE_v2_to_v3/#test-setup","text":"Add the opentelemetry test dependency 1 testImplementation 'io.opentelemetry:opentelemetry-sdk-testing' Replace io.opentracing.mock.MockTracer with io.opentelemetry.sdk.testing.junit5.OpenTelemetryExtension 1 2 3 4 - private final MockTracer mockTracer = new MockTracer(); + @RegisterExtension + @Order(0) + static final OpenTelemetryExtension OTEL = OpenTelemetryExtension.create(); - Clear metrics and spans before each test using OpenTelemetryExtension 1 2 3 4 5 6 7 8 9 10 - @BeforeEach - void setupTestData() throws FolderException { - GlobalTracerTestUtil.setGlobalTracerUnconditionally(mockTracer); - mockTracer.reset(); - } + @BeforeEach + void setupTestData() throws FolderException { + OTEL.clearMetrics(); // OTEL is the OpenTelemetryExtension instance + OTEL.clearSpans(); + } Capture spans using OpenTelemetryExtension and io.opentelemetry.sdk.trace.data.SpanData 1 2 3 - assertThat(mockTracer.finishedSpans().stream() - .filter(s -> s.operationName().equals(\"expectedTracing\"))); + assertThat(OTEL.getSpans().stream().filter(s -> s.getName().equals(\"expectedTracing\")));","title":"Test Setup"},{"location":"migration-guides/MIGRATION_GUIDE_v2_to_v3/#5-sda-commons-server-morphia","text":"Migrate to Spring Data Mongo . The spring-data-mongo package is available at the following coordinates: 1 implementation \"org.sdase.commons:sda-commons-server-spring-data-mongo\"","title":"5 sda-commons-server-morphia"},{"location":"migration-guides/MIGRATION_GUIDE_v2_to_v3/#replacing-the-morphiabundle","text":"1 2 3 4 5 6 7 8 9 10 11 12 - private final MorphiaBundle<AppConfiguration> morphiaBundle = - MorphiaBundle.builder() - .withConfigurationProvider(AppConfiguration::getMongo) - .withEntityScanPackageClass(ConsentConfigurationEntity.class) - .withValidation() - .build(); + private final SpringDataMongoBundle<AppConfiguration> mongoBundle = + SpringDataMongoBundle.builder() + .withConfigurationProvider(AppConfiguration::getMongo) + .withEntities(ConsentConfigurationEntity.class) + .build(); Morphia's Datastore can be replaced by Spring Data Mongo's MongoOperations : 1 2 3 public MongoOperations getMongoOperations () { return this . mongoBundle . getMongoOperations (); }","title":"Replacing the MorphiaBundle:"},{"location":"migration-guides/MIGRATION_GUIDE_v2_to_v3/#mongo-configuration-options","text":"The database connection can be configured in the config.yaml of the application. Please consult the README of the Spring Data Mongo module for further information on how to configure your database connection. Please note that we now prefer to configure the MongoDB connection using MongoDB's connection string. All other configuration options like hosts, options, etc. are still available but deprecated and will be removed in the next major release. There is only one exception: The deprecated option caCertificate was removed. For more information how to mount a certificate please read the module's documentation .","title":"Mongo Configuration Options"},{"location":"migration-guides/MIGRATION_GUIDE_v2_to_v3/#morphia-compatibility","text":"When upgrading a service from 2.x.x a set of converters is provided to stay compatible with a database that was initialized with Morphia. To enable these converters, .withMorphiaCompatibility() can be used when building the SpringDataMongoBundle . 1 2 3 4 SpringDataMongoBundle . builder () . withConfigurationProvider ( AppConfiguration :: getSpringDataMongo ) . withMorphiaCompatibility () build ();","title":"Morphia compatibility"},{"location":"migration-guides/MIGRATION_GUIDE_v2_to_v3/#validation","text":"Automatic JSR validation is no longer provided in v3. If you still want to validate your models you can do so manually by using the ValidationFactory : 1 2 3 4 5 boolean isValid = Validation . buildDefaultValidatorFactory () . getValidator () . validate ( myEntity ) . isEmpty ();","title":"Validation"},{"location":"migration-guides/MIGRATION_GUIDE_v2_to_v3/#queries","text":"","title":"Queries"},{"location":"migration-guides/MIGRATION_GUIDE_v2_to_v3/#building-queries-with-the-datastore","text":"Simple operations can be realised by the MongoOperations directly. Saving an entity: 1 2 3 public void save ( SampleEntity entity ){ mongoOperations . insert ( entity ); } Find by id: 1 2 3 public Optional < SampleEntity > findById ( String id ){ return Optional . ofNullable ( mongoOperations . findById ( id , SampleEntity . class )); } More complex queries can be realised by building a compound Query : 1 2 3 4 5 6 7 8 public Optional < SampleEntity > findByArbitraryFields ( String identifier , String value ) { return Optional . ofNullable ( mongoOperations . findOne ( new Query ( where ( EntityFields . ARBITRARY_FIELD ). is ( identifier )) . and ( EntityFields . OTHER_FIELD ). ne ( value ), SampleEntity . class )); } See the official documentation for further information.","title":"Building queries with the datastore"},{"location":"migration-guides/MIGRATION_GUIDE_v2_to_v3/#auto-generate-queries-out-of-method-names-with-the-mongorepository-interface","text":"Depending on the complexity, queries can also be auto-generated based on the method names by using the MongoRepository Interface provided by spring-data-mongo. Imagine the following simple entity class: 1 2 3 4 public class Person { private String Name ; private int age ; } Now we can define queries directly through the method names (see the official documentation for the specific syntax). 1 2 3 4 5 6 7 public interface PersonMongoRepository extends MongoRepository < Person , String > { Optional < Person > findByName ( String name ); List < Person > findAllByAgeIsLessThanEqual ( int age ); List < Person > findAllByNameIsNot ( String name ); } See the official documentation for further details.","title":"Auto-generate queries out of method names with the MongoRepository interface"},{"location":"migration-guides/MIGRATION_GUIDE_v2_to_v3/#query-functions","text":"As Spring Data Mongo doesn't support/provide many query functions provided by Morphia, below are some replacements/alternatives.","title":"Query functions"},{"location":"migration-guides/MIGRATION_GUIDE_v2_to_v3/#equalignorecase","text":"Morphia supports usage of equalIgnoreCase() . Use regex() in Spring Data Mongo. For example Morphia - query.criteria(\"fieldName\").equalIgnoreCase(entity.getFieldName()); Spring Data Mongo - query.addCriteria(where(\"fieldName\").regex(Pattern.compile(\"^\" + Pattern.quote(entity.getFieldName()), Pattern.CASE_INSENSITIVE)));","title":"EqualIgnoreCase"},{"location":"migration-guides/MIGRATION_GUIDE_v2_to_v3/#hasanyof","text":"Morphia supports hasAnyOf() method. Use in() in Spring Data Mongo. For example Morphia - query.field(\"id\").hasAnyOf(getIds()); Spring Data Mongo - query.addCriteria(where(\"id\").in(getIds()));","title":"HasAnyOf"},{"location":"migration-guides/MIGRATION_GUIDE_v2_to_v3/#filter","text":"Morphia supports filter() method. Use is() in Spring Data Mongo. For example Morphia - query.filter(\"id\", getId()); Spring Data Mongo - query.addCriteria(where(\"id\").is(getId()));","title":"Filter"},{"location":"migration-guides/MIGRATION_GUIDE_v2_to_v3/#contains","text":"Morphia supports contains() method. Use regex() in Spring Data Mongo. For example Morphia - query.criteria(\"fieldName\").contains(entity.getFieldName()); Spring Data Mongo - query.addCriteria(where(\"fieldName\").regex(Pattern.compile(\".*\" + Pattern.quote(entity.getFieldName()) + \".*\")));","title":"Contains"},{"location":"migration-guides/MIGRATION_GUIDE_v2_to_v3/#hasallof","text":"Morphia supports contains() method. Use regex() in Spring Data Mongo. For example Morphia - query.field(\"fieldName\").hasAllOf(getFieldNameValues()); Spring Data Mongo - query.addCriteria(where(\"fieldName\").all(getFieldNameValues()));","title":"HasAllOf"},{"location":"migration-guides/MIGRATION_GUIDE_v2_to_v3/#error-handling","text":"Morphia raised ValidationException s or subclasses of it when validation errors like duplicate unique keys occur at database level. Spring Data Mongo will throw more specific exceptions like org.springframework.dao.DuplicateKeyException or more generic org.springframework.dao.DataAccessException . Error handling must be updated.","title":"Error handling"},{"location":"migration-guides/MIGRATION_GUIDE_v2_to_v3/#indexes","text":"While all @Indexes can be created at the main entity class in Morphia, each field must be @Indexed , @TextIndexed or @WildcardIndexed where it is defined in Spring Data Mongo. A @CompoundIndex can be declared at the entity class. It is important to set the index name in the annotations to match the previous name created by Morphia. MongoDB makes it quite difficult to rename an index. It is highly recommended validating the indexes that are generated by Spring Data MongoDB in comparison with the existing indexes of a database populated with the latest version of a service.","title":"Indexes"},{"location":"migration-guides/MIGRATION_GUIDE_v2_to_v3/#custom-converters","text":"Custom converters can be added directly to the Entity class in Morphia by using annotation like @Converters(value = {ExampleConverter.class}) . In Spring Data Mongo the custom converters can be added to the builder in your Application.class like below: 1 2 3 4 5 6 7 SpringDataMongoBundle . builder () . withConfigurationProvider ( AppConfiguration :: getSpringDataMongo ) . withEntities ( ExampleClass . class ) . addCustomConverters ( new ExampleReadingConverter (), new ExampleWritingConverter ()) build (); Implementing the converter changes from Morphia interface to using the Spring interface can be done like here ZonedDateTimeReadConverter.java .","title":"Custom converters"},{"location":"migration-guides/MIGRATION_GUIDE_v2_to_v3/#annotations","text":"Morphia Spring Data Mongo @Entity(noClassnameStored = true, name=\"exampleEntity\") @Document(\"exampleEntity\") . There is no property similar to noClassnameStored as the type/class can't be excluded with Spring Data. @PrePersist There is no replacement annotation for this. If you are using this on any fields, please set the field before save(). One very common example is to set the creation date like this entity.setCreated(ZonedDateTime.now(ZoneOffset.UTC)); @Embedded No replacement available as Spring Data already embeds the document. @Converters() Replaced with @ReadingConverter and @WritingConverter","title":"Annotations"},{"location":"migration-guides/MIGRATION_GUIDE_v2_to_v3/#6-sda-commons-server-kafka","text":"Some deprecated code was removed. Especially we removed the feature createTopicIfMissing because we don't recommend services to do that. You usually need different privileges for topic creation that you don't want to give to your services. We also removed the topicana classes in package org.sdase.commons.server.kafka.topicana . For this reason the following methods were removed or changed on MessageListenerRegistration and ProducerRegistration : checkTopicConfiguration This method compared the current topics with the configured ones and threw a org.sdase.commons.server.kafka.topicana.MismatchedTopicConfigException in case they did not match. It was removed in this version. forTopicConfigs This method accepted a list of org.sdase.commons.server.kafka.topicana.ExpectedTopicConfiguration to compare against the current topic configuration. Now it accepts a list of TopicConfig , with only the name of the topic, but it does not perform any check in the configuration. forTopic(ExpectedTopicConfiguration topic) This method accepted an instance of org.sdase.commons.server.kafka.topicana.ExpectedTopicConfiguration to compare against the current topic configuration. Now it accepts a TopicConfig instance, with only the name of the topic, but it does not perform any check in the configuration.","title":"6 sda-commons-server-kafka"},{"location":"migration-guides/MIGRATION_GUIDE_v2_to_v3/#kafka-configuration","text":"As topic configuration is not defined in the service anymore, all properties but name have been removed from the topic configuration. 1 2 3 4 5 6 7 8 # example changes in config.yml kafka: topics: event-topic: name: ${KAFKA_CONSUMER_EVENT_TOPIC:-partner-ods-event-topic} - partitions: ${KAFKA_CONSUMER_EVENT_TOPIC_PARTITIONS:-2} - replicationFactor: ${KAFKA_CONSUMER_EVENT_TOPIC_REPLICATION_FACTOR:-1}","title":"Kafka Configuration"},{"location":"migration-guides/MIGRATION_GUIDE_v2_to_v3/#7-deployment-and-release-of-upgraded-services","text":"The changes mentioned above also have an impact on the deployment of a containerized service. This section summarizes notable changes of deployments, that are derived from the required migration in a service. Services that upgrade to SDA Dropwizard Commons v3 should mention this in their release notes. It may be required to introduce the upgrade as breaking change (aka new major release) if deployments are incompatible to the previous version. Additional service specific changes may apply as well.","title":"7. Deployment and Release of upgraded services"},{"location":"migration-guides/MIGRATION_GUIDE_v2_to_v3/#tracing","text":"Any Jaeger related configuration should be migrated to the Open Telemetry variant. The release notes of a service should provide respective information. Changes with backward compatible fallback: To identify the service, JAEGER_SERVICE_NAME is still supported. Changing to OTEL_SERVICE_NAME is recommended. To disable tracing, JAEGER_SAMPLER_TYPE=const in combination with JAEGER_SAMPLER_PARAM=0 is still supported. Changing to TRACING_DISABLED=true is recommended. Incompatible changes: JAEGER_AGENT_HOST is not supported anymore. The official documentation provides all available options. In an environment that previously used Jaeger, OTEL_TRACES_EXPORTER=jaeger and OTEL_EXPORTER_JAEGER_ENDPOINT=http://jaeger-collector.jaeger:14250 may be suitable. Note that the collector endpoint may not be the same as the agent endpoint configured for Jaeger.","title":"Tracing"},{"location":"migration-guides/MIGRATION_GUIDE_v2_to_v3/#kafka","text":"All topic configurations for topic creation and validation are not considered anymore. Users of the service should be informed that the service will not create any topic and does not validate existing topics. Therefore, some configuration options of a service may be removed.","title":"Kafka"},{"location":"migration-guides/MIGRATION_GUIDE_v2_to_v3/#mongodb","text":"Configuration of individual parts of the MongoDB Connection String is deprecated. Each service should provide a configuration option for the full MongoDB Connection String and inform about the deprecation in the release notes.","title":"MongoDB"},{"location":"migration-guides/MIGRATION_GUIDE_v5_to_v6/","text":"Migration Guide from v5 to v6 \u00b6 General migration changes \u00b6 Version 6 of sda-dropwizard-commons now builds with Java 17 + 21. Support for Java 11 was dropped. Dropwizard 4 \u00b6 One of the major changes was the update from Dropwizard 2 to 4. Dropwizard 4 is no longer based on the Java EE (and the javax.* packages) but on Jakarta EE and the jakarta.* packages. Please read the full release notes in the official Dropwizard repository . Below we list the most important changes: Dropwizard Package Structure and JPMS \u00b6 In order to properly support the Java Platform Module System (JPMS), the Java packages in modules must not overlap, or put differently, the packages may not be split into multiple modules. Affected packages: Maven module Old package New package dropwizard-core io.dropwizard io.dropwizard.core dropwizard-logging io.dropwizard.logging io.dropwizard.logging.common dropwizard-metrics io.dropwizard.metrics io.dropwizard.metrics.common dropwizard-views io.dropwizard.views io.dropwizard.views.common Jakarta package namespace \u00b6 The previous package namespace for Java EE, javax.* was replaced by jakarta namespace, so it was necessary to replace all the imports starting with import javax. to import jakarta. and dependencies. Here is the list with the modified dependencies: javax.xml.bind:jaxb-api -> jakarta.xml.bind:jakarta.xml.bind-api javax.annotation:javax.annotation-api -> jakarta.annotation:jakarta.annotation-api javax.transaction:javax.transaction-api -> jakarta.transaction:jakarta.transaction-api io.swagger.core.v3:swagger-annotations -> io.swagger.core.v3:swagger-annotations-jakarta io.swagger.core.v3:swagger-jaxrs2 -> io.swagger.core.v3:swagger-jaxrs2-jakarta io.swagger.core.v3:swagger-annotations -> io.swagger.core.v3:swagger-annotations-jakarta io.swagger.core.v3:swagger-core -> io.swagger.core.v3:swagger-core-jakarta You can find more details about this change on Upgrade Notes for Dropwizard 4.0.x . Apache Http Client \u00b6 The Apache version was upgraded from v4 to v5. The imports were changed from org.apache.http to org.apache.hc.core5 and org.apache.hc.client5 . You can check the full migration guide in the official documentation: Migration from Apache HttpClient 4.x APIs Closing Responses The Apache 5 connector seems to be more sensitive and might get stuck if you don't close your Response objects. Make sure to use try-with-resources or finally when you use Jersey clients (either in tests or in production!). Example: 1 2 3 4 5 6 7 try ( Response response = DW . client () . target ( \"http://localhost:\" + DW . getLocalPort ()) . path ( \"/example\" ) . request ( APPLICATION_JSON ) . get ()) { assertThat ( response . getStatus ()). isEqualTo ( 200 ); } Jetty 11 \u00b6 Dropwizard v4 upgraded to Jetty 11.0.x. The main changes were regarding supporting jakarta.servlet namespace and a complete WebSocket refactoring, those using the Jetty APIs or embedded-jetty will need to update their code. You can read more information in the release notes and in the official migration guide . Hibernate 6 \u00b6 The Hibernate library was upgraded to 6.1. Both of them provide compatible implementations for Jakarta Persistence 3.0. You can check the migration guide to v6.0 and to v6.1 . Modules \u00b6 The following modules contain changes: sda-commons-server-testing sda-commons-server-spring-data-mongo sda-commons-server-mongo-testing sda-commons-client-jersey-wiremock-testing sda-commons-server-circuitbreaker sda-commons-shared-asyncapi sda-commons-server-kafka sda-commons-server-s3 sda-commons-server-s3-testing sda-commons-server-prometheus 1 sda-commons-server-testing \u00b6 Removed Support for JUnit 4.x You must use all JUnit 5 extensions, classes, annotations, and libraries and migrate all your JUnit 4 tests to JUnit 5. 2 sda-commons-server-spring-data-mongo \u00b6 The deprecated legacy configuration support for individual properties like hosts or database was removed. The database connection must be configured with connectionString . 3 sda-commons-server-mongo-testing \u00b6 Removed custom proxy configuration for MongoDB executable download. OS proxy settings should be configured instead. Removed getters from the MongoDb interface which affects the MongoDbClassExtension . You can retrieve information about the database, username or password from the ConnectionString that is provided by MongoDbClassExtension#getMongoConnectionString() . FixtureHelpers \u00b6 The class io.drowizard.helpers.fixtures.FixtureHelpers is not available in Dropwizard v4. So you must read the file using other approaches, e.g. using Wiremock response body or using an ObjectMapper . 4 sda-commons-client-jersey-wiremock-testing \u00b6 Dropwizard v4 uses wiremock v3.x version. Were introduced some breaking changes, like dropping support for Java 8, upgrading from Jetty 9 to Jetty 11 and changing the repository groupID to org.wiremock for all artifacts : wiremock, wiremock-standalone, wiremock-webhooks-extension. Module sda-commons-client-jersey-wiremock-testing was renamed to sda-commons-shared-wiremock-testing . SDA specific Wiremock test extensions were removed and replaced with Wiremock internal extensions. You will find example tests in WireMockExampleTest . You can see all the release notes and breaking changes in the official repository . 5 sda-commons-server-circuitbreaker \u00b6 Resilience4j-Circuitbreaker was updated from 1.7.x to 2.1. Please check their release notes for details. The class org.sdase.commons.server.circuitbreaker.metrics.SdaCircuitBreakerMetricsCollector was removed. We now collect metrics using Micrometer . The metric named resilience4j_circuitbreaker_calls_bucket is not exposed anymore. Please use Micrometer's metric resilience4j_circuitbreaker_calls_count instead. 6 sda-commons-shared-asyncapi \u00b6 Json Schemas for AsyncAPI are generated with Victools' Json Schema Generator now. The previously used library is barely maintained in the past years. The old library provided their own annotations. Now, annotations of Jackson (e.g. @JsonSchemaDescription ), Swagger (e.g. @Schema ) and Jakarta Validation (e.g. NotNull ) can be used. Note that not all attributes of all annotations are covered and multiple examples are not possible anymore. Only one example can be defined with @Schema(example = \"value\") . How the Java classes for schema definitions in the AsyncAPI are defined has changed. Previously, classes to integrate were defined in the code ( .withSchema(\"./schema.json\", BaseEvent.class) ) and referenced in the AsyncAPI template ( $ref: './schema.json#/definitions/CarManufactured' ). Now the classes are referenced directly in the template ( $ref: 'class://com.example.BaseEvent ). The builder method withSchema does not exist anymore. Please review the differences in the generated AsyncAPI file. Both libraries work different and have a different feature set. The new generator may have some limitations but a great API for extensions. Please file an issue if something important can't be expressed. 7 sda-commons-server-kafka \u00b6 The public init method MessageListenerStrategy#init(ConsumerTopicMessageHistogram consumerTopicMessageHistogram, Set<String> metadataFields) was removed. Internal metrics are collected automatically. Please use MessageListenerStrategy#init(Set<String> metadataFields) 8 sda-commons-server-s3 \u00b6 The AWS SDK was upgraded from 1.12.x to https://github.com/aws/aws-sdk-java-v2 . The new version is not compatible with the old one. You can find the official documentation for the migration https://github.com/aws/aws-sdk-java-v2/blob/master/docs/LaunchChangelog.md . This can also be explained by the Jakarta update because the old AWS SDK still used the Apache HTTP Client v4, which is not compatible with Jakarta. The old AWS SDK did not have the option to pick a different HTTP client. Most noticeably, the base package of the classes moved from com.amazonaws to software.amazon.awssdk . The S3Bundle will not return an instance of software.amazon.awssdk.services.s3.S3Client instead of com.amazonaws.services.s3.AmazonS3 Moreover, the bundle now supports using the AWS default credentials provider chain to retrieve credentials if you don't provide them in the S3Configuration . You can now also pass credentials via environment variables AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY or via system properties aws.accessKeyId and aws.secretAccessKey . 9 sda-commons-server-s3-testing \u00b6 Our previous S3 mock library s3mock was also based on the old AWS SDK v1.12.x and is no longer maintained. We switched to Robothy local-s3 as alternative. You can still use our S3ClassExtension to start the S3 mock server in your tests. But it will now give you an instance of software.amazon.awssdk.services.s3.S3Client for the S3 client. Additionally, you need to annotate your S3 tests with @LocalS3 due to an implementation detail of the underlying test library. 10 sda-commons-server-prometheus \u00b6 The endpoint /healthcheck/prometheus was removed. Health checks metrics are available using endpoint /metrics/prometheus . SDA specific metric http_request_duration_seconds was removed, please use http_server_requests_seconds instead. 11 sda-commons-server-opentelemetry \u00b6 Important : The Java classes in the io.opentelemetry.instrumentation.apachehttpclient.v5_2 package are provided as a temporary solution to support Apache Http Client version 5.2. However, these classes will only be available until the official Open Telemetry instrumentation team releases a version fully compatible with Apache Http Client 5.2. Future Updates : Once the official Open Telemetry instrumentation team releases a version fully compatible with Apache Http Client 5.2, we will release a new version. This new release will seamlessly replace the temporary package with the official library, ensuring continued compatibility and access to the latest features. Automation \u00b6 The following bash script can help you to quickly migrate your project to sda-dropwizard-commons 6. Copy the content to a file in the root of your project and execute it. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 touch migrate.sh chmod u+x migrate.sh ```` ``` bash #!/bin/bash echo \"Migrating Java files\" for file in ` find . -type f -name \"*.java\" ` do echo \"Processing $file \" sed -i '' -e 's/javax.validation/jakarta.validation/g' $file sed -i '' -e 's/javax.ws.rs/jakarta.ws.rs/g' $file sed -i '' -e 's/javax.annotation/jakarta.annotation/g' $file sed -i '' -e 's/io.dropwizard.Application/io.dropwizard.core.Application/g' $file sed -i '' -e 's/io.dropwizard.Configuration/io.dropwizard.core.Configuration/g' $file sed -i '' -e 's/io.dropwizard.setup.Bootstrap/io.dropwizard.core.setup.Bootstrap/g' $file sed -i '' -e 's/io.dropwizard.setup.Environment/io.dropwizard.core.setup.Environment/g' $file sed -i '' -e 's/com.amazonaws.services.s3.AmazonS3/software.amazon.awssdk.services.s3.S3Client/g' $file sed -i '' -e 's/AmazonS3/S3Client/g' $file sed -i '' -e 's/org.apache.http.HttpStatus/org.apache.hc.core5.http.HttpStatus/g' $file # test files only if [[ $file = ~ .*src \\/ test.* ]] ; then sed -i '' -e 's/org.sdase.commons.client.jersey.wiremock.testing.WireMockClassExtension/com.github.tomakehurst.wiremock.junit5.WireMockExtension/g' $file sed -i '' -e 's/new WireMockClassExtension(wireMockConfig().dynamicPort());/new WireMockExtension().builder().build()/g' $file sed -i '' -e 's/WireMockClassExtension;/WireMockExtension/g' $file fi done echo \"Migrating Gradle files\" for file in ` find . -type f -name \"build.gradle\" ` do echo \"Processing $file \" sed -i '' -e 's/org.sdase.commons:sda-commons-client-jersey-wiremock-testing/org.sdase.commons:sda-commons-shared-wiremock-testing/g' $file done","title":"Migrate from v5 to v6"},{"location":"migration-guides/MIGRATION_GUIDE_v5_to_v6/#migration-guide-from-v5-to-v6","text":"","title":"Migration Guide from v5 to v6"},{"location":"migration-guides/MIGRATION_GUIDE_v5_to_v6/#general-migration-changes","text":"Version 6 of sda-dropwizard-commons now builds with Java 17 + 21. Support for Java 11 was dropped.","title":"General migration changes"},{"location":"migration-guides/MIGRATION_GUIDE_v5_to_v6/#dropwizard-4","text":"One of the major changes was the update from Dropwizard 2 to 4. Dropwizard 4 is no longer based on the Java EE (and the javax.* packages) but on Jakarta EE and the jakarta.* packages. Please read the full release notes in the official Dropwizard repository . Below we list the most important changes:","title":"Dropwizard 4"},{"location":"migration-guides/MIGRATION_GUIDE_v5_to_v6/#dropwizard-package-structure-and-jpms","text":"In order to properly support the Java Platform Module System (JPMS), the Java packages in modules must not overlap, or put differently, the packages may not be split into multiple modules. Affected packages: Maven module Old package New package dropwizard-core io.dropwizard io.dropwizard.core dropwizard-logging io.dropwizard.logging io.dropwizard.logging.common dropwizard-metrics io.dropwizard.metrics io.dropwizard.metrics.common dropwizard-views io.dropwizard.views io.dropwizard.views.common","title":"Dropwizard Package Structure and JPMS"},{"location":"migration-guides/MIGRATION_GUIDE_v5_to_v6/#jakarta-package-namespace","text":"The previous package namespace for Java EE, javax.* was replaced by jakarta namespace, so it was necessary to replace all the imports starting with import javax. to import jakarta. and dependencies. Here is the list with the modified dependencies: javax.xml.bind:jaxb-api -> jakarta.xml.bind:jakarta.xml.bind-api javax.annotation:javax.annotation-api -> jakarta.annotation:jakarta.annotation-api javax.transaction:javax.transaction-api -> jakarta.transaction:jakarta.transaction-api io.swagger.core.v3:swagger-annotations -> io.swagger.core.v3:swagger-annotations-jakarta io.swagger.core.v3:swagger-jaxrs2 -> io.swagger.core.v3:swagger-jaxrs2-jakarta io.swagger.core.v3:swagger-annotations -> io.swagger.core.v3:swagger-annotations-jakarta io.swagger.core.v3:swagger-core -> io.swagger.core.v3:swagger-core-jakarta You can find more details about this change on Upgrade Notes for Dropwizard 4.0.x .","title":"Jakarta package namespace"},{"location":"migration-guides/MIGRATION_GUIDE_v5_to_v6/#apache-http-client","text":"The Apache version was upgraded from v4 to v5. The imports were changed from org.apache.http to org.apache.hc.core5 and org.apache.hc.client5 . You can check the full migration guide in the official documentation: Migration from Apache HttpClient 4.x APIs Closing Responses The Apache 5 connector seems to be more sensitive and might get stuck if you don't close your Response objects. Make sure to use try-with-resources or finally when you use Jersey clients (either in tests or in production!). Example: 1 2 3 4 5 6 7 try ( Response response = DW . client () . target ( \"http://localhost:\" + DW . getLocalPort ()) . path ( \"/example\" ) . request ( APPLICATION_JSON ) . get ()) { assertThat ( response . getStatus ()). isEqualTo ( 200 ); }","title":"Apache Http Client"},{"location":"migration-guides/MIGRATION_GUIDE_v5_to_v6/#jetty-11","text":"Dropwizard v4 upgraded to Jetty 11.0.x. The main changes were regarding supporting jakarta.servlet namespace and a complete WebSocket refactoring, those using the Jetty APIs or embedded-jetty will need to update their code. You can read more information in the release notes and in the official migration guide .","title":"Jetty 11"},{"location":"migration-guides/MIGRATION_GUIDE_v5_to_v6/#hibernate-6","text":"The Hibernate library was upgraded to 6.1. Both of them provide compatible implementations for Jakarta Persistence 3.0. You can check the migration guide to v6.0 and to v6.1 .","title":"Hibernate 6"},{"location":"migration-guides/MIGRATION_GUIDE_v5_to_v6/#modules","text":"The following modules contain changes: sda-commons-server-testing sda-commons-server-spring-data-mongo sda-commons-server-mongo-testing sda-commons-client-jersey-wiremock-testing sda-commons-server-circuitbreaker sda-commons-shared-asyncapi sda-commons-server-kafka sda-commons-server-s3 sda-commons-server-s3-testing sda-commons-server-prometheus","title":"Modules"},{"location":"migration-guides/MIGRATION_GUIDE_v5_to_v6/#1-sda-commons-server-testing","text":"Removed Support for JUnit 4.x You must use all JUnit 5 extensions, classes, annotations, and libraries and migrate all your JUnit 4 tests to JUnit 5.","title":"1 sda-commons-server-testing"},{"location":"migration-guides/MIGRATION_GUIDE_v5_to_v6/#2-sda-commons-server-spring-data-mongo","text":"The deprecated legacy configuration support for individual properties like hosts or database was removed. The database connection must be configured with connectionString .","title":"2 sda-commons-server-spring-data-mongo"},{"location":"migration-guides/MIGRATION_GUIDE_v5_to_v6/#3-sda-commons-server-mongo-testing","text":"Removed custom proxy configuration for MongoDB executable download. OS proxy settings should be configured instead. Removed getters from the MongoDb interface which affects the MongoDbClassExtension . You can retrieve information about the database, username or password from the ConnectionString that is provided by MongoDbClassExtension#getMongoConnectionString() .","title":"3 sda-commons-server-mongo-testing"},{"location":"migration-guides/MIGRATION_GUIDE_v5_to_v6/#fixturehelpers","text":"The class io.drowizard.helpers.fixtures.FixtureHelpers is not available in Dropwizard v4. So you must read the file using other approaches, e.g. using Wiremock response body or using an ObjectMapper .","title":"FixtureHelpers"},{"location":"migration-guides/MIGRATION_GUIDE_v5_to_v6/#4-sda-commons-client-jersey-wiremock-testing","text":"Dropwizard v4 uses wiremock v3.x version. Were introduced some breaking changes, like dropping support for Java 8, upgrading from Jetty 9 to Jetty 11 and changing the repository groupID to org.wiremock for all artifacts : wiremock, wiremock-standalone, wiremock-webhooks-extension. Module sda-commons-client-jersey-wiremock-testing was renamed to sda-commons-shared-wiremock-testing . SDA specific Wiremock test extensions were removed and replaced with Wiremock internal extensions. You will find example tests in WireMockExampleTest . You can see all the release notes and breaking changes in the official repository .","title":"4 sda-commons-client-jersey-wiremock-testing"},{"location":"migration-guides/MIGRATION_GUIDE_v5_to_v6/#5-sda-commons-server-circuitbreaker","text":"Resilience4j-Circuitbreaker was updated from 1.7.x to 2.1. Please check their release notes for details. The class org.sdase.commons.server.circuitbreaker.metrics.SdaCircuitBreakerMetricsCollector was removed. We now collect metrics using Micrometer . The metric named resilience4j_circuitbreaker_calls_bucket is not exposed anymore. Please use Micrometer's metric resilience4j_circuitbreaker_calls_count instead.","title":"5 sda-commons-server-circuitbreaker"},{"location":"migration-guides/MIGRATION_GUIDE_v5_to_v6/#6-sda-commons-shared-asyncapi","text":"Json Schemas for AsyncAPI are generated with Victools' Json Schema Generator now. The previously used library is barely maintained in the past years. The old library provided their own annotations. Now, annotations of Jackson (e.g. @JsonSchemaDescription ), Swagger (e.g. @Schema ) and Jakarta Validation (e.g. NotNull ) can be used. Note that not all attributes of all annotations are covered and multiple examples are not possible anymore. Only one example can be defined with @Schema(example = \"value\") . How the Java classes for schema definitions in the AsyncAPI are defined has changed. Previously, classes to integrate were defined in the code ( .withSchema(\"./schema.json\", BaseEvent.class) ) and referenced in the AsyncAPI template ( $ref: './schema.json#/definitions/CarManufactured' ). Now the classes are referenced directly in the template ( $ref: 'class://com.example.BaseEvent ). The builder method withSchema does not exist anymore. Please review the differences in the generated AsyncAPI file. Both libraries work different and have a different feature set. The new generator may have some limitations but a great API for extensions. Please file an issue if something important can't be expressed.","title":"6 sda-commons-shared-asyncapi"},{"location":"migration-guides/MIGRATION_GUIDE_v5_to_v6/#7-sda-commons-server-kafka","text":"The public init method MessageListenerStrategy#init(ConsumerTopicMessageHistogram consumerTopicMessageHistogram, Set<String> metadataFields) was removed. Internal metrics are collected automatically. Please use MessageListenerStrategy#init(Set<String> metadataFields)","title":"7 sda-commons-server-kafka"},{"location":"migration-guides/MIGRATION_GUIDE_v5_to_v6/#8-sda-commons-server-s3","text":"The AWS SDK was upgraded from 1.12.x to https://github.com/aws/aws-sdk-java-v2 . The new version is not compatible with the old one. You can find the official documentation for the migration https://github.com/aws/aws-sdk-java-v2/blob/master/docs/LaunchChangelog.md . This can also be explained by the Jakarta update because the old AWS SDK still used the Apache HTTP Client v4, which is not compatible with Jakarta. The old AWS SDK did not have the option to pick a different HTTP client. Most noticeably, the base package of the classes moved from com.amazonaws to software.amazon.awssdk . The S3Bundle will not return an instance of software.amazon.awssdk.services.s3.S3Client instead of com.amazonaws.services.s3.AmazonS3 Moreover, the bundle now supports using the AWS default credentials provider chain to retrieve credentials if you don't provide them in the S3Configuration . You can now also pass credentials via environment variables AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY or via system properties aws.accessKeyId and aws.secretAccessKey .","title":"8 sda-commons-server-s3"},{"location":"migration-guides/MIGRATION_GUIDE_v5_to_v6/#9-sda-commons-server-s3-testing","text":"Our previous S3 mock library s3mock was also based on the old AWS SDK v1.12.x and is no longer maintained. We switched to Robothy local-s3 as alternative. You can still use our S3ClassExtension to start the S3 mock server in your tests. But it will now give you an instance of software.amazon.awssdk.services.s3.S3Client for the S3 client. Additionally, you need to annotate your S3 tests with @LocalS3 due to an implementation detail of the underlying test library.","title":"9 sda-commons-server-s3-testing"},{"location":"migration-guides/MIGRATION_GUIDE_v5_to_v6/#10-sda-commons-server-prometheus","text":"The endpoint /healthcheck/prometheus was removed. Health checks metrics are available using endpoint /metrics/prometheus . SDA specific metric http_request_duration_seconds was removed, please use http_server_requests_seconds instead.","title":"10 sda-commons-server-prometheus"},{"location":"migration-guides/MIGRATION_GUIDE_v5_to_v6/#11-sda-commons-server-opentelemetry","text":"Important : The Java classes in the io.opentelemetry.instrumentation.apachehttpclient.v5_2 package are provided as a temporary solution to support Apache Http Client version 5.2. However, these classes will only be available until the official Open Telemetry instrumentation team releases a version fully compatible with Apache Http Client 5.2. Future Updates : Once the official Open Telemetry instrumentation team releases a version fully compatible with Apache Http Client 5.2, we will release a new version. This new release will seamlessly replace the temporary package with the official library, ensuring continued compatibility and access to the latest features.","title":"11 sda-commons-server-opentelemetry"},{"location":"migration-guides/MIGRATION_GUIDE_v5_to_v6/#automation","text":"The following bash script can help you to quickly migrate your project to sda-dropwizard-commons 6. Copy the content to a file in the root of your project and execute it. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 touch migrate.sh chmod u+x migrate.sh ```` ``` bash #!/bin/bash echo \"Migrating Java files\" for file in ` find . -type f -name \"*.java\" ` do echo \"Processing $file \" sed -i '' -e 's/javax.validation/jakarta.validation/g' $file sed -i '' -e 's/javax.ws.rs/jakarta.ws.rs/g' $file sed -i '' -e 's/javax.annotation/jakarta.annotation/g' $file sed -i '' -e 's/io.dropwizard.Application/io.dropwizard.core.Application/g' $file sed -i '' -e 's/io.dropwizard.Configuration/io.dropwizard.core.Configuration/g' $file sed -i '' -e 's/io.dropwizard.setup.Bootstrap/io.dropwizard.core.setup.Bootstrap/g' $file sed -i '' -e 's/io.dropwizard.setup.Environment/io.dropwizard.core.setup.Environment/g' $file sed -i '' -e 's/com.amazonaws.services.s3.AmazonS3/software.amazon.awssdk.services.s3.S3Client/g' $file sed -i '' -e 's/AmazonS3/S3Client/g' $file sed -i '' -e 's/org.apache.http.HttpStatus/org.apache.hc.core5.http.HttpStatus/g' $file # test files only if [[ $file = ~ .*src \\/ test.* ]] ; then sed -i '' -e 's/org.sdase.commons.client.jersey.wiremock.testing.WireMockClassExtension/com.github.tomakehurst.wiremock.junit5.WireMockExtension/g' $file sed -i '' -e 's/new WireMockClassExtension(wireMockConfig().dynamicPort());/new WireMockExtension().builder().build()/g' $file sed -i '' -e 's/WireMockClassExtension;/WireMockExtension/g' $file fi done echo \"Migrating Gradle files\" for file in ` find . -type f -name \"build.gradle\" ` do echo \"Processing $file \" sed -i '' -e 's/org.sdase.commons:sda-commons-client-jersey-wiremock-testing/org.sdase.commons:sda-commons-shared-wiremock-testing/g' $file done","title":"Automation"}]}